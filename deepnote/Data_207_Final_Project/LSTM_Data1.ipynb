{
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.impute import KNNImputer\nfrom sklearn.decomposition import PCA\n! pip install scikit-optimize\n# report scikit-optimize version number\nimport skopt\nprint('skopt %s' % skopt.__version__)\nfrom skopt.space import Integer\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.metrics import classification_report\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, Embedding, Attention\nfrom tensorflow.keras.optimizers.legacy import SGD\nfrom tensorflow.keras import backend as K\nimport argparse",
      "metadata": {
        "cell_id": "b7b22da73d2d4b19bdd724d01c826daf",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already satisfied: scikit-optimize in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (0.10.1)\nRequirement already satisfied: joblib>=0.11 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (1.2.0)\nRequirement already satisfied: pyaml>=16.9 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (23.12.0)\nRequirement already satisfied: numpy>=1.20.3 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (1.26.4)\nRequirement already satisfied: scipy>=1.1.0 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (1.11.4)\nRequirement already satisfied: scikit-learn>=1.0.0 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (1.2.2)\nRequirement already satisfied: packaging>=21.3 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (23.1)\nRequirement already satisfied: PyYAML in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.0->scikit-optimize) (2.2.0)\nskopt 0.10.1\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 1,
      "block_group": "d0ffa8face124ae9b78532c548c11b4a",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "#data from kaggle dataset: \"Prediction of music genre\"\ndata1 = pd.read_csv(\"~/Downloads/music_genre.csv\")\n#data from kaggle data set: \"Spotify Tracks Dataset\"\ndata2 = pd.read_csv(\"~/Downloads/dataset.csv\")",
      "metadata": {
        "cell_id": "eb62585793ce4294a463c4591a7b43ee",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": 2,
      "block_group": "7f0c9765ea49410da12724e4f0bf8e7e",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "#because all the column values are none values, drop those rows\nbefore_drop_na = len(data1)\ndata1 = data1.dropna()\nafter_drop_na = len(data1)\nprint(\"number of data dropped:\", before_drop_na-after_drop_na)",
      "metadata": {
        "cell_id": "fa8ed1b398c34471902fd266a23da014",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "number of data dropped: 5\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 3,
      "block_group": "75a4b43893504a8a9d93fa837f2433b8",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "#make key, mode into One Hot encoding\n#music_genre into label encoding\nonehot =OneHotEncoder(sparse=False)\nonehot.fit(data1[['key', 'mode']])\nonehotencoded = onehot.transform(data1[['key', 'mode']])\ncategorical_columns = [f'{col}_{cat}' for i, col in enumerate(data1[['key', 'mode']].columns) for cat in onehot.categories_[i]]\nonehotdf = pd.DataFrame(onehotencoded, columns=categorical_columns)\ndata1 = pd.concat([data1.reset_index(), onehotdf.reset_index()], axis=1)\nlabelencoder = LabelEncoder()\ndata1['key_num'] = labelencoder.fit_transform(data1['key'])\ndata1['mode_num'] = labelencoder.fit_transform(data1['mode'])\ndata1['music_genre_num'] = labelencoder.fit_transform(data1['music_genre'])\ndata1",
      "metadata": {
        "cell_id": "3333a60bed07485995049df3b2499eff",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/Users/mkobayashi/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n"
        },
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>instance_id</th>\n      <th>artist_name</th>\n      <th>track_name</th>\n      <th>popularity</th>\n      <th>acousticness</th>\n      <th>danceability</th>\n      <th>duration_ms</th>\n      <th>energy</th>\n      <th>instrumentalness</th>\n      <th>...</th>\n      <th>key_E</th>\n      <th>key_F</th>\n      <th>key_F#</th>\n      <th>key_G</th>\n      <th>key_G#</th>\n      <th>mode_Major</th>\n      <th>mode_Minor</th>\n      <th>key_num</th>\n      <th>mode_num</th>\n      <th>music_genre_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>32894.0</td>\n      <td>Röyksopp</td>\n      <td>Röyksopp's Night Out</td>\n      <td>27.0</td>\n      <td>0.00468</td>\n      <td>0.652</td>\n      <td>-1.0</td>\n      <td>0.941</td>\n      <td>0.79200</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>46652.0</td>\n      <td>Thievery Corporation</td>\n      <td>The Shining Path</td>\n      <td>31.0</td>\n      <td>0.01270</td>\n      <td>0.622</td>\n      <td>218293.0</td>\n      <td>0.890</td>\n      <td>0.95000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>30097.0</td>\n      <td>Dillon Francis</td>\n      <td>Hurricane</td>\n      <td>28.0</td>\n      <td>0.00306</td>\n      <td>0.620</td>\n      <td>215613.0</td>\n      <td>0.755</td>\n      <td>0.01180</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>11</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>62177.0</td>\n      <td>Dubloadz</td>\n      <td>Nitro</td>\n      <td>34.0</td>\n      <td>0.02540</td>\n      <td>0.774</td>\n      <td>166875.0</td>\n      <td>0.700</td>\n      <td>0.00253</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>24907.0</td>\n      <td>What So Not</td>\n      <td>Divide &amp; Conquer</td>\n      <td>32.0</td>\n      <td>0.00465</td>\n      <td>0.638</td>\n      <td>222369.0</td>\n      <td>0.587</td>\n      <td>0.90900</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>50000</td>\n      <td>58878.0</td>\n      <td>BEXEY</td>\n      <td>GO GETTA</td>\n      <td>59.0</td>\n      <td>0.03340</td>\n      <td>0.913</td>\n      <td>-1.0</td>\n      <td>0.574</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>50001</td>\n      <td>43557.0</td>\n      <td>Roy Woods</td>\n      <td>Drama (feat. Drake)</td>\n      <td>72.0</td>\n      <td>0.15700</td>\n      <td>0.709</td>\n      <td>251860.0</td>\n      <td>0.362</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>50002</td>\n      <td>39767.0</td>\n      <td>Berner</td>\n      <td>Lovin' Me (feat. Smiggz)</td>\n      <td>51.0</td>\n      <td>0.00597</td>\n      <td>0.693</td>\n      <td>189483.0</td>\n      <td>0.763</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>50003</td>\n      <td>57944.0</td>\n      <td>The-Dream</td>\n      <td>Shawty Is Da Shit</td>\n      <td>65.0</td>\n      <td>0.08310</td>\n      <td>0.782</td>\n      <td>262773.0</td>\n      <td>0.472</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>10</td>\n      <td>1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>50004</td>\n      <td>63470.0</td>\n      <td>Naughty By Nature</td>\n      <td>Hip Hop Hooray</td>\n      <td>67.0</td>\n      <td>0.10200</td>\n      <td>0.862</td>\n      <td>267267.0</td>\n      <td>0.642</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>9</td>\n      <td>1</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows × 37 columns</p>\n</div>",
            "text/plain": "       index  instance_id           artist_name                track_name  \\\n0          0      32894.0              Röyksopp      Röyksopp's Night Out   \n1          1      46652.0  Thievery Corporation          The Shining Path   \n2          2      30097.0        Dillon Francis                 Hurricane   \n3          3      62177.0              Dubloadz                     Nitro   \n4          4      24907.0           What So Not          Divide & Conquer   \n...      ...          ...                   ...                       ...   \n49995  50000      58878.0                 BEXEY                  GO GETTA   \n49996  50001      43557.0             Roy Woods       Drama (feat. Drake)   \n49997  50002      39767.0                Berner  Lovin' Me (feat. Smiggz)   \n49998  50003      57944.0             The-Dream         Shawty Is Da Shit   \n49999  50004      63470.0     Naughty By Nature            Hip Hop Hooray   \n\n       popularity  acousticness  danceability  duration_ms  energy  \\\n0            27.0       0.00468         0.652         -1.0   0.941   \n1            31.0       0.01270         0.622     218293.0   0.890   \n2            28.0       0.00306         0.620     215613.0   0.755   \n3            34.0       0.02540         0.774     166875.0   0.700   \n4            32.0       0.00465         0.638     222369.0   0.587   \n...           ...           ...           ...          ...     ...   \n49995        59.0       0.03340         0.913         -1.0   0.574   \n49996        72.0       0.15700         0.709     251860.0   0.362   \n49997        51.0       0.00597         0.693     189483.0   0.763   \n49998        65.0       0.08310         0.782     262773.0   0.472   \n49999        67.0       0.10200         0.862     267267.0   0.642   \n\n       instrumentalness  ... key_E  key_F  key_F# key_G  key_G# mode_Major  \\\n0               0.79200  ...   0.0    0.0     0.0   0.0     0.0        0.0   \n1               0.95000  ...   0.0    0.0     0.0   0.0     0.0        0.0   \n2               0.01180  ...   0.0    0.0     0.0   0.0     1.0        1.0   \n3               0.00253  ...   0.0    0.0     0.0   0.0     0.0        1.0   \n4               0.90900  ...   0.0    0.0     1.0   0.0     0.0        1.0   \n...                 ...  ...   ...    ...     ...   ...     ...        ...   \n49995           0.00000  ...   0.0    0.0     0.0   0.0     0.0        1.0   \n49996           0.00000  ...   0.0    0.0     0.0   0.0     0.0        1.0   \n49997           0.00000  ...   0.0    0.0     0.0   0.0     0.0        1.0   \n49998           0.00000  ...   0.0    0.0     0.0   1.0     0.0        0.0   \n49999           0.00000  ...   0.0    0.0     1.0   0.0     0.0        0.0   \n\n      mode_Minor  key_num mode_num  music_genre_num  \n0            1.0        1        1                5  \n1            1.0        5        1                5  \n2            0.0       11        0                5  \n3            0.0        4        0                5  \n4            0.0        9        0                5  \n...          ...      ...      ...              ...  \n49995        0.0        4        0                6  \n49996        0.0        2        0                6  \n49997        0.0        5        0                6  \n49998        1.0       10        1                6  \n49999        1.0        9        1                6  \n\n[50000 rows x 37 columns]"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "outputs_reference": null,
      "execution_count": 4,
      "block_group": "0b46261d0ce245ec81d14705c10650d7",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "#not including tempo\ndata1_edit = data1.drop(columns = [\"index\", \"instance_id\", \"artist_name\", \"track_name\", \"key\", \"mode\", \"tempo\", \"obtained_date\", \"music_genre\", \"key_num\", \"mode_num\", \"music_genre_num\"])\nfeatures = data1_edit.columns",
      "metadata": {
        "cell_id": "4a77f23dccaf41aa9068387eb61b1b21",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": 5,
      "block_group": "9f01861ffacf488e96114d541ff2ecf1",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "features",
      "metadata": {
        "cell_id": "03e8eda2761f491aac7893fdfa100434",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "Index(['popularity', 'acousticness', 'danceability', 'duration_ms', 'energy',\n       'instrumentalness', 'liveness', 'loudness', 'speechiness', 'valence',\n       'key_A', 'key_A#', 'key_B', 'key_C', 'key_C#', 'key_D', 'key_D#',\n       'key_E', 'key_F', 'key_F#', 'key_G', 'key_G#', 'mode_Major',\n       'mode_Minor'],\n      dtype='object')"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "outputs_reference": null,
      "execution_count": 6,
      "block_group": "92e3605f3548463787f8007e6ed51a0d",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "# train 80%, val 20%, test 20%\nX_train1b, X_test1b, y_train1b, y_test1b = train_test_split(data1[features], data1[\"music_genre_num\"], test_size=0.2, random_state=1)\nX_train1b, X_val1b, y_train1b, y_val1b = train_test_split(X_train1b, y_train1b, test_size=0.25, random_state=1)",
      "metadata": {
        "cell_id": "9335cb4c02a242b6a3d66c70e98cfbef",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": 7,
      "block_group": "5bb7496a1a1c454d936dcbc051688a52",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "model1b = Sequential()\nmodel1b.add(LSTM(units=256,input_shape=(24, 1)))\nmodel1b.add(Dense(10, activation = \"softmax\"))\n\nmodel1b.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\nhistory1b = model1b.fit(X_train1b, y_train1b, validation_data=(X_val1b, y_val1b), batch_size=32, epochs=100)",
      "metadata": {
        "cell_id": "b11454ea94674b60bb797fd956ea27e6",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n938/938 [==============================] - 36s 37ms/step - loss: 1.7101 - accuracy: 0.3301 - val_loss: 1.5566 - val_accuracy: 0.3808\nEpoch 2/100\n938/938 [==============================] - 46s 49ms/step - loss: 1.5496 - accuracy: 0.3897 - val_loss: 1.5348 - val_accuracy: 0.4005\nEpoch 3/100\n938/938 [==============================] - 49s 52ms/step - loss: 1.5111 - accuracy: 0.4052 - val_loss: 1.4906 - val_accuracy: 0.4132\nEpoch 4/100\n938/938 [==============================] - 50s 53ms/step - loss: 1.4403 - accuracy: 0.4322 - val_loss: 1.3924 - val_accuracy: 0.4471\nEpoch 5/100\n938/938 [==============================] - 51s 54ms/step - loss: 1.3894 - accuracy: 0.4581 - val_loss: 1.3219 - val_accuracy: 0.4897\nEpoch 6/100\n938/938 [==============================] - 44s 47ms/step - loss: 1.2892 - accuracy: 0.5016 - val_loss: 1.4262 - val_accuracy: 0.4614\nEpoch 7/100\n938/938 [==============================] - 44s 47ms/step - loss: 1.2175 - accuracy: 0.5244 - val_loss: 1.2659 - val_accuracy: 0.5024\nEpoch 8/100\n938/938 [==============================] - 65s 70ms/step - loss: 1.1913 - accuracy: 0.5359 - val_loss: 1.1636 - val_accuracy: 0.5469\nEpoch 9/100\n938/938 [==============================] - 63s 67ms/step - loss: 1.1687 - accuracy: 0.5476 - val_loss: 1.1762 - val_accuracy: 0.5431\nEpoch 10/100\n938/938 [==============================] - 53s 57ms/step - loss: 1.1549 - accuracy: 0.5470 - val_loss: 1.1649 - val_accuracy: 0.5438\nEpoch 11/100\n938/938 [==============================] - 56s 60ms/step - loss: 1.1444 - accuracy: 0.5534 - val_loss: 1.1462 - val_accuracy: 0.5515\nEpoch 12/100\n938/938 [==============================] - 53s 57ms/step - loss: 1.1340 - accuracy: 0.5557 - val_loss: 1.1642 - val_accuracy: 0.5519\nEpoch 13/100\n938/938 [==============================] - 59s 63ms/step - loss: 1.1230 - accuracy: 0.5586 - val_loss: 1.1426 - val_accuracy: 0.5538\nEpoch 14/100\n938/938 [==============================] - 56s 59ms/step - loss: 1.1115 - accuracy: 0.5625 - val_loss: 1.1392 - val_accuracy: 0.5566\nEpoch 15/100\n938/938 [==============================] - 55s 59ms/step - loss: 1.1100 - accuracy: 0.5641 - val_loss: 1.1502 - val_accuracy: 0.5514\nEpoch 16/100\n938/938 [==============================] - 59s 62ms/step - loss: 1.0972 - accuracy: 0.5684 - val_loss: 1.1225 - val_accuracy: 0.5666\nEpoch 17/100\n938/938 [==============================] - 64s 67ms/step - loss: 1.0893 - accuracy: 0.5724 - val_loss: 1.1510 - val_accuracy: 0.5544\nEpoch 18/100\n938/938 [==============================] - 64s 68ms/step - loss: 1.0839 - accuracy: 0.5706 - val_loss: 1.1590 - val_accuracy: 0.5405\nEpoch 19/100\n938/938 [==============================] - 68s 73ms/step - loss: 1.0779 - accuracy: 0.5727 - val_loss: 1.1470 - val_accuracy: 0.5553\nEpoch 20/100\n938/938 [==============================] - 62s 66ms/step - loss: 1.0696 - accuracy: 0.5767 - val_loss: 1.1451 - val_accuracy: 0.5486\nEpoch 21/100\n938/938 [==============================] - 62s 67ms/step - loss: 1.0604 - accuracy: 0.5781 - val_loss: 1.1351 - val_accuracy: 0.5608\nEpoch 22/100\n938/938 [==============================] - 61s 65ms/step - loss: 1.0499 - accuracy: 0.5839 - val_loss: 1.1348 - val_accuracy: 0.5579\nEpoch 23/100\n938/938 [==============================] - 58s 62ms/step - loss: 1.0439 - accuracy: 0.5831 - val_loss: 1.1263 - val_accuracy: 0.5592\nEpoch 24/100\n938/938 [==============================] - 59s 63ms/step - loss: 1.0338 - accuracy: 0.5871 - val_loss: 1.1295 - val_accuracy: 0.5588\nEpoch 25/100\n938/938 [==============================] - 68s 73ms/step - loss: 1.0276 - accuracy: 0.5878 - val_loss: 1.1483 - val_accuracy: 0.5575\nEpoch 26/100\n938/938 [==============================] - 61s 65ms/step - loss: 1.0154 - accuracy: 0.5943 - val_loss: 1.1577 - val_accuracy: 0.5549\nEpoch 27/100\n938/938 [==============================] - 59s 63ms/step - loss: 1.0110 - accuracy: 0.5970 - val_loss: 1.1552 - val_accuracy: 0.5561\nEpoch 28/100\n938/938 [==============================] - 56s 60ms/step - loss: 0.9930 - accuracy: 0.6002 - val_loss: 1.1751 - val_accuracy: 0.5506\nEpoch 29/100\n938/938 [==============================] - 57s 61ms/step - loss: 0.9849 - accuracy: 0.6060 - val_loss: 1.1564 - val_accuracy: 0.5501\nEpoch 30/100\n938/938 [==============================] - 60s 64ms/step - loss: 0.9740 - accuracy: 0.6074 - val_loss: 1.1748 - val_accuracy: 0.5532\nEpoch 31/100\n938/938 [==============================] - 67s 72ms/step - loss: 0.9567 - accuracy: 0.6136 - val_loss: 1.1805 - val_accuracy: 0.5477\nEpoch 32/100\n938/938 [==============================] - 68s 73ms/step - loss: 0.9389 - accuracy: 0.6194 - val_loss: 1.2004 - val_accuracy: 0.5488\nEpoch 33/100\n938/938 [==============================] - 69s 73ms/step - loss: 0.9272 - accuracy: 0.6275 - val_loss: 1.2033 - val_accuracy: 0.5496\nEpoch 34/100\n938/938 [==============================] - 55s 59ms/step - loss: 0.9101 - accuracy: 0.6305 - val_loss: 1.2014 - val_accuracy: 0.5529\nEpoch 35/100\n938/938 [==============================] - 52s 56ms/step - loss: 0.8965 - accuracy: 0.6376 - val_loss: 1.2242 - val_accuracy: 0.5440\nEpoch 36/100\n938/938 [==============================] - 44s 47ms/step - loss: 0.8743 - accuracy: 0.6476 - val_loss: 1.2479 - val_accuracy: 0.5416\nEpoch 37/100\n938/938 [==============================] - 43s 46ms/step - loss: 0.8559 - accuracy: 0.6541 - val_loss: 1.2772 - val_accuracy: 0.5314\nEpoch 38/100\n938/938 [==============================] - 52s 56ms/step - loss: 0.8347 - accuracy: 0.6614 - val_loss: 1.2925 - val_accuracy: 0.5370\nEpoch 39/100\n938/938 [==============================] - 46s 49ms/step - loss: 0.8184 - accuracy: 0.6690 - val_loss: 1.2867 - val_accuracy: 0.5315\nEpoch 40/100\n938/938 [==============================] - 47s 50ms/step - loss: 0.7915 - accuracy: 0.6799 - val_loss: 1.3099 - val_accuracy: 0.5351\nEpoch 41/100\n938/938 [==============================] - 54s 57ms/step - loss: 0.7796 - accuracy: 0.6827 - val_loss: 1.3385 - val_accuracy: 0.5261\nEpoch 42/100\n938/938 [==============================] - 43s 45ms/step - loss: 0.7558 - accuracy: 0.6955 - val_loss: 1.3579 - val_accuracy: 0.5328\nEpoch 43/100\n938/938 [==============================] - 40s 43ms/step - loss: 0.7307 - accuracy: 0.7019 - val_loss: 1.3814 - val_accuracy: 0.5382\nEpoch 44/100\n938/938 [==============================] - 51s 54ms/step - loss: 0.7079 - accuracy: 0.7102 - val_loss: 1.4250 - val_accuracy: 0.5226\nEpoch 45/100\n938/938 [==============================] - 39s 41ms/step - loss: 0.6838 - accuracy: 0.7229 - val_loss: 1.4758 - val_accuracy: 0.5167\nEpoch 46/100\n938/938 [==============================] - 38s 40ms/step - loss: 0.6698 - accuracy: 0.7288 - val_loss: 1.4749 - val_accuracy: 0.5191\nEpoch 47/100\n938/938 [==============================] - 36s 38ms/step - loss: 0.6519 - accuracy: 0.7353 - val_loss: 1.5033 - val_accuracy: 0.5215\nEpoch 48/100\n938/938 [==============================] - 37s 39ms/step - loss: 0.6183 - accuracy: 0.7503 - val_loss: 1.5435 - val_accuracy: 0.5196\nEpoch 49/100\n938/938 [==============================] - 37s 40ms/step - loss: 0.6076 - accuracy: 0.7566 - val_loss: 1.5622 - val_accuracy: 0.5176\nEpoch 50/100\n938/938 [==============================] - 38s 40ms/step - loss: 0.5949 - accuracy: 0.7600 - val_loss: 1.6006 - val_accuracy: 0.5119\nEpoch 51/100\n938/938 [==============================] - 256s 273ms/step - loss: 0.5592 - accuracy: 0.7747 - val_loss: 1.6435 - val_accuracy: 0.5033\nEpoch 52/100\n938/938 [==============================] - 51s 54ms/step - loss: 0.5498 - accuracy: 0.7803 - val_loss: 1.7051 - val_accuracy: 0.5133\nEpoch 53/100\n938/938 [==============================] - 46s 49ms/step - loss: 0.5393 - accuracy: 0.7823 - val_loss: 1.6975 - val_accuracy: 0.5040\nEpoch 54/100\n938/938 [==============================] - 39s 41ms/step - loss: 0.5221 - accuracy: 0.7884 - val_loss: 1.7376 - val_accuracy: 0.5089\nEpoch 55/100\n938/938 [==============================] - 42s 44ms/step - loss: 0.4989 - accuracy: 0.8006 - val_loss: 1.7354 - val_accuracy: 0.4963\nEpoch 56/100\n938/938 [==============================] - 47s 50ms/step - loss: 0.4813 - accuracy: 0.8064 - val_loss: 1.8139 - val_accuracy: 0.5011\nEpoch 57/100\n938/938 [==============================] - 37s 40ms/step - loss: 0.4695 - accuracy: 0.8107 - val_loss: 1.8318 - val_accuracy: 0.5038\nEpoch 58/100\n938/938 [==============================] - 41s 44ms/step - loss: 0.4543 - accuracy: 0.8195 - val_loss: 1.8677 - val_accuracy: 0.4998\nEpoch 59/100\n938/938 [==============================] - 48s 51ms/step - loss: 0.4361 - accuracy: 0.8255 - val_loss: 1.9272 - val_accuracy: 0.4978\nEpoch 60/100\n938/938 [==============================] - 39s 42ms/step - loss: 0.4174 - accuracy: 0.8340 - val_loss: 1.9171 - val_accuracy: 0.4983\nEpoch 61/100\n938/938 [==============================] - 46s 49ms/step - loss: 0.4290 - accuracy: 0.8288 - val_loss: 1.9475 - val_accuracy: 0.5020\nEpoch 62/100\n938/938 [==============================] - 56s 59ms/step - loss: 0.4077 - accuracy: 0.8366 - val_loss: 2.0417 - val_accuracy: 0.4954\nEpoch 63/100\n938/938 [==============================] - 50s 53ms/step - loss: 0.3834 - accuracy: 0.8482 - val_loss: 2.0304 - val_accuracy: 0.4967\nEpoch 64/100\n938/938 [==============================] - 49s 52ms/step - loss: 0.3818 - accuracy: 0.8490 - val_loss: 2.0451 - val_accuracy: 0.4853\nEpoch 65/100\n938/938 [==============================] - 57s 60ms/step - loss: 0.3775 - accuracy: 0.8516 - val_loss: 2.1284 - val_accuracy: 0.4933\nEpoch 66/100\n938/938 [==============================] - 57s 61ms/step - loss: 0.3564 - accuracy: 0.8580 - val_loss: 2.1045 - val_accuracy: 0.4930\nEpoch 67/100\n938/938 [==============================] - 47s 50ms/step - loss: 0.3496 - accuracy: 0.8617 - val_loss: 2.1791 - val_accuracy: 0.4966\nEpoch 68/100\n938/938 [==============================] - 51s 54ms/step - loss: 0.3659 - accuracy: 0.8554 - val_loss: 2.1751 - val_accuracy: 0.4938\nEpoch 69/100\n938/938 [==============================] - 49s 52ms/step - loss: 0.3178 - accuracy: 0.8749 - val_loss: 2.2288 - val_accuracy: 0.4921\nEpoch 70/100\n938/938 [==============================] - 51s 54ms/step - loss: 0.3151 - accuracy: 0.8766 - val_loss: 2.2822 - val_accuracy: 0.4812\nEpoch 71/100\n938/938 [==============================] - 49s 52ms/step - loss: 0.3060 - accuracy: 0.8790 - val_loss: 2.2926 - val_accuracy: 0.4928\nEpoch 72/100\n938/938 [==============================] - 54s 57ms/step - loss: 0.3178 - accuracy: 0.8745 - val_loss: 2.2989 - val_accuracy: 0.4870\nEpoch 73/100\n938/938 [==============================] - 65s 69ms/step - loss: 0.3020 - accuracy: 0.8827 - val_loss: 2.3388 - val_accuracy: 0.4924\nEpoch 74/100\n938/938 [==============================] - 50s 53ms/step - loss: 0.2796 - accuracy: 0.8909 - val_loss: 2.3459 - val_accuracy: 0.4904\nEpoch 75/100\n938/938 [==============================] - 52s 55ms/step - loss: 0.2887 - accuracy: 0.8870 - val_loss: 2.4199 - val_accuracy: 0.4876\nEpoch 76/100\n938/938 [==============================] - 52s 55ms/step - loss: 0.2845 - accuracy: 0.8884 - val_loss: 2.4136 - val_accuracy: 0.4877\nEpoch 77/100\n938/938 [==============================] - 60s 64ms/step - loss: 0.2738 - accuracy: 0.8943 - val_loss: 2.4611 - val_accuracy: 0.4839\nEpoch 78/100\n938/938 [==============================] - 73s 78ms/step - loss: 0.2692 - accuracy: 0.8957 - val_loss: 2.4607 - val_accuracy: 0.4857\nEpoch 79/100\n938/938 [==============================] - 50s 54ms/step - loss: 0.3538 - accuracy: 0.8685 - val_loss: 2.4164 - val_accuracy: 0.4757\nEpoch 80/100\n938/938 [==============================] - 49s 52ms/step - loss: 0.2814 - accuracy: 0.8928 - val_loss: 2.4904 - val_accuracy: 0.4837\nEpoch 81/100\n938/938 [==============================] - 52s 55ms/step - loss: 0.2544 - accuracy: 0.9020 - val_loss: 2.4839 - val_accuracy: 0.4868\nEpoch 82/100\n938/938 [==============================] - 61s 65ms/step - loss: 0.2505 - accuracy: 0.9027 - val_loss: 2.5915 - val_accuracy: 0.4876\nEpoch 83/100\n938/938 [==============================] - 58s 62ms/step - loss: 0.2317 - accuracy: 0.9102 - val_loss: 2.5730 - val_accuracy: 0.4863\nEpoch 84/100\n938/938 [==============================] - 42s 44ms/step - loss: 0.2551 - accuracy: 0.9008 - val_loss: 2.5681 - val_accuracy: 0.4770\nEpoch 85/100\n938/938 [==============================] - 46s 49ms/step - loss: 0.2620 - accuracy: 0.8988 - val_loss: 2.5732 - val_accuracy: 0.4795\nEpoch 86/100\n938/938 [==============================] - 50s 53ms/step - loss: 0.2237 - accuracy: 0.9129 - val_loss: 2.6513 - val_accuracy: 0.4762\nEpoch 87/100\n938/938 [==============================] - 51s 55ms/step - loss: 0.2547 - accuracy: 0.9021 - val_loss: 2.5954 - val_accuracy: 0.4826\nEpoch 88/100\n938/938 [==============================] - 50s 53ms/step - loss: 0.2228 - accuracy: 0.9152 - val_loss: 2.6234 - val_accuracy: 0.4772\nEpoch 89/100\n938/938 [==============================] - 47s 50ms/step - loss: 0.2967 - accuracy: 0.8903 - val_loss: 2.5799 - val_accuracy: 0.4773\nEpoch 90/100\n938/938 [==============================] - 47s 51ms/step - loss: 0.2898 - accuracy: 0.8890 - val_loss: 2.5904 - val_accuracy: 0.4835\nEpoch 91/100\n938/938 [==============================] - 48s 52ms/step - loss: 0.2101 - accuracy: 0.9175 - val_loss: 2.6839 - val_accuracy: 0.4833\nEpoch 92/100\n938/938 [==============================] - 48s 52ms/step - loss: 0.2856 - accuracy: 0.8955 - val_loss: 2.6518 - val_accuracy: 0.4823\nEpoch 93/100\n938/938 [==============================] - 48s 52ms/step - loss: 0.2343 - accuracy: 0.9118 - val_loss: 2.6529 - val_accuracy: 0.4772\nEpoch 94/100\n938/938 [==============================] - 50s 53ms/step - loss: 0.2178 - accuracy: 0.9169 - val_loss: 2.7054 - val_accuracy: 0.4802\nEpoch 95/100\n938/938 [==============================] - 51s 55ms/step - loss: 0.1939 - accuracy: 0.9244 - val_loss: 2.7401 - val_accuracy: 0.4759\nEpoch 96/100\n938/938 [==============================] - 49s 52ms/step - loss: 0.2190 - accuracy: 0.9139 - val_loss: 2.7228 - val_accuracy: 0.4746\nEpoch 97/100\n938/938 [==============================] - 48s 51ms/step - loss: 0.2344 - accuracy: 0.9091 - val_loss: 2.7270 - val_accuracy: 0.4744\nEpoch 98/100\n938/938 [==============================] - 47s 50ms/step - loss: 0.2140 - accuracy: 0.9162 - val_loss: 2.7861 - val_accuracy: 0.4834\nEpoch 99/100\n938/938 [==============================] - 42s 45ms/step - loss: 0.2131 - accuracy: 0.9181 - val_loss: 2.7426 - val_accuracy: 0.4839\nEpoch 100/100\n938/938 [==============================] - 46s 49ms/step - loss: 0.2348 - accuracy: 0.9108 - val_loss: 2.7381 - val_accuracy: 0.4771\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 8,
      "block_group": "fc0d5aedafca40519634ea2c0b4cd8ed",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model1b.evaluate(X_train1b, y_train1b))\nprint(\"Test Accuracy:\", model1b.evaluate(X_test1b, y_test1b))",
      "metadata": {
        "cell_id": "9d0b8d2fdbb74d269603b6dfe2695e81",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "938/938 [==============================] - 19s 20ms/step - loss: 0.1733 - accuracy: 0.9367\nTraining Accuracy: [0.1732586920261383, 0.9366666674613953]\n313/313 [==============================] - 5s 16ms/step - loss: 2.6673 - accuracy: 0.4794\nTest Accuracy: [2.667311906814575, 0.47940000891685486]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 9,
      "block_group": "224be39e1b3d4a5fa57299b071c59d50",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(classification_report(y_test1b, np.argmax(model1b.predict(X_test1b, 1), axis =-1)))",
      "metadata": {
        "cell_id": "f96942e4ecc349aea8e781d3c6b82220",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "10000/10000 [==============================] - 49s 5ms/step\n              precision    recall  f1-score   support\n\n           0       0.32      0.33      0.32      1005\n           1       0.70      0.65      0.68      1057\n           2       0.47      0.43      0.45       985\n           3       0.77      0.78      0.77       967\n           4       0.41      0.46      0.44       969\n           5       0.54      0.54      0.54      1029\n           6       0.37      0.33      0.35      1012\n           7       0.43      0.41      0.42       996\n           8       0.34      0.37      0.35       982\n           9       0.45      0.48      0.47       998\n\n    accuracy                           0.48     10000\n   macro avg       0.48      0.48      0.48     10000\nweighted avg       0.48      0.48      0.48     10000\n\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 10,
      "block_group": "2c6bd99401c04a2abf9473724308c5ef",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "model1b2 = Sequential()\nmodel1b2.add(LSTM(units=128,input_shape=(24, 1)))\nmodel1b2.add(Dense(10, activation = \"softmax\"))\n\nmodel1b2.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\nhistory1b2 = model1b2.fit(X_train1b, y_train1b, validation_data=(X_val1b, y_val1b), batch_size=32, epochs=100)",
      "metadata": {
        "cell_id": "1caa440d183f444ab6fb1206ef047eff",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n938/938 [==============================] - 39s 40ms/step - loss: 1.7809 - accuracy: 0.3041 - val_loss: 1.5648 - val_accuracy: 0.3789\nEpoch 2/100\n938/938 [==============================] - 36s 39ms/step - loss: 1.5610 - accuracy: 0.3852 - val_loss: 1.5273 - val_accuracy: 0.3966\nEpoch 3/100\n938/938 [==============================] - 34s 36ms/step - loss: 1.5243 - accuracy: 0.3980 - val_loss: 1.5191 - val_accuracy: 0.3872\nEpoch 4/100\n938/938 [==============================] - 39s 42ms/step - loss: 1.5009 - accuracy: 0.4077 - val_loss: 1.4972 - val_accuracy: 0.3962\nEpoch 5/100\n938/938 [==============================] - 33s 35ms/step - loss: 1.4462 - accuracy: 0.4292 - val_loss: 1.4057 - val_accuracy: 0.4421\nEpoch 6/100\n938/938 [==============================] - 43s 46ms/step - loss: 1.3937 - accuracy: 0.4530 - val_loss: 1.4025 - val_accuracy: 0.4676\nEpoch 7/100\n938/938 [==============================] - 43s 46ms/step - loss: 1.3206 - accuracy: 0.4858 - val_loss: 1.4160 - val_accuracy: 0.4685\nEpoch 8/100\n938/938 [==============================] - 37s 40ms/step - loss: 1.2650 - accuracy: 0.5043 - val_loss: 1.2289 - val_accuracy: 0.5222\nEpoch 9/100\n938/938 [==============================] - 34s 37ms/step - loss: 1.2292 - accuracy: 0.5210 - val_loss: 1.2103 - val_accuracy: 0.5271\nEpoch 10/100\n938/938 [==============================] - 34s 36ms/step - loss: 1.2051 - accuracy: 0.5283 - val_loss: 1.2354 - val_accuracy: 0.5234\nEpoch 11/100\n938/938 [==============================] - 40s 42ms/step - loss: 1.1912 - accuracy: 0.5360 - val_loss: 1.2192 - val_accuracy: 0.5290\nEpoch 12/100\n938/938 [==============================] - 30s 32ms/step - loss: 1.1754 - accuracy: 0.5390 - val_loss: 1.1810 - val_accuracy: 0.5439\nEpoch 13/100\n938/938 [==============================] - 31s 33ms/step - loss: 1.1580 - accuracy: 0.5467 - val_loss: 1.2152 - val_accuracy: 0.5324\nEpoch 14/100\n938/938 [==============================] - 29s 31ms/step - loss: 1.1485 - accuracy: 0.5507 - val_loss: 1.1658 - val_accuracy: 0.5469\nEpoch 15/100\n938/938 [==============================] - 41s 44ms/step - loss: 1.1376 - accuracy: 0.5539 - val_loss: 1.1545 - val_accuracy: 0.5540\nEpoch 16/100\n938/938 [==============================] - 43s 46ms/step - loss: 1.1326 - accuracy: 0.5548 - val_loss: 1.1713 - val_accuracy: 0.5445\nEpoch 17/100\n938/938 [==============================] - 37s 39ms/step - loss: 1.1205 - accuracy: 0.5562 - val_loss: 1.1749 - val_accuracy: 0.5425\nEpoch 18/100\n938/938 [==============================] - 29s 31ms/step - loss: 1.1186 - accuracy: 0.5576 - val_loss: 1.1361 - val_accuracy: 0.5561\nEpoch 19/100\n938/938 [==============================] - 37s 39ms/step - loss: 1.1139 - accuracy: 0.5618 - val_loss: 1.1656 - val_accuracy: 0.5478\nEpoch 20/100\n938/938 [==============================] - 35s 37ms/step - loss: 1.1036 - accuracy: 0.5636 - val_loss: 1.1455 - val_accuracy: 0.5559\nEpoch 21/100\n938/938 [==============================] - 40s 43ms/step - loss: 1.1018 - accuracy: 0.5635 - val_loss: 1.1494 - val_accuracy: 0.5491\nEpoch 22/100\n938/938 [==============================] - 32s 35ms/step - loss: 1.0964 - accuracy: 0.5668 - val_loss: 1.1480 - val_accuracy: 0.5532\nEpoch 23/100\n938/938 [==============================] - 40s 43ms/step - loss: 1.0876 - accuracy: 0.5713 - val_loss: 1.1297 - val_accuracy: 0.5611\nEpoch 24/100\n938/938 [==============================] - 35s 38ms/step - loss: 1.0859 - accuracy: 0.5728 - val_loss: 1.1386 - val_accuracy: 0.5598\nEpoch 25/100\n938/938 [==============================] - 35s 38ms/step - loss: 1.0805 - accuracy: 0.5718 - val_loss: 1.1410 - val_accuracy: 0.5536\nEpoch 26/100\n938/938 [==============================] - 34s 36ms/step - loss: 1.0759 - accuracy: 0.5738 - val_loss: 1.1290 - val_accuracy: 0.5577\nEpoch 27/100\n938/938 [==============================] - 35s 37ms/step - loss: 1.0713 - accuracy: 0.5758 - val_loss: 1.1503 - val_accuracy: 0.5520\nEpoch 28/100\n938/938 [==============================] - 37s 40ms/step - loss: 1.0675 - accuracy: 0.5787 - val_loss: 1.1407 - val_accuracy: 0.5602\nEpoch 29/100\n938/938 [==============================] - 38s 40ms/step - loss: 1.0639 - accuracy: 0.5774 - val_loss: 1.1328 - val_accuracy: 0.5581\nEpoch 30/100\n938/938 [==============================] - 32s 34ms/step - loss: 1.0577 - accuracy: 0.5798 - val_loss: 1.1324 - val_accuracy: 0.5600\nEpoch 31/100\n938/938 [==============================] - 35s 37ms/step - loss: 1.0520 - accuracy: 0.5845 - val_loss: 1.1435 - val_accuracy: 0.5551\nEpoch 32/100\n938/938 [==============================] - 35s 38ms/step - loss: 1.0448 - accuracy: 0.5844 - val_loss: 1.1383 - val_accuracy: 0.5607\nEpoch 33/100\n938/938 [==============================] - 31s 33ms/step - loss: 1.0425 - accuracy: 0.5863 - val_loss: 1.1341 - val_accuracy: 0.5557\nEpoch 34/100\n938/938 [==============================] - 33s 35ms/step - loss: 1.0370 - accuracy: 0.5877 - val_loss: 1.1519 - val_accuracy: 0.5471\nEpoch 35/100\n938/938 [==============================] - 31s 33ms/step - loss: 1.0333 - accuracy: 0.5920 - val_loss: 1.1311 - val_accuracy: 0.5591\nEpoch 36/100\n938/938 [==============================] - 41s 44ms/step - loss: 1.0286 - accuracy: 0.5892 - val_loss: 1.1609 - val_accuracy: 0.5523\nEpoch 37/100\n938/938 [==============================] - 35s 37ms/step - loss: 1.0234 - accuracy: 0.5908 - val_loss: 1.1676 - val_accuracy: 0.5522\nEpoch 38/100\n938/938 [==============================] - 30s 32ms/step - loss: 1.0143 - accuracy: 0.5961 - val_loss: 1.1484 - val_accuracy: 0.5565\nEpoch 39/100\n938/938 [==============================] - 30s 32ms/step - loss: 1.0148 - accuracy: 0.5937 - val_loss: 1.1739 - val_accuracy: 0.5446\nEpoch 40/100\n938/938 [==============================] - 29s 31ms/step - loss: 1.0104 - accuracy: 0.5917 - val_loss: 1.1455 - val_accuracy: 0.5583\nEpoch 41/100\n938/938 [==============================] - 34s 36ms/step - loss: 0.9997 - accuracy: 0.6009 - val_loss: 1.1609 - val_accuracy: 0.5507\nEpoch 42/100\n938/938 [==============================] - 35s 37ms/step - loss: 1.0000 - accuracy: 0.6001 - val_loss: 1.1736 - val_accuracy: 0.5538\nEpoch 43/100\n938/938 [==============================] - 37s 39ms/step - loss: 0.9916 - accuracy: 0.6028 - val_loss: 1.1729 - val_accuracy: 0.5470\nEpoch 44/100\n938/938 [==============================] - 36s 38ms/step - loss: 0.9920 - accuracy: 0.6003 - val_loss: 1.1703 - val_accuracy: 0.5503\nEpoch 45/100\n938/938 [==============================] - 49s 52ms/step - loss: 0.9796 - accuracy: 0.6068 - val_loss: 1.1806 - val_accuracy: 0.5470\nEpoch 46/100\n938/938 [==============================] - 39s 42ms/step - loss: 0.9718 - accuracy: 0.6110 - val_loss: 1.1779 - val_accuracy: 0.5547\nEpoch 47/100\n938/938 [==============================] - 38s 40ms/step - loss: 0.9664 - accuracy: 0.6133 - val_loss: 1.1766 - val_accuracy: 0.5504\nEpoch 48/100\n938/938 [==============================] - 41s 44ms/step - loss: 0.9578 - accuracy: 0.6156 - val_loss: 1.1901 - val_accuracy: 0.5472\nEpoch 49/100\n938/938 [==============================] - 44s 47ms/step - loss: 0.9500 - accuracy: 0.6169 - val_loss: 1.1813 - val_accuracy: 0.5560\nEpoch 50/100\n938/938 [==============================] - 43s 46ms/step - loss: 0.9460 - accuracy: 0.6200 - val_loss: 1.1859 - val_accuracy: 0.5460\nEpoch 51/100\n938/938 [==============================] - 43s 46ms/step - loss: 0.9366 - accuracy: 0.6243 - val_loss: 1.2024 - val_accuracy: 0.5456\nEpoch 52/100\n938/938 [==============================] - 51s 55ms/step - loss: 0.9319 - accuracy: 0.6271 - val_loss: 1.2161 - val_accuracy: 0.5472\nEpoch 53/100\n938/938 [==============================] - 47s 50ms/step - loss: 0.9280 - accuracy: 0.6283 - val_loss: 1.2185 - val_accuracy: 0.5416\nEpoch 54/100\n938/938 [==============================] - 57s 61ms/step - loss: 0.9172 - accuracy: 0.6300 - val_loss: 1.2262 - val_accuracy: 0.5424\nEpoch 55/100\n938/938 [==============================] - 46s 49ms/step - loss: 0.9121 - accuracy: 0.6328 - val_loss: 1.2264 - val_accuracy: 0.5417\nEpoch 56/100\n938/938 [==============================] - 50s 53ms/step - loss: 0.9034 - accuracy: 0.6343 - val_loss: 1.2243 - val_accuracy: 0.5419\nEpoch 57/100\n938/938 [==============================] - 35s 37ms/step - loss: 0.8961 - accuracy: 0.6399 - val_loss: 1.2315 - val_accuracy: 0.5458\nEpoch 58/100\n938/938 [==============================] - 33s 35ms/step - loss: 0.8917 - accuracy: 0.6373 - val_loss: 1.2483 - val_accuracy: 0.5486\nEpoch 59/100\n938/938 [==============================] - 26s 28ms/step - loss: 0.8835 - accuracy: 0.6439 - val_loss: 1.2449 - val_accuracy: 0.5416\nEpoch 60/100\n938/938 [==============================] - 28s 29ms/step - loss: 0.8753 - accuracy: 0.6485 - val_loss: 1.2665 - val_accuracy: 0.5423\nEpoch 61/100\n938/938 [==============================] - 33s 36ms/step - loss: 0.8645 - accuracy: 0.6523 - val_loss: 1.2580 - val_accuracy: 0.5386\nEpoch 62/100\n938/938 [==============================] - 25s 27ms/step - loss: 0.8575 - accuracy: 0.6532 - val_loss: 1.2816 - val_accuracy: 0.5310\nEpoch 63/100\n938/938 [==============================] - 24s 26ms/step - loss: 0.8495 - accuracy: 0.6576 - val_loss: 1.2800 - val_accuracy: 0.5400\nEpoch 64/100\n938/938 [==============================] - 27s 29ms/step - loss: 0.8482 - accuracy: 0.6560 - val_loss: 1.3019 - val_accuracy: 0.5357\nEpoch 65/100\n938/938 [==============================] - 33s 35ms/step - loss: 0.8382 - accuracy: 0.6617 - val_loss: 1.3005 - val_accuracy: 0.5321\nEpoch 66/100\n938/938 [==============================] - 32s 35ms/step - loss: 0.8280 - accuracy: 0.6646 - val_loss: 1.3164 - val_accuracy: 0.5264\nEpoch 67/100\n938/938 [==============================] - 30s 32ms/step - loss: 0.8241 - accuracy: 0.6655 - val_loss: 1.3032 - val_accuracy: 0.5408\nEpoch 68/100\n938/938 [==============================] - 28s 30ms/step - loss: 0.8093 - accuracy: 0.6714 - val_loss: 1.3385 - val_accuracy: 0.5265\nEpoch 69/100\n938/938 [==============================] - 33s 35ms/step - loss: 0.8048 - accuracy: 0.6743 - val_loss: 1.3276 - val_accuracy: 0.5294\nEpoch 70/100\n938/938 [==============================] - 32s 34ms/step - loss: 0.7939 - accuracy: 0.6818 - val_loss: 1.3556 - val_accuracy: 0.5273\nEpoch 71/100\n938/938 [==============================] - 48s 51ms/step - loss: 0.7850 - accuracy: 0.6820 - val_loss: 1.3499 - val_accuracy: 0.5262\nEpoch 72/100\n938/938 [==============================] - 40s 43ms/step - loss: 0.7822 - accuracy: 0.6855 - val_loss: 1.3756 - val_accuracy: 0.5324\nEpoch 73/100\n938/938 [==============================] - 35s 37ms/step - loss: 0.7702 - accuracy: 0.6876 - val_loss: 1.3831 - val_accuracy: 0.5281\nEpoch 74/100\n938/938 [==============================] - 44s 47ms/step - loss: 0.7633 - accuracy: 0.6910 - val_loss: 1.3949 - val_accuracy: 0.5298\nEpoch 75/100\n938/938 [==============================] - 34s 37ms/step - loss: 0.7575 - accuracy: 0.6924 - val_loss: 1.4217 - val_accuracy: 0.5147\nEpoch 76/100\n938/938 [==============================] - 29s 30ms/step - loss: 0.7407 - accuracy: 0.7009 - val_loss: 1.4197 - val_accuracy: 0.5269\nEpoch 77/100\n938/938 [==============================] - 42s 45ms/step - loss: 0.7378 - accuracy: 0.6988 - val_loss: 1.4158 - val_accuracy: 0.5239\nEpoch 78/100\n938/938 [==============================] - 28s 30ms/step - loss: 0.7257 - accuracy: 0.7082 - val_loss: 1.4564 - val_accuracy: 0.5232\nEpoch 79/100\n938/938 [==============================] - 30s 32ms/step - loss: 0.7236 - accuracy: 0.7075 - val_loss: 1.4589 - val_accuracy: 0.5206\nEpoch 80/100\n938/938 [==============================] - 31s 33ms/step - loss: 0.7069 - accuracy: 0.7148 - val_loss: 1.4697 - val_accuracy: 0.5177\nEpoch 81/100\n938/938 [==============================] - 32s 34ms/step - loss: 0.7051 - accuracy: 0.7154 - val_loss: 1.4642 - val_accuracy: 0.5095\nEpoch 82/100\n938/938 [==============================] - 31s 33ms/step - loss: 0.6911 - accuracy: 0.7200 - val_loss: 1.4757 - val_accuracy: 0.5167\nEpoch 83/100\n938/938 [==============================] - 32s 34ms/step - loss: 0.6907 - accuracy: 0.7209 - val_loss: 1.5008 - val_accuracy: 0.5166\nEpoch 84/100\n938/938 [==============================] - 28s 30ms/step - loss: 0.6886 - accuracy: 0.7230 - val_loss: 1.5180 - val_accuracy: 0.5130\nEpoch 85/100\n938/938 [==============================] - 34s 37ms/step - loss: 0.6636 - accuracy: 0.7320 - val_loss: 1.5450 - val_accuracy: 0.5092\nEpoch 86/100\n938/938 [==============================] - 30s 32ms/step - loss: 0.6649 - accuracy: 0.7358 - val_loss: 1.5653 - val_accuracy: 0.5090\nEpoch 87/100\n938/938 [==============================] - 31s 33ms/step - loss: 0.6590 - accuracy: 0.7333 - val_loss: 1.5641 - val_accuracy: 0.5122\nEpoch 88/100\n938/938 [==============================] - 31s 33ms/step - loss: 0.6449 - accuracy: 0.7413 - val_loss: 1.5818 - val_accuracy: 0.5126\nEpoch 89/100\n938/938 [==============================] - 46s 49ms/step - loss: 0.6472 - accuracy: 0.7375 - val_loss: 1.5840 - val_accuracy: 0.5051\nEpoch 90/100\n938/938 [==============================] - 31s 33ms/step - loss: 0.6412 - accuracy: 0.7424 - val_loss: 1.6284 - val_accuracy: 0.5154\nEpoch 91/100\n938/938 [==============================] - 33s 36ms/step - loss: 0.6277 - accuracy: 0.7461 - val_loss: 1.6477 - val_accuracy: 0.5075\nEpoch 92/100\n938/938 [==============================] - 39s 42ms/step - loss: 0.6276 - accuracy: 0.7478 - val_loss: 1.6302 - val_accuracy: 0.5080\nEpoch 93/100\n938/938 [==============================] - 41s 44ms/step - loss: 0.6156 - accuracy: 0.7498 - val_loss: 1.6615 - val_accuracy: 0.5027\nEpoch 94/100\n938/938 [==============================] - 36s 38ms/step - loss: 0.6084 - accuracy: 0.7546 - val_loss: 1.6428 - val_accuracy: 0.5009\nEpoch 95/100\n938/938 [==============================] - 30s 32ms/step - loss: 0.5992 - accuracy: 0.7600 - val_loss: 1.6777 - val_accuracy: 0.5079\nEpoch 96/100\n938/938 [==============================] - 25s 26ms/step - loss: 0.5974 - accuracy: 0.7604 - val_loss: 1.6763 - val_accuracy: 0.5057\nEpoch 97/100\n938/938 [==============================] - 30s 32ms/step - loss: 0.5973 - accuracy: 0.7571 - val_loss: 1.7205 - val_accuracy: 0.5042\nEpoch 98/100\n938/938 [==============================] - 29s 31ms/step - loss: 0.5868 - accuracy: 0.7634 - val_loss: 1.7277 - val_accuracy: 0.5027\nEpoch 99/100\n938/938 [==============================] - 31s 33ms/step - loss: 0.5781 - accuracy: 0.7669 - val_loss: 1.7330 - val_accuracy: 0.4979\nEpoch 100/100\n938/938 [==============================] - 34s 36ms/step - loss: 0.5850 - accuracy: 0.7651 - val_loss: 1.7448 - val_accuracy: 0.5016\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 11,
      "block_group": "d9df173d0ed04c32b3c005aae3de3ab3",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model1b2.evaluate(X_train1b, y_train1b))\nprint(\"Test Accuracy:\", model1b2.evaluate(X_test1b, y_test1b))",
      "metadata": {
        "cell_id": "def17b6c06e9494193d5f1624708ff53",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "938/938 [==============================] - 8s 8ms/step - loss: 0.5711 - accuracy: 0.7712\nTraining Accuracy: [0.5711295008659363, 0.7712000012397766]\n313/313 [==============================] - 3s 8ms/step - loss: 1.7273 - accuracy: 0.5023\nTest Accuracy: [1.7272535562515259, 0.5023000240325928]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 12,
      "block_group": "25f703d76d9e4380865357ed9371cac5",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "scaler1 = preprocessing.MinMaxScaler()\nscaled_feat1 = scaler1.fit_transform(data1[features])\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(scaled_feat1, data1[\"music_genre_num\"], test_size=0.2, random_state=1)\nX_train2, X_val2, y_train2, y_val2 = train_test_split(X_train2, y_train2, test_size=0.25, random_state=1)",
      "metadata": {
        "cell_id": "67c26f07d0bc4888854ffa7210d252b6",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": 13,
      "block_group": "b06bd0a62cea4be084b1062ebe031f3d",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "model2 = Sequential()\n#forget gates = 0, paththrough gate = 1\nmodel2.add(LSTM(units=128,input_shape=(24, 1)))\nmodel2.add(Dense(10, activation = \"softmax\"))\n\n#bidirectional, attention block (inform importance of other positions)- layers.attention\n\nmodel2.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\nhistory2 = model2.fit(X_train2, y_train2, validation_data=(X_val2, y_val2), batch_size=32, epochs=100)",
      "metadata": {
        "cell_id": "1f197651b45944a3bb91b80c5765c2fd",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n938/938 [==============================] - 35s 35ms/step - loss: 2.1250 - accuracy: 0.2064 - val_loss: 2.0174 - val_accuracy: 0.2521\nEpoch 2/100\n938/938 [==============================] - 28s 30ms/step - loss: 1.7712 - accuracy: 0.3427 - val_loss: 1.5663 - val_accuracy: 0.4027\nEpoch 3/100\n938/938 [==============================] - 31s 33ms/step - loss: 1.4581 - accuracy: 0.4423 - val_loss: 1.4072 - val_accuracy: 0.4596\nEpoch 4/100\n938/938 [==============================] - 27s 29ms/step - loss: 1.3974 - accuracy: 0.4649 - val_loss: 1.3714 - val_accuracy: 0.4668\nEpoch 5/100\n938/938 [==============================] - 25s 27ms/step - loss: 1.3587 - accuracy: 0.4782 - val_loss: 1.3226 - val_accuracy: 0.4858\nEpoch 6/100\n938/938 [==============================] - 24s 26ms/step - loss: 1.3214 - accuracy: 0.4915 - val_loss: 1.3348 - val_accuracy: 0.4762\nEpoch 7/100\n938/938 [==============================] - 29s 31ms/step - loss: 1.2893 - accuracy: 0.4988 - val_loss: 1.2745 - val_accuracy: 0.5088\nEpoch 8/100\n938/938 [==============================] - 26s 28ms/step - loss: 1.2651 - accuracy: 0.5095 - val_loss: 1.2531 - val_accuracy: 0.5100\nEpoch 9/100\n938/938 [==============================] - 19s 20ms/step - loss: 1.2375 - accuracy: 0.5161 - val_loss: 1.2952 - val_accuracy: 0.4960\nEpoch 10/100\n938/938 [==============================] - 19s 20ms/step - loss: 1.2283 - accuracy: 0.5208 - val_loss: 1.2309 - val_accuracy: 0.5243\nEpoch 11/100\n938/938 [==============================] - 26s 28ms/step - loss: 1.2139 - accuracy: 0.5281 - val_loss: 1.2141 - val_accuracy: 0.5210\nEpoch 12/100\n938/938 [==============================] - 26s 28ms/step - loss: 1.2041 - accuracy: 0.5308 - val_loss: 1.2051 - val_accuracy: 0.5315\nEpoch 13/100\n938/938 [==============================] - 23s 24ms/step - loss: 1.1942 - accuracy: 0.5362 - val_loss: 1.2340 - val_accuracy: 0.5229\nEpoch 14/100\n938/938 [==============================] - 22s 24ms/step - loss: 1.1869 - accuracy: 0.5397 - val_loss: 1.2179 - val_accuracy: 0.5282\nEpoch 15/100\n938/938 [==============================] - 28s 30ms/step - loss: 1.1785 - accuracy: 0.5420 - val_loss: 1.1869 - val_accuracy: 0.5350\nEpoch 16/100\n938/938 [==============================] - 19s 21ms/step - loss: 1.1670 - accuracy: 0.5426 - val_loss: 1.1850 - val_accuracy: 0.5388\nEpoch 17/100\n938/938 [==============================] - 17s 18ms/step - loss: 1.1617 - accuracy: 0.5469 - val_loss: 1.1863 - val_accuracy: 0.5346\nEpoch 18/100\n938/938 [==============================] - 20s 22ms/step - loss: 1.1543 - accuracy: 0.5483 - val_loss: 1.1742 - val_accuracy: 0.5441\nEpoch 19/100\n938/938 [==============================] - 18s 19ms/step - loss: 1.1459 - accuracy: 0.5513 - val_loss: 1.2187 - val_accuracy: 0.5285\nEpoch 20/100\n938/938 [==============================] - 16s 17ms/step - loss: 1.1409 - accuracy: 0.5526 - val_loss: 1.1811 - val_accuracy: 0.5336\nEpoch 21/100\n938/938 [==============================] - 20s 21ms/step - loss: 1.1363 - accuracy: 0.5536 - val_loss: 1.1574 - val_accuracy: 0.5515\nEpoch 22/100\n938/938 [==============================] - 20s 21ms/step - loss: 1.1327 - accuracy: 0.5561 - val_loss: 1.1809 - val_accuracy: 0.5432\nEpoch 23/100\n938/938 [==============================] - 21s 22ms/step - loss: 1.1263 - accuracy: 0.5556 - val_loss: 1.1564 - val_accuracy: 0.5471\nEpoch 24/100\n938/938 [==============================] - 36s 39ms/step - loss: 1.1196 - accuracy: 0.5639 - val_loss: 1.1791 - val_accuracy: 0.5381\nEpoch 25/100\n938/938 [==============================] - 23s 24ms/step - loss: 1.1189 - accuracy: 0.5620 - val_loss: 1.1592 - val_accuracy: 0.5469\nEpoch 26/100\n938/938 [==============================] - 25s 26ms/step - loss: 1.1101 - accuracy: 0.5649 - val_loss: 1.1415 - val_accuracy: 0.5562\nEpoch 27/100\n938/938 [==============================] - 23s 24ms/step - loss: 1.1082 - accuracy: 0.5602 - val_loss: 1.1669 - val_accuracy: 0.5492\nEpoch 28/100\n938/938 [==============================] - 20s 21ms/step - loss: 1.1033 - accuracy: 0.5639 - val_loss: 1.1466 - val_accuracy: 0.5501\nEpoch 29/100\n938/938 [==============================] - 23s 24ms/step - loss: 1.0970 - accuracy: 0.5671 - val_loss: 1.1546 - val_accuracy: 0.5390\nEpoch 30/100\n938/938 [==============================] - 26s 28ms/step - loss: 1.0963 - accuracy: 0.5635 - val_loss: 1.1579 - val_accuracy: 0.5526\nEpoch 31/100\n938/938 [==============================] - 25s 26ms/step - loss: 1.0887 - accuracy: 0.5699 - val_loss: 1.1905 - val_accuracy: 0.5397\nEpoch 32/100\n938/938 [==============================] - 19s 21ms/step - loss: 1.0863 - accuracy: 0.5705 - val_loss: 1.1356 - val_accuracy: 0.5555\nEpoch 33/100\n938/938 [==============================] - 21s 22ms/step - loss: 1.0804 - accuracy: 0.5726 - val_loss: 1.1325 - val_accuracy: 0.5550\nEpoch 34/100\n938/938 [==============================] - 21s 23ms/step - loss: 1.0754 - accuracy: 0.5753 - val_loss: 1.1403 - val_accuracy: 0.5586\nEpoch 35/100\n938/938 [==============================] - 25s 26ms/step - loss: 1.0705 - accuracy: 0.5771 - val_loss: 1.1447 - val_accuracy: 0.5552\nEpoch 36/100\n938/938 [==============================] - 21s 23ms/step - loss: 1.0662 - accuracy: 0.5785 - val_loss: 1.1574 - val_accuracy: 0.5503\nEpoch 37/100\n938/938 [==============================] - 21s 23ms/step - loss: 1.0658 - accuracy: 0.5763 - val_loss: 1.1438 - val_accuracy: 0.5489\nEpoch 38/100\n938/938 [==============================] - 22s 23ms/step - loss: 1.0574 - accuracy: 0.5794 - val_loss: 1.1589 - val_accuracy: 0.5510\nEpoch 39/100\n938/938 [==============================] - 19s 20ms/step - loss: 1.0552 - accuracy: 0.5824 - val_loss: 1.1507 - val_accuracy: 0.5551\nEpoch 40/100\n938/938 [==============================] - 22s 23ms/step - loss: 1.0520 - accuracy: 0.5848 - val_loss: 1.1652 - val_accuracy: 0.5525\nEpoch 41/100\n938/938 [==============================] - 23s 24ms/step - loss: 1.0459 - accuracy: 0.5825 - val_loss: 1.1424 - val_accuracy: 0.5557\nEpoch 42/100\n938/938 [==============================] - 21s 23ms/step - loss: 1.0407 - accuracy: 0.5855 - val_loss: 1.1343 - val_accuracy: 0.5562\nEpoch 43/100\n938/938 [==============================] - 24s 25ms/step - loss: 1.0357 - accuracy: 0.5888 - val_loss: 1.1367 - val_accuracy: 0.5554\nEpoch 44/100\n938/938 [==============================] - 19s 20ms/step - loss: 1.0321 - accuracy: 0.5899 - val_loss: 1.1453 - val_accuracy: 0.5509\nEpoch 45/100\n938/938 [==============================] - 21s 22ms/step - loss: 1.0256 - accuracy: 0.5871 - val_loss: 1.1839 - val_accuracy: 0.5424\nEpoch 46/100\n938/938 [==============================] - 22s 24ms/step - loss: 1.0239 - accuracy: 0.5926 - val_loss: 1.1594 - val_accuracy: 0.5439\nEpoch 47/100\n938/938 [==============================] - 22s 24ms/step - loss: 1.0140 - accuracy: 0.5975 - val_loss: 1.1524 - val_accuracy: 0.5472\nEpoch 48/100\n938/938 [==============================] - 21s 22ms/step - loss: 1.0097 - accuracy: 0.5978 - val_loss: 1.1832 - val_accuracy: 0.5390\nEpoch 49/100\n938/938 [==============================] - 19s 20ms/step - loss: 1.0032 - accuracy: 0.5998 - val_loss: 1.1574 - val_accuracy: 0.5548\nEpoch 50/100\n938/938 [==============================] - 19s 21ms/step - loss: 1.0000 - accuracy: 0.6001 - val_loss: 1.1599 - val_accuracy: 0.5492\nEpoch 51/100\n938/938 [==============================] - 22s 24ms/step - loss: 0.9936 - accuracy: 0.6035 - val_loss: 1.1576 - val_accuracy: 0.5488\nEpoch 52/100\n938/938 [==============================] - 24s 26ms/step - loss: 0.9848 - accuracy: 0.6041 - val_loss: 1.1714 - val_accuracy: 0.5522\nEpoch 53/100\n938/938 [==============================] - 19s 20ms/step - loss: 0.9845 - accuracy: 0.6046 - val_loss: 1.1660 - val_accuracy: 0.5490\nEpoch 54/100\n938/938 [==============================] - 24s 26ms/step - loss: 0.9770 - accuracy: 0.6109 - val_loss: 1.1783 - val_accuracy: 0.5489\nEpoch 55/100\n938/938 [==============================] - 20s 21ms/step - loss: 0.9681 - accuracy: 0.6126 - val_loss: 1.1682 - val_accuracy: 0.5474\nEpoch 56/100\n938/938 [==============================] - 23s 24ms/step - loss: 0.9652 - accuracy: 0.6118 - val_loss: 1.1807 - val_accuracy: 0.5448\nEpoch 57/100\n938/938 [==============================] - 19s 20ms/step - loss: 0.9575 - accuracy: 0.6140 - val_loss: 1.1845 - val_accuracy: 0.5502\nEpoch 58/100\n938/938 [==============================] - 19s 21ms/step - loss: 0.9524 - accuracy: 0.6169 - val_loss: 1.2000 - val_accuracy: 0.5401\nEpoch 59/100\n938/938 [==============================] - 18s 19ms/step - loss: 0.9430 - accuracy: 0.6219 - val_loss: 1.1811 - val_accuracy: 0.5513\nEpoch 60/100\n938/938 [==============================] - 18s 20ms/step - loss: 0.9333 - accuracy: 0.6254 - val_loss: 1.2033 - val_accuracy: 0.5434\nEpoch 61/100\n938/938 [==============================] - 22s 24ms/step - loss: 0.9320 - accuracy: 0.6270 - val_loss: 1.1854 - val_accuracy: 0.5416\nEpoch 62/100\n938/938 [==============================] - 26s 27ms/step - loss: 0.9221 - accuracy: 0.6289 - val_loss: 1.1913 - val_accuracy: 0.5421\nEpoch 63/100\n938/938 [==============================] - 24s 26ms/step - loss: 0.9151 - accuracy: 0.6304 - val_loss: 1.2088 - val_accuracy: 0.5394\nEpoch 64/100\n938/938 [==============================] - 22s 23ms/step - loss: 0.9084 - accuracy: 0.6335 - val_loss: 1.2122 - val_accuracy: 0.5425\nEpoch 65/100\n938/938 [==============================] - 21s 22ms/step - loss: 0.8948 - accuracy: 0.6386 - val_loss: 1.2360 - val_accuracy: 0.5365\nEpoch 66/100\n938/938 [==============================] - 21s 22ms/step - loss: 0.8957 - accuracy: 0.6400 - val_loss: 1.2317 - val_accuracy: 0.5333\nEpoch 67/100\n938/938 [==============================] - 18s 19ms/step - loss: 0.8833 - accuracy: 0.6432 - val_loss: 1.2143 - val_accuracy: 0.5428\nEpoch 68/100\n938/938 [==============================] - 30s 32ms/step - loss: 0.8786 - accuracy: 0.6411 - val_loss: 1.2339 - val_accuracy: 0.5334\nEpoch 69/100\n938/938 [==============================] - 20s 22ms/step - loss: 0.8684 - accuracy: 0.6495 - val_loss: 1.2462 - val_accuracy: 0.5366\nEpoch 70/100\n938/938 [==============================] - 27s 29ms/step - loss: 0.8580 - accuracy: 0.6517 - val_loss: 1.2572 - val_accuracy: 0.5351\nEpoch 71/100\n938/938 [==============================] - 27s 29ms/step - loss: 0.8524 - accuracy: 0.6540 - val_loss: 1.2580 - val_accuracy: 0.5309\nEpoch 72/100\n938/938 [==============================] - 25s 26ms/step - loss: 0.8394 - accuracy: 0.6606 - val_loss: 1.2765 - val_accuracy: 0.5337\nEpoch 73/100\n938/938 [==============================] - 25s 27ms/step - loss: 0.8393 - accuracy: 0.6594 - val_loss: 1.2851 - val_accuracy: 0.5251\nEpoch 74/100\n938/938 [==============================] - 26s 28ms/step - loss: 0.8260 - accuracy: 0.6664 - val_loss: 1.2923 - val_accuracy: 0.5334\nEpoch 75/100\n938/938 [==============================] - 17s 18ms/step - loss: 0.8161 - accuracy: 0.6689 - val_loss: 1.2918 - val_accuracy: 0.5304\nEpoch 76/100\n938/938 [==============================] - 18s 19ms/step - loss: 0.8070 - accuracy: 0.6741 - val_loss: 1.2882 - val_accuracy: 0.5329\nEpoch 77/100\n938/938 [==============================] - 16s 18ms/step - loss: 0.7994 - accuracy: 0.6777 - val_loss: 1.2985 - val_accuracy: 0.5347\nEpoch 78/100\n938/938 [==============================] - 17s 18ms/step - loss: 0.7855 - accuracy: 0.6821 - val_loss: 1.3254 - val_accuracy: 0.5252\nEpoch 79/100\n938/938 [==============================] - 17s 19ms/step - loss: 0.7804 - accuracy: 0.6841 - val_loss: 1.3398 - val_accuracy: 0.5265\nEpoch 80/100\n938/938 [==============================] - 17s 18ms/step - loss: 0.7730 - accuracy: 0.6870 - val_loss: 1.3494 - val_accuracy: 0.5244\nEpoch 81/100\n938/938 [==============================] - 18s 20ms/step - loss: 0.7571 - accuracy: 0.6933 - val_loss: 1.3532 - val_accuracy: 0.5265\nEpoch 82/100\n938/938 [==============================] - 16s 17ms/step - loss: 0.7535 - accuracy: 0.6940 - val_loss: 1.3697 - val_accuracy: 0.5234\nEpoch 83/100\n938/938 [==============================] - 17s 18ms/step - loss: 0.7413 - accuracy: 0.7025 - val_loss: 1.3856 - val_accuracy: 0.5231\nEpoch 84/100\n938/938 [==============================] - 17s 18ms/step - loss: 0.7367 - accuracy: 0.7034 - val_loss: 1.3860 - val_accuracy: 0.5204\nEpoch 85/100\n938/938 [==============================] - 19s 20ms/step - loss: 0.7273 - accuracy: 0.7042 - val_loss: 1.4111 - val_accuracy: 0.5211\nEpoch 86/100\n938/938 [==============================] - 16s 17ms/step - loss: 0.7107 - accuracy: 0.7145 - val_loss: 1.4274 - val_accuracy: 0.5131\nEpoch 87/100\n938/938 [==============================] - 17s 18ms/step - loss: 0.7080 - accuracy: 0.7153 - val_loss: 1.4348 - val_accuracy: 0.5095\nEpoch 88/100\n938/938 [==============================] - 16s 17ms/step - loss: 0.6973 - accuracy: 0.7187 - val_loss: 1.4383 - val_accuracy: 0.5157\nEpoch 89/100\n938/938 [==============================] - 17s 19ms/step - loss: 0.6913 - accuracy: 0.7217 - val_loss: 1.4524 - val_accuracy: 0.5091\nEpoch 90/100\n938/938 [==============================] - 17s 18ms/step - loss: 0.6757 - accuracy: 0.7272 - val_loss: 1.4799 - val_accuracy: 0.5133\nEpoch 91/100\n938/938 [==============================] - 18s 19ms/step - loss: 0.6650 - accuracy: 0.7342 - val_loss: 1.5095 - val_accuracy: 0.5093\nEpoch 92/100\n938/938 [==============================] - 1000s 1s/step - loss: 0.6712 - accuracy: 0.7316 - val_loss: 1.4996 - val_accuracy: 0.5130\nEpoch 93/100\n938/938 [==============================] - 14s 15ms/step - loss: 0.6559 - accuracy: 0.7386 - val_loss: 1.5017 - val_accuracy: 0.5065\nEpoch 94/100\n938/938 [==============================] - 13s 14ms/step - loss: 0.6445 - accuracy: 0.7395 - val_loss: 1.5073 - val_accuracy: 0.5153\nEpoch 95/100\n938/938 [==============================] - 11s 12ms/step - loss: 0.6361 - accuracy: 0.7439 - val_loss: 1.5327 - val_accuracy: 0.5086\nEpoch 96/100\n938/938 [==============================] - 14s 15ms/step - loss: 0.6206 - accuracy: 0.7508 - val_loss: 1.5511 - val_accuracy: 0.5140\nEpoch 97/100\n938/938 [==============================] - 12s 13ms/step - loss: 0.6196 - accuracy: 0.7539 - val_loss: 1.5459 - val_accuracy: 0.5086\nEpoch 98/100\n938/938 [==============================] - 14s 15ms/step - loss: 0.6075 - accuracy: 0.7546 - val_loss: 1.5908 - val_accuracy: 0.5072\nEpoch 99/100\n938/938 [==============================] - 13s 14ms/step - loss: 0.5980 - accuracy: 0.7590 - val_loss: 1.5882 - val_accuracy: 0.5036\nEpoch 100/100\n938/938 [==============================] - 12s 13ms/step - loss: 0.5930 - accuracy: 0.7632 - val_loss: 1.5967 - val_accuracy: 0.5046\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 14,
      "block_group": "04ff0da89f884d7b8a37700b66b1d49d",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model2.evaluate(X_train2, y_train2))\nprint(\"Test Accuracy:\", model2.evaluate(X_test2, y_test2))",
      "metadata": {
        "cell_id": "0de8efe222c240feabd26e05d5dd5fe5",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "938/938 [==============================] - 4s 4ms/step - loss: 0.5500 - accuracy: 0.7856\nTraining Accuracy: [0.5499528050422668, 0.7856000065803528]\n313/313 [==============================] - 2s 5ms/step - loss: 1.5875 - accuracy: 0.5095\nTest Accuracy: [1.5874824523925781, 0.5095000267028809]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 15,
      "block_group": "0d5a50cc8e92438ca029c581416f1c64",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "model3 = Sequential()\n#forget gates = 0, paththrough gate = 1\n\nmodel3.add((Bidirectional(LSTM(units=128, input_shape=(24, 1)))))\nmodel3.add(Dense(10, activation = \"softmax\"))\n\n#bidirectional, attention block (inform importance of other positions)- layers.attention\n\nmodel3.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\nhistory3 = model3.fit(np.expand_dims(X_train2, 1), y_train2, validation_data=(np.expand_dims(X_val2, 1), y_val2), batch_size=32, epochs=100)",
      "metadata": {
        "cell_id": "c58413466ac442819360e65a19cb9fd6",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n938/938 [==============================] - 180s 188ms/step - loss: 1.6104 - accuracy: 0.4168 - val_loss: 1.3468 - val_accuracy: 0.5060\nEpoch 2/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.3037 - accuracy: 0.5108 - val_loss: 1.2940 - val_accuracy: 0.5115\nEpoch 3/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.2718 - accuracy: 0.5196 - val_loss: 1.2824 - val_accuracy: 0.5172\nEpoch 4/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.2557 - accuracy: 0.5258 - val_loss: 1.2722 - val_accuracy: 0.5182\nEpoch 5/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.2466 - accuracy: 0.5290 - val_loss: 1.2662 - val_accuracy: 0.5224\nEpoch 6/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.2403 - accuracy: 0.5330 - val_loss: 1.2570 - val_accuracy: 0.5326\nEpoch 7/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.2343 - accuracy: 0.5327 - val_loss: 1.2524 - val_accuracy: 0.5302\nEpoch 8/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.2279 - accuracy: 0.5367 - val_loss: 1.2447 - val_accuracy: 0.5366\nEpoch 9/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.2210 - accuracy: 0.5364 - val_loss: 1.2512 - val_accuracy: 0.5259\nEpoch 10/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.2162 - accuracy: 0.5395 - val_loss: 1.2460 - val_accuracy: 0.5370\nEpoch 11/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.2125 - accuracy: 0.5407 - val_loss: 1.2336 - val_accuracy: 0.5321\nEpoch 12/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.2070 - accuracy: 0.5423 - val_loss: 1.2325 - val_accuracy: 0.5453\nEpoch 13/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.2030 - accuracy: 0.5431 - val_loss: 1.2266 - val_accuracy: 0.5331\nEpoch 14/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.2008 - accuracy: 0.5433 - val_loss: 1.2204 - val_accuracy: 0.5375\nEpoch 15/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1952 - accuracy: 0.5440 - val_loss: 1.2255 - val_accuracy: 0.5365\nEpoch 16/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1931 - accuracy: 0.5459 - val_loss: 1.2220 - val_accuracy: 0.5397\nEpoch 17/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1892 - accuracy: 0.5474 - val_loss: 1.2213 - val_accuracy: 0.5418\nEpoch 18/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1856 - accuracy: 0.5470 - val_loss: 1.2175 - val_accuracy: 0.5377\nEpoch 19/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1833 - accuracy: 0.5502 - val_loss: 1.2145 - val_accuracy: 0.5404\nEpoch 20/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1804 - accuracy: 0.5483 - val_loss: 1.2089 - val_accuracy: 0.5490\nEpoch 21/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1792 - accuracy: 0.5495 - val_loss: 1.2131 - val_accuracy: 0.5360\nEpoch 22/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1760 - accuracy: 0.5507 - val_loss: 1.2061 - val_accuracy: 0.5489\nEpoch 23/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.1741 - accuracy: 0.5524 - val_loss: 1.2064 - val_accuracy: 0.5392\nEpoch 24/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1710 - accuracy: 0.5507 - val_loss: 1.2100 - val_accuracy: 0.5338\nEpoch 25/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1671 - accuracy: 0.5541 - val_loss: 1.2104 - val_accuracy: 0.5469\nEpoch 26/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1675 - accuracy: 0.5542 - val_loss: 1.2008 - val_accuracy: 0.5545\nEpoch 27/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1633 - accuracy: 0.5550 - val_loss: 1.2029 - val_accuracy: 0.5560\nEpoch 28/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1622 - accuracy: 0.5535 - val_loss: 1.1925 - val_accuracy: 0.5510\nEpoch 29/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1599 - accuracy: 0.5552 - val_loss: 1.2006 - val_accuracy: 0.5404\nEpoch 30/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1576 - accuracy: 0.5535 - val_loss: 1.1962 - val_accuracy: 0.5512\nEpoch 31/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1562 - accuracy: 0.5564 - val_loss: 1.1890 - val_accuracy: 0.5523\nEpoch 32/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1529 - accuracy: 0.5585 - val_loss: 1.1943 - val_accuracy: 0.5479\nEpoch 33/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1517 - accuracy: 0.5589 - val_loss: 1.1889 - val_accuracy: 0.5523\nEpoch 34/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1487 - accuracy: 0.5594 - val_loss: 1.1858 - val_accuracy: 0.5530\nEpoch 35/100\n938/938 [==============================] - 2s 3ms/step - loss: 1.1465 - accuracy: 0.5612 - val_loss: 1.1818 - val_accuracy: 0.5595\nEpoch 36/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1455 - accuracy: 0.5596 - val_loss: 1.1826 - val_accuracy: 0.5530\nEpoch 37/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1432 - accuracy: 0.5630 - val_loss: 1.1855 - val_accuracy: 0.5473\nEpoch 38/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1407 - accuracy: 0.5601 - val_loss: 1.1828 - val_accuracy: 0.5486\nEpoch 39/100\n938/938 [==============================] - 2s 3ms/step - loss: 1.1394 - accuracy: 0.5618 - val_loss: 1.1824 - val_accuracy: 0.5454\nEpoch 40/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1367 - accuracy: 0.5619 - val_loss: 1.1888 - val_accuracy: 0.5496\nEpoch 41/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1354 - accuracy: 0.5629 - val_loss: 1.1786 - val_accuracy: 0.5526\nEpoch 42/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1323 - accuracy: 0.5660 - val_loss: 1.1836 - val_accuracy: 0.5455\nEpoch 43/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1314 - accuracy: 0.5631 - val_loss: 1.1730 - val_accuracy: 0.5600\nEpoch 44/100\n938/938 [==============================] - 5s 5ms/step - loss: 1.1289 - accuracy: 0.5681 - val_loss: 1.1687 - val_accuracy: 0.5547\nEpoch 45/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.1268 - accuracy: 0.5675 - val_loss: 1.1740 - val_accuracy: 0.5568\nEpoch 46/100\n938/938 [==============================] - 9s 10ms/step - loss: 1.1251 - accuracy: 0.5688 - val_loss: 1.1702 - val_accuracy: 0.5577\nEpoch 47/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1242 - accuracy: 0.5686 - val_loss: 1.1706 - val_accuracy: 0.5509\nEpoch 48/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1209 - accuracy: 0.5710 - val_loss: 1.1728 - val_accuracy: 0.5511\nEpoch 49/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1193 - accuracy: 0.5707 - val_loss: 1.1770 - val_accuracy: 0.5543\nEpoch 50/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1175 - accuracy: 0.5718 - val_loss: 1.1690 - val_accuracy: 0.5529\nEpoch 51/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1155 - accuracy: 0.5701 - val_loss: 1.1668 - val_accuracy: 0.5597\nEpoch 52/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1144 - accuracy: 0.5705 - val_loss: 1.1651 - val_accuracy: 0.5566\nEpoch 53/100\n938/938 [==============================] - 7s 7ms/step - loss: 1.1122 - accuracy: 0.5718 - val_loss: 1.1656 - val_accuracy: 0.5573\nEpoch 54/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1114 - accuracy: 0.5735 - val_loss: 1.1630 - val_accuracy: 0.5586\nEpoch 55/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.1090 - accuracy: 0.5727 - val_loss: 1.1663 - val_accuracy: 0.5502\nEpoch 56/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1074 - accuracy: 0.5740 - val_loss: 1.1625 - val_accuracy: 0.5539\nEpoch 57/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1048 - accuracy: 0.5731 - val_loss: 1.1670 - val_accuracy: 0.5565\nEpoch 58/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1042 - accuracy: 0.5723 - val_loss: 1.1576 - val_accuracy: 0.5574\nEpoch 59/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.1021 - accuracy: 0.5747 - val_loss: 1.1544 - val_accuracy: 0.5589\nEpoch 60/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1006 - accuracy: 0.5773 - val_loss: 1.1590 - val_accuracy: 0.5514\nEpoch 61/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.0983 - accuracy: 0.5775 - val_loss: 1.1599 - val_accuracy: 0.5559\nEpoch 62/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0973 - accuracy: 0.5758 - val_loss: 1.1640 - val_accuracy: 0.5535\nEpoch 63/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0959 - accuracy: 0.5758 - val_loss: 1.1546 - val_accuracy: 0.5563\nEpoch 64/100\n938/938 [==============================] - 2s 3ms/step - loss: 1.0944 - accuracy: 0.5784 - val_loss: 1.1633 - val_accuracy: 0.5535\nEpoch 65/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0923 - accuracy: 0.5777 - val_loss: 1.1568 - val_accuracy: 0.5539\nEpoch 66/100\n938/938 [==============================] - 4s 5ms/step - loss: 1.0912 - accuracy: 0.5778 - val_loss: 1.1492 - val_accuracy: 0.5591\nEpoch 67/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0889 - accuracy: 0.5799 - val_loss: 1.1548 - val_accuracy: 0.5593\nEpoch 68/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.0873 - accuracy: 0.5800 - val_loss: 1.1519 - val_accuracy: 0.5604\nEpoch 69/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0858 - accuracy: 0.5791 - val_loss: 1.1536 - val_accuracy: 0.5570\nEpoch 70/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0850 - accuracy: 0.5820 - val_loss: 1.1495 - val_accuracy: 0.5647\nEpoch 71/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0828 - accuracy: 0.5806 - val_loss: 1.1529 - val_accuracy: 0.5602\nEpoch 72/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.0825 - accuracy: 0.5815 - val_loss: 1.1454 - val_accuracy: 0.5584\nEpoch 73/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.0807 - accuracy: 0.5828 - val_loss: 1.1447 - val_accuracy: 0.5642\nEpoch 74/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.0795 - accuracy: 0.5798 - val_loss: 1.1481 - val_accuracy: 0.5574\nEpoch 75/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.0770 - accuracy: 0.5845 - val_loss: 1.1499 - val_accuracy: 0.5572\nEpoch 76/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.0764 - accuracy: 0.5823 - val_loss: 1.1422 - val_accuracy: 0.5630\nEpoch 77/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.0747 - accuracy: 0.5866 - val_loss: 1.1466 - val_accuracy: 0.5625\nEpoch 78/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.0723 - accuracy: 0.5833 - val_loss: 1.1437 - val_accuracy: 0.5617\nEpoch 79/100\n938/938 [==============================] - 2s 3ms/step - loss: 1.0720 - accuracy: 0.5844 - val_loss: 1.1491 - val_accuracy: 0.5520\nEpoch 80/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0700 - accuracy: 0.5870 - val_loss: 1.1448 - val_accuracy: 0.5581\nEpoch 81/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0687 - accuracy: 0.5864 - val_loss: 1.1510 - val_accuracy: 0.5606\nEpoch 82/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0670 - accuracy: 0.5866 - val_loss: 1.1448 - val_accuracy: 0.5622\nEpoch 83/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0659 - accuracy: 0.5870 - val_loss: 1.1477 - val_accuracy: 0.5598\nEpoch 84/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0649 - accuracy: 0.5860 - val_loss: 1.1450 - val_accuracy: 0.5589\nEpoch 85/100\n938/938 [==============================] - 5s 5ms/step - loss: 1.0641 - accuracy: 0.5871 - val_loss: 1.1475 - val_accuracy: 0.5629\nEpoch 86/100\n938/938 [==============================] - 5s 5ms/step - loss: 1.0610 - accuracy: 0.5896 - val_loss: 1.1432 - val_accuracy: 0.5605\nEpoch 87/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0606 - accuracy: 0.5871 - val_loss: 1.1451 - val_accuracy: 0.5569\nEpoch 88/100\n938/938 [==============================] - 3s 4ms/step - loss: 1.0590 - accuracy: 0.5874 - val_loss: 1.1452 - val_accuracy: 0.5539\nEpoch 89/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0580 - accuracy: 0.5878 - val_loss: 1.1410 - val_accuracy: 0.5654\nEpoch 90/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0568 - accuracy: 0.5881 - val_loss: 1.1440 - val_accuracy: 0.5534\nEpoch 91/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0553 - accuracy: 0.5903 - val_loss: 1.1414 - val_accuracy: 0.5595\nEpoch 92/100\n938/938 [==============================] - 3s 4ms/step - loss: 1.0542 - accuracy: 0.5892 - val_loss: 1.1426 - val_accuracy: 0.5607\nEpoch 93/100\n938/938 [==============================] - 3s 4ms/step - loss: 1.0530 - accuracy: 0.5921 - val_loss: 1.1393 - val_accuracy: 0.5614\nEpoch 94/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0509 - accuracy: 0.5920 - val_loss: 1.1441 - val_accuracy: 0.5541\nEpoch 95/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0517 - accuracy: 0.5944 - val_loss: 1.1363 - val_accuracy: 0.5606\nEpoch 96/100\n938/938 [==============================] - 4s 5ms/step - loss: 1.0493 - accuracy: 0.5924 - val_loss: 1.1403 - val_accuracy: 0.5589\nEpoch 97/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0482 - accuracy: 0.5917 - val_loss: 1.1387 - val_accuracy: 0.5632\nEpoch 98/100\n938/938 [==============================] - 3s 4ms/step - loss: 1.0465 - accuracy: 0.5935 - val_loss: 1.1412 - val_accuracy: 0.5567\nEpoch 99/100\n938/938 [==============================] - 2s 3ms/step - loss: 1.0456 - accuracy: 0.5921 - val_loss: 1.1413 - val_accuracy: 0.5610\nEpoch 100/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0434 - accuracy: 0.5968 - val_loss: 1.1405 - val_accuracy: 0.5595\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 16,
      "block_group": "8a2dbc0f409e4e87a3c2658700da3122",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model3.evaluate(np.expand_dims(X_train2, 1), y_train2))\nprint(\"Test Accuracy:\", model3.evaluate(np.expand_dims(X_test2, 1), y_test2))",
      "metadata": {
        "cell_id": "72eb555325fe4a2a91d669e727682547",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "938/938 [==============================] - 1s 1ms/step - loss: 1.0345 - accuracy: 0.6003\nTraining Accuracy: [1.0344985723495483, 0.6003333330154419]\n313/313 [==============================] - 0s 1ms/step - loss: 1.1183 - accuracy: 0.5665\nTest Accuracy: [1.1183393001556396, 0.5665000081062317]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 17,
      "block_group": "473a6762c8c342a8a70c6c2525a60e70",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(classification_report(y_test2, np.argmax(model3.predict(np.expand_dims(X_test2, 1)), axis =-1)))",
      "metadata": {
        "cell_id": "e5e4130da4a34d589dbd9c2acff1b74f",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "313/313 [==============================] - 1s 767us/step\n              precision    recall  f1-score   support\n\n           0       0.42      0.40      0.41      1005\n           1       0.73      0.71      0.72      1057\n           2       0.51      0.59      0.55       985\n           3       0.83      0.86      0.84       967\n           4       0.56      0.46      0.51       969\n           5       0.70      0.54      0.61      1029\n           6       0.45      0.44      0.45      1012\n           7       0.53      0.52      0.53       996\n           8       0.44      0.49      0.47       982\n           9       0.53      0.65      0.59       998\n\n    accuracy                           0.57     10000\n   macro avg       0.57      0.57      0.57     10000\nweighted avg       0.57      0.57      0.57     10000\n\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 18,
      "block_group": "9226924527514207b3e06150b28cda5e",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "model4 = Sequential()\n#forget gates = 0, paththrough gate = 1\n\nmodel4.add(Bidirectional(LSTM(units=128, input_shape=(24, 1))))\nmodel4.add(Dropout(0.1))\nmodel4.add(Dense(10, activation = \"softmax\"))\n\n#bidirectional, attention block (inform importance of other positions)- layers.attention\n\nmodel4.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\nhistory4 = model4.fit(np.expand_dims(X_train2, 1), y_train2, validation_data=(np.expand_dims(X_val2, 1), y_val2), batch_size=32, epochs=100)",
      "metadata": {
        "cell_id": "3cb67f8ee2814a71b64ddd6106eed82b",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n938/938 [==============================] - 7s 4ms/step - loss: 1.6191 - accuracy: 0.4173 - val_loss: 1.3493 - val_accuracy: 0.4950\nEpoch 2/100\n938/938 [==============================] - 5s 5ms/step - loss: 1.3094 - accuracy: 0.5102 - val_loss: 1.2993 - val_accuracy: 0.5126\nEpoch 3/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.2751 - accuracy: 0.5207 - val_loss: 1.2790 - val_accuracy: 0.5233\nEpoch 4/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.2617 - accuracy: 0.5239 - val_loss: 1.2672 - val_accuracy: 0.5253\nEpoch 5/100\n938/938 [==============================] - 2s 2ms/step - loss: 1.2543 - accuracy: 0.5270 - val_loss: 1.2644 - val_accuracy: 0.5256\nEpoch 6/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.2478 - accuracy: 0.5277 - val_loss: 1.2603 - val_accuracy: 0.5309\nEpoch 7/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.2402 - accuracy: 0.5332 - val_loss: 1.2516 - val_accuracy: 0.5325\nEpoch 8/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.2339 - accuracy: 0.5308 - val_loss: 1.2517 - val_accuracy: 0.5294\nEpoch 9/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.2305 - accuracy: 0.5338 - val_loss: 1.2567 - val_accuracy: 0.5211\nEpoch 10/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.2229 - accuracy: 0.5365 - val_loss: 1.2426 - val_accuracy: 0.5340\nEpoch 11/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.2194 - accuracy: 0.5393 - val_loss: 1.2332 - val_accuracy: 0.5353\nEpoch 12/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.2150 - accuracy: 0.5397 - val_loss: 1.2366 - val_accuracy: 0.5371\nEpoch 13/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.2115 - accuracy: 0.5385 - val_loss: 1.2294 - val_accuracy: 0.5358\nEpoch 14/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.2075 - accuracy: 0.5416 - val_loss: 1.2216 - val_accuracy: 0.5390\nEpoch 15/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.2046 - accuracy: 0.5403 - val_loss: 1.2252 - val_accuracy: 0.5406\nEpoch 16/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.2014 - accuracy: 0.5419 - val_loss: 1.2154 - val_accuracy: 0.5434\nEpoch 17/100\n938/938 [==============================] - 2s 3ms/step - loss: 1.1975 - accuracy: 0.5425 - val_loss: 1.2207 - val_accuracy: 0.5376\nEpoch 18/100\n938/938 [==============================] - 3s 4ms/step - loss: 1.1955 - accuracy: 0.5439 - val_loss: 1.2216 - val_accuracy: 0.5309\nEpoch 19/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1912 - accuracy: 0.5470 - val_loss: 1.2189 - val_accuracy: 0.5431\nEpoch 20/100\n938/938 [==============================] - 3s 4ms/step - loss: 1.1898 - accuracy: 0.5471 - val_loss: 1.2119 - val_accuracy: 0.5408\nEpoch 21/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1870 - accuracy: 0.5477 - val_loss: 1.2136 - val_accuracy: 0.5404\nEpoch 22/100\n938/938 [==============================] - 3s 4ms/step - loss: 1.1862 - accuracy: 0.5468 - val_loss: 1.2164 - val_accuracy: 0.5311\nEpoch 23/100\n938/938 [==============================] - 3s 4ms/step - loss: 1.1835 - accuracy: 0.5476 - val_loss: 1.2084 - val_accuracy: 0.5421\nEpoch 24/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1817 - accuracy: 0.5492 - val_loss: 1.2070 - val_accuracy: 0.5462\nEpoch 25/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1781 - accuracy: 0.5493 - val_loss: 1.2041 - val_accuracy: 0.5455\nEpoch 26/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1768 - accuracy: 0.5538 - val_loss: 1.2049 - val_accuracy: 0.5429\nEpoch 27/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1756 - accuracy: 0.5489 - val_loss: 1.2012 - val_accuracy: 0.5412\nEpoch 28/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1712 - accuracy: 0.5519 - val_loss: 1.2096 - val_accuracy: 0.5436\nEpoch 29/100\n938/938 [==============================] - 4s 5ms/step - loss: 1.1711 - accuracy: 0.5543 - val_loss: 1.1957 - val_accuracy: 0.5556\nEpoch 30/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.1675 - accuracy: 0.5529 - val_loss: 1.1961 - val_accuracy: 0.5504\nEpoch 31/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1662 - accuracy: 0.5541 - val_loss: 1.1945 - val_accuracy: 0.5537\nEpoch 32/100\n938/938 [==============================] - 2s 3ms/step - loss: 1.1649 - accuracy: 0.5558 - val_loss: 1.1928 - val_accuracy: 0.5472\nEpoch 33/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1626 - accuracy: 0.5555 - val_loss: 1.2012 - val_accuracy: 0.5431\nEpoch 34/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1601 - accuracy: 0.5558 - val_loss: 1.1935 - val_accuracy: 0.5436\nEpoch 35/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1587 - accuracy: 0.5578 - val_loss: 1.1902 - val_accuracy: 0.5579\nEpoch 36/100\n938/938 [==============================] - 2s 3ms/step - loss: 1.1564 - accuracy: 0.5576 - val_loss: 1.1836 - val_accuracy: 0.5551\nEpoch 37/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1537 - accuracy: 0.5587 - val_loss: 1.1897 - val_accuracy: 0.5559\nEpoch 38/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1524 - accuracy: 0.5601 - val_loss: 1.1799 - val_accuracy: 0.5513\nEpoch 39/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1500 - accuracy: 0.5600 - val_loss: 1.1839 - val_accuracy: 0.5496\nEpoch 40/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1488 - accuracy: 0.5615 - val_loss: 1.1831 - val_accuracy: 0.5509\nEpoch 41/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1470 - accuracy: 0.5627 - val_loss: 1.1791 - val_accuracy: 0.5556\nEpoch 42/100\n938/938 [==============================] - 4s 5ms/step - loss: 1.1447 - accuracy: 0.5631 - val_loss: 1.1754 - val_accuracy: 0.5528\nEpoch 43/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1412 - accuracy: 0.5620 - val_loss: 1.1756 - val_accuracy: 0.5538\nEpoch 44/100\n938/938 [==============================] - 5s 5ms/step - loss: 1.1412 - accuracy: 0.5617 - val_loss: 1.1752 - val_accuracy: 0.5550\nEpoch 45/100\n938/938 [==============================] - 3s 4ms/step - loss: 1.1396 - accuracy: 0.5620 - val_loss: 1.1765 - val_accuracy: 0.5525\nEpoch 46/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1376 - accuracy: 0.5626 - val_loss: 1.1749 - val_accuracy: 0.5564\nEpoch 47/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1351 - accuracy: 0.5636 - val_loss: 1.1709 - val_accuracy: 0.5537\nEpoch 48/100\n938/938 [==============================] - 3s 4ms/step - loss: 1.1345 - accuracy: 0.5659 - val_loss: 1.1667 - val_accuracy: 0.5546\nEpoch 49/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1313 - accuracy: 0.5650 - val_loss: 1.1706 - val_accuracy: 0.5572\nEpoch 50/100\n938/938 [==============================] - 3s 4ms/step - loss: 1.1301 - accuracy: 0.5644 - val_loss: 1.1650 - val_accuracy: 0.5615\nEpoch 51/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1296 - accuracy: 0.5660 - val_loss: 1.1691 - val_accuracy: 0.5548\nEpoch 52/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1270 - accuracy: 0.5652 - val_loss: 1.1623 - val_accuracy: 0.5588\nEpoch 53/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1259 - accuracy: 0.5655 - val_loss: 1.1631 - val_accuracy: 0.5565\nEpoch 54/100\n938/938 [==============================] - 3s 4ms/step - loss: 1.1253 - accuracy: 0.5674 - val_loss: 1.1638 - val_accuracy: 0.5637\nEpoch 55/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.1217 - accuracy: 0.5702 - val_loss: 1.1657 - val_accuracy: 0.5506\nEpoch 56/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1199 - accuracy: 0.5693 - val_loss: 1.1622 - val_accuracy: 0.5548\nEpoch 57/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1194 - accuracy: 0.5678 - val_loss: 1.1641 - val_accuracy: 0.5572\nEpoch 58/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1170 - accuracy: 0.5691 - val_loss: 1.1619 - val_accuracy: 0.5541\nEpoch 59/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1154 - accuracy: 0.5727 - val_loss: 1.1613 - val_accuracy: 0.5555\nEpoch 60/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.1129 - accuracy: 0.5720 - val_loss: 1.1556 - val_accuracy: 0.5578\nEpoch 61/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1131 - accuracy: 0.5708 - val_loss: 1.1555 - val_accuracy: 0.5621\nEpoch 62/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1128 - accuracy: 0.5702 - val_loss: 1.1533 - val_accuracy: 0.5602\nEpoch 63/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1116 - accuracy: 0.5721 - val_loss: 1.1559 - val_accuracy: 0.5567\nEpoch 64/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1106 - accuracy: 0.5730 - val_loss: 1.1583 - val_accuracy: 0.5582\nEpoch 65/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1072 - accuracy: 0.5728 - val_loss: 1.1626 - val_accuracy: 0.5506\nEpoch 66/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1075 - accuracy: 0.5723 - val_loss: 1.1544 - val_accuracy: 0.5630\nEpoch 67/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1043 - accuracy: 0.5749 - val_loss: 1.1575 - val_accuracy: 0.5539\nEpoch 68/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1043 - accuracy: 0.5732 - val_loss: 1.1568 - val_accuracy: 0.5567\nEpoch 69/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1026 - accuracy: 0.5756 - val_loss: 1.1541 - val_accuracy: 0.5593\nEpoch 70/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1025 - accuracy: 0.5746 - val_loss: 1.1511 - val_accuracy: 0.5571\nEpoch 71/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1014 - accuracy: 0.5742 - val_loss: 1.1521 - val_accuracy: 0.5574\nEpoch 72/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.1000 - accuracy: 0.5761 - val_loss: 1.1585 - val_accuracy: 0.5519\nEpoch 73/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0987 - accuracy: 0.5768 - val_loss: 1.1491 - val_accuracy: 0.5620\nEpoch 74/100\n938/938 [==============================] - 5s 6ms/step - loss: 1.0954 - accuracy: 0.5784 - val_loss: 1.1492 - val_accuracy: 0.5590\nEpoch 75/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0957 - accuracy: 0.5772 - val_loss: 1.1507 - val_accuracy: 0.5615\nEpoch 76/100\n938/938 [==============================] - 5s 6ms/step - loss: 1.0937 - accuracy: 0.5786 - val_loss: 1.1447 - val_accuracy: 0.5619\nEpoch 77/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0941 - accuracy: 0.5774 - val_loss: 1.1470 - val_accuracy: 0.5550\nEpoch 78/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0936 - accuracy: 0.5794 - val_loss: 1.1439 - val_accuracy: 0.5608\nEpoch 79/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0894 - accuracy: 0.5797 - val_loss: 1.1422 - val_accuracy: 0.5667\nEpoch 80/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0892 - accuracy: 0.5788 - val_loss: 1.1502 - val_accuracy: 0.5570\nEpoch 81/100\n938/938 [==============================] - 6s 7ms/step - loss: 1.0895 - accuracy: 0.5806 - val_loss: 1.1426 - val_accuracy: 0.5617\nEpoch 82/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0880 - accuracy: 0.5814 - val_loss: 1.1377 - val_accuracy: 0.5613\nEpoch 83/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0872 - accuracy: 0.5788 - val_loss: 1.1467 - val_accuracy: 0.5544\nEpoch 84/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0870 - accuracy: 0.5796 - val_loss: 1.1387 - val_accuracy: 0.5643\nEpoch 85/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0859 - accuracy: 0.5814 - val_loss: 1.1435 - val_accuracy: 0.5558\nEpoch 86/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0830 - accuracy: 0.5822 - val_loss: 1.1433 - val_accuracy: 0.5573\nEpoch 87/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0842 - accuracy: 0.5806 - val_loss: 1.1479 - val_accuracy: 0.5594\nEpoch 88/100\n938/938 [==============================] - 3s 3ms/step - loss: 1.0814 - accuracy: 0.5819 - val_loss: 1.1435 - val_accuracy: 0.5662\nEpoch 89/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0842 - accuracy: 0.5816 - val_loss: 1.1397 - val_accuracy: 0.5643\nEpoch 90/100\n938/938 [==============================] - 5s 5ms/step - loss: 1.0819 - accuracy: 0.5817 - val_loss: 1.1362 - val_accuracy: 0.5570\nEpoch 91/100\n938/938 [==============================] - 6s 6ms/step - loss: 1.0805 - accuracy: 0.5814 - val_loss: 1.1390 - val_accuracy: 0.5590\nEpoch 92/100\n938/938 [==============================] - 5s 5ms/step - loss: 1.0783 - accuracy: 0.5846 - val_loss: 1.1368 - val_accuracy: 0.5648\nEpoch 93/100\n938/938 [==============================] - 6s 6ms/step - loss: 1.0761 - accuracy: 0.5842 - val_loss: 1.1386 - val_accuracy: 0.5605\nEpoch 94/100\n938/938 [==============================] - 6s 7ms/step - loss: 1.0765 - accuracy: 0.5836 - val_loss: 1.1385 - val_accuracy: 0.5603\nEpoch 95/100\n938/938 [==============================] - 5s 6ms/step - loss: 1.0754 - accuracy: 0.5838 - val_loss: 1.1390 - val_accuracy: 0.5604\nEpoch 96/100\n938/938 [==============================] - 6s 6ms/step - loss: 1.0756 - accuracy: 0.5849 - val_loss: 1.1407 - val_accuracy: 0.5627\nEpoch 97/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0743 - accuracy: 0.5841 - val_loss: 1.1406 - val_accuracy: 0.5581\nEpoch 98/100\n938/938 [==============================] - 4s 4ms/step - loss: 1.0725 - accuracy: 0.5848 - val_loss: 1.1382 - val_accuracy: 0.5604\nEpoch 99/100\n938/938 [==============================] - 5s 6ms/step - loss: 1.0731 - accuracy: 0.5850 - val_loss: 1.1379 - val_accuracy: 0.5641\nEpoch 100/100\n938/938 [==============================] - 5s 5ms/step - loss: 1.0728 - accuracy: 0.5856 - val_loss: 1.1401 - val_accuracy: 0.5610\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 19,
      "block_group": "e549aa4edbf94ab0af50ad750d810fe0",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model4.evaluate(np.expand_dims(X_train2, 1), y_train2))\nprint(\"Test Accuracy:\", model4.evaluate(np.expand_dims(X_test2, 1), y_test2))",
      "metadata": {
        "cell_id": "fa9a9080ede44820a5275a438c0abdd3",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "938/938 [==============================] - 2s 2ms/step - loss: 1.0514 - accuracy: 0.5916\nTraining Accuracy: [1.051355242729187, 0.5916333198547363]\n313/313 [==============================] - 1s 2ms/step - loss: 1.1241 - accuracy: 0.5655\nTest Accuracy: [1.1240668296813965, 0.565500020980835]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 20,
      "block_group": "fe2b4aa91aa04b8191b9be1347f0de6e",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=3da032f1-e5ab-4726-ac09-eb3d9c053730' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "cf73980c93a143efbc39a42d010584dc",
    "deepnote_execution_queue": []
  }
}