{
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.impute import KNNImputer\nfrom sklearn.decomposition import PCA\n! pip install scikit-optimize\n# report scikit-optimize version number\nimport skopt\nprint('skopt %s' % skopt.__version__)\nfrom skopt.space import Integer\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.metrics import classification_report\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LayerNormalization\nfrom tensorflow.keras.optimizers.legacy import SGD\nfrom tensorflow.keras import backend as K\nimport argparse",
      "metadata": {
        "cell_id": "4169c8ed9a78470986d3cec055a548f0",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already satisfied: scikit-optimize in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (0.10.1)\nRequirement already satisfied: joblib>=0.11 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (1.2.0)\nRequirement already satisfied: pyaml>=16.9 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (23.12.0)\nRequirement already satisfied: numpy>=1.20.3 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (1.26.4)\nRequirement already satisfied: scipy>=1.1.0 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (1.11.4)\nRequirement already satisfied: scikit-learn>=1.0.0 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (1.2.2)\nRequirement already satisfied: packaging>=21.3 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (23.1)\nRequirement already satisfied: PyYAML in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.0->scikit-optimize) (2.2.0)\nskopt 0.10.1\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 1,
      "block_group": "6ff3ae9f19f0430b8a580d1b7265fa37",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "#data from kaggle dataset: \"Prediction of music genre\"\ndata1 = pd.read_csv(\"~/Downloads/music_genre.csv\")\n#data from kaggle data set: \"Spotify Tracks Dataset\"\ndata2 = pd.read_csv(\"~/Downloads/dataset.csv\")",
      "metadata": {
        "cell_id": "478fa62ab8e542279c7585922c308fb5",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": 2,
      "block_group": "6bc1b4a38d2643b793303f258ef4c2a6",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<h1>Data 2</h1>",
      "metadata": {
        "cell_id": "8657b33eeefe4f03b0b43790f6e9d972",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "d9067666aa9b47869538d6189a39e334"
    },
    {
      "cell_type": "code",
      "source": "#drop all the rows that are null\ndata2 = data2.dropna()",
      "metadata": {
        "cell_id": "d00320ddc9c242ff830db203767e0f23",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": 3,
      "block_group": "adaeefcebbe8487c989104626b335339",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "#make track_genre into label encoding\nlabelencoder = LabelEncoder()\ndata2['track_genre_num'] = labelencoder.fit_transform(data2['track_genre'])\ndata2",
      "metadata": {
        "cell_id": "1040a04d6a5b480a907593f72aaf603a",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>track_id</th>\n      <th>artists</th>\n      <th>album_name</th>\n      <th>track_name</th>\n      <th>popularity</th>\n      <th>duration_ms</th>\n      <th>explicit</th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>...</th>\n      <th>mode</th>\n      <th>speechiness</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>tempo</th>\n      <th>time_signature</th>\n      <th>track_genre</th>\n      <th>track_genre_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>5SuOikwiRyPMVoIQDJUgSV</td>\n      <td>Gen Hoshino</td>\n      <td>Comedy</td>\n      <td>Comedy</td>\n      <td>73</td>\n      <td>230666</td>\n      <td>False</td>\n      <td>0.676</td>\n      <td>0.4610</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.1430</td>\n      <td>0.0322</td>\n      <td>0.000001</td>\n      <td>0.3580</td>\n      <td>0.7150</td>\n      <td>87.917</td>\n      <td>4</td>\n      <td>acoustic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>4qPNDBW1i3p13qLCt0Ki3A</td>\n      <td>Ben Woodward</td>\n      <td>Ghost (Acoustic)</td>\n      <td>Ghost - Acoustic</td>\n      <td>55</td>\n      <td>149610</td>\n      <td>False</td>\n      <td>0.420</td>\n      <td>0.1660</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0763</td>\n      <td>0.9240</td>\n      <td>0.000006</td>\n      <td>0.1010</td>\n      <td>0.2670</td>\n      <td>77.489</td>\n      <td>4</td>\n      <td>acoustic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1iJBSr7s7jYXzM8EGcbK5b</td>\n      <td>Ingrid Michaelson;ZAYN</td>\n      <td>To Begin Again</td>\n      <td>To Begin Again</td>\n      <td>57</td>\n      <td>210826</td>\n      <td>False</td>\n      <td>0.438</td>\n      <td>0.3590</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0557</td>\n      <td>0.2100</td>\n      <td>0.000000</td>\n      <td>0.1170</td>\n      <td>0.1200</td>\n      <td>76.332</td>\n      <td>4</td>\n      <td>acoustic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>6lfxq3CG4xtTiEg7opyCyx</td>\n      <td>Kina Grannis</td>\n      <td>Crazy Rich Asians (Original Motion Picture Sou...</td>\n      <td>Can't Help Falling In Love</td>\n      <td>71</td>\n      <td>201933</td>\n      <td>False</td>\n      <td>0.266</td>\n      <td>0.0596</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0363</td>\n      <td>0.9050</td>\n      <td>0.000071</td>\n      <td>0.1320</td>\n      <td>0.1430</td>\n      <td>181.740</td>\n      <td>3</td>\n      <td>acoustic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5vjLSffimiIP26QG5WcN2K</td>\n      <td>Chord Overstreet</td>\n      <td>Hold On</td>\n      <td>Hold On</td>\n      <td>82</td>\n      <td>198853</td>\n      <td>False</td>\n      <td>0.618</td>\n      <td>0.4430</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0526</td>\n      <td>0.4690</td>\n      <td>0.000000</td>\n      <td>0.0829</td>\n      <td>0.1670</td>\n      <td>119.949</td>\n      <td>4</td>\n      <td>acoustic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>113995</th>\n      <td>113995</td>\n      <td>2C3TZjDRiAzdyViavDJ217</td>\n      <td>Rainy Lullaby</td>\n      <td>#mindfulness - Soft Rain for Mindful Meditatio...</td>\n      <td>Sleep My Little Boy</td>\n      <td>21</td>\n      <td>384999</td>\n      <td>False</td>\n      <td>0.172</td>\n      <td>0.2350</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0422</td>\n      <td>0.6400</td>\n      <td>0.928000</td>\n      <td>0.0863</td>\n      <td>0.0339</td>\n      <td>125.995</td>\n      <td>5</td>\n      <td>world-music</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>113996</th>\n      <td>113996</td>\n      <td>1hIz5L4IB9hN3WRYPOCGPw</td>\n      <td>Rainy Lullaby</td>\n      <td>#mindfulness - Soft Rain for Mindful Meditatio...</td>\n      <td>Water Into Light</td>\n      <td>22</td>\n      <td>385000</td>\n      <td>False</td>\n      <td>0.174</td>\n      <td>0.1170</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0401</td>\n      <td>0.9940</td>\n      <td>0.976000</td>\n      <td>0.1050</td>\n      <td>0.0350</td>\n      <td>85.239</td>\n      <td>4</td>\n      <td>world-music</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>113997</th>\n      <td>113997</td>\n      <td>6x8ZfSoqDjuNa5SVP5QjvX</td>\n      <td>Cesária Evora</td>\n      <td>Best Of</td>\n      <td>Miss Perfumado</td>\n      <td>22</td>\n      <td>271466</td>\n      <td>False</td>\n      <td>0.629</td>\n      <td>0.3290</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0420</td>\n      <td>0.8670</td>\n      <td>0.000000</td>\n      <td>0.0839</td>\n      <td>0.7430</td>\n      <td>132.378</td>\n      <td>4</td>\n      <td>world-music</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>113998</th>\n      <td>113998</td>\n      <td>2e6sXL2bYv4bSz6VTdnfLs</td>\n      <td>Michael W. Smith</td>\n      <td>Change Your World</td>\n      <td>Friends</td>\n      <td>41</td>\n      <td>283893</td>\n      <td>False</td>\n      <td>0.587</td>\n      <td>0.5060</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0297</td>\n      <td>0.3810</td>\n      <td>0.000000</td>\n      <td>0.2700</td>\n      <td>0.4130</td>\n      <td>135.960</td>\n      <td>4</td>\n      <td>world-music</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>113999</th>\n      <td>113999</td>\n      <td>2hETkH7cOfqmz3LqZDHZf5</td>\n      <td>Cesária Evora</td>\n      <td>Miss Perfumado</td>\n      <td>Barbincor</td>\n      <td>22</td>\n      <td>241826</td>\n      <td>False</td>\n      <td>0.526</td>\n      <td>0.4870</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0725</td>\n      <td>0.6810</td>\n      <td>0.000000</td>\n      <td>0.0893</td>\n      <td>0.7080</td>\n      <td>79.198</td>\n      <td>4</td>\n      <td>world-music</td>\n      <td>113</td>\n    </tr>\n  </tbody>\n</table>\n<p>113999 rows × 22 columns</p>\n</div>",
            "text/plain": "        Unnamed: 0                track_id                 artists  \\\n0                0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n1                1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n2                2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n3                3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n4                4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n...            ...                     ...                     ...   \n113995      113995  2C3TZjDRiAzdyViavDJ217           Rainy Lullaby   \n113996      113996  1hIz5L4IB9hN3WRYPOCGPw           Rainy Lullaby   \n113997      113997  6x8ZfSoqDjuNa5SVP5QjvX           Cesária Evora   \n113998      113998  2e6sXL2bYv4bSz6VTdnfLs        Michael W. Smith   \n113999      113999  2hETkH7cOfqmz3LqZDHZf5           Cesária Evora   \n\n                                               album_name  \\\n0                                                  Comedy   \n1                                        Ghost (Acoustic)   \n2                                          To Begin Again   \n3       Crazy Rich Asians (Original Motion Picture Sou...   \n4                                                 Hold On   \n...                                                   ...   \n113995  #mindfulness - Soft Rain for Mindful Meditatio...   \n113996  #mindfulness - Soft Rain for Mindful Meditatio...   \n113997                                            Best Of   \n113998                                  Change Your World   \n113999                                     Miss Perfumado   \n\n                        track_name  popularity  duration_ms  explicit  \\\n0                           Comedy          73       230666     False   \n1                 Ghost - Acoustic          55       149610     False   \n2                   To Begin Again          57       210826     False   \n3       Can't Help Falling In Love          71       201933     False   \n4                          Hold On          82       198853     False   \n...                            ...         ...          ...       ...   \n113995         Sleep My Little Boy          21       384999     False   \n113996            Water Into Light          22       385000     False   \n113997              Miss Perfumado          22       271466     False   \n113998                     Friends          41       283893     False   \n113999                   Barbincor          22       241826     False   \n\n        danceability  energy  ...  mode  speechiness  acousticness  \\\n0              0.676  0.4610  ...     0       0.1430        0.0322   \n1              0.420  0.1660  ...     1       0.0763        0.9240   \n2              0.438  0.3590  ...     1       0.0557        0.2100   \n3              0.266  0.0596  ...     1       0.0363        0.9050   \n4              0.618  0.4430  ...     1       0.0526        0.4690   \n...              ...     ...  ...   ...          ...           ...   \n113995         0.172  0.2350  ...     1       0.0422        0.6400   \n113996         0.174  0.1170  ...     0       0.0401        0.9940   \n113997         0.629  0.3290  ...     0       0.0420        0.8670   \n113998         0.587  0.5060  ...     1       0.0297        0.3810   \n113999         0.526  0.4870  ...     0       0.0725        0.6810   \n\n        instrumentalness  liveness  valence    tempo  time_signature  \\\n0               0.000001    0.3580   0.7150   87.917               4   \n1               0.000006    0.1010   0.2670   77.489               4   \n2               0.000000    0.1170   0.1200   76.332               4   \n3               0.000071    0.1320   0.1430  181.740               3   \n4               0.000000    0.0829   0.1670  119.949               4   \n...                  ...       ...      ...      ...             ...   \n113995          0.928000    0.0863   0.0339  125.995               5   \n113996          0.976000    0.1050   0.0350   85.239               4   \n113997          0.000000    0.0839   0.7430  132.378               4   \n113998          0.000000    0.2700   0.4130  135.960               4   \n113999          0.000000    0.0893   0.7080   79.198               4   \n\n        track_genre  track_genre_num  \n0          acoustic                0  \n1          acoustic                0  \n2          acoustic                0  \n3          acoustic                0  \n4          acoustic                0  \n...             ...              ...  \n113995  world-music              113  \n113996  world-music              113  \n113997  world-music              113  \n113998  world-music              113  \n113999  world-music              113  \n\n[113999 rows x 22 columns]"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "outputs_reference": null,
      "execution_count": 4,
      "block_group": "4d419abe713d4024a8adf3157eebc408",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "features = [\"popularity\", \"duration_ms\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\nlen(features)",
      "metadata": {
        "cell_id": "84b1ec2eefa74908a59fd42325ea9a8d",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "14"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "outputs_reference": null,
      "execution_count": 5,
      "block_group": "9e142bd6f4464e9fa263a6e14237e4d2",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<h3>Baseline Model with SGD</h3>",
      "metadata": {
        "cell_id": "d0a0a691c5c442fd87007d053868d357",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "6bd79fa2d96a4c55b9f27043fc50e4df"
    },
    {
      "cell_type": "code",
      "source": "# train 80%, val 20%, test 20%\nX_train2_1b, X_test2_1b, y_train2_1b, y_test2_1b = train_test_split(data2[features], data2[\"track_genre_num\"], test_size=0.2, random_state=1)",
      "metadata": {
        "cell_id": "c2c3c199a0184fcf8238f2eda3760d41",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": 6,
      "block_group": "bbb148b290204b6687cc05d34e16da94",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "model2bs = Sequential()\nmodel2bs.add(Dense(256, input_shape=(14,), activation=\"relu\"))\nmodel2bs.add(Dense(114, activation=\"softmax\"))\n\nsgd = SGD(0.01) #learning rate = 0.01\nmodel2bs.compile(loss=\"sparse_categorical_crossentropy\", optimizer=sgd,\n\tmetrics=[\"accuracy\"]) #use sparse for loss because label is integer and not vector\nH2bs = model2bs.fit(X_train2_1b, y_train2_1b, validation_split=0.2,\n\tepochs=100, batch_size=64)",
      "metadata": {
        "cell_id": "cfbc86c4be2c4efe82e8049c95e4f79e",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 12614315.0000 - accuracy: 0.0084 - val_loss: 4.7363 - val_accuracy: 0.0077\nEpoch 2/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7362 - accuracy: 0.0094 - val_loss: 4.7364 - val_accuracy: 0.0077\nEpoch 3/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7362 - accuracy: 0.0095 - val_loss: 4.7364 - val_accuracy: 0.0077\nEpoch 4/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7361 - accuracy: 0.0095 - val_loss: 4.7365 - val_accuracy: 0.0077\nEpoch 5/100\n1140/1140 [==============================] - 1s 821us/step - loss: 4.7361 - accuracy: 0.0095 - val_loss: 4.7365 - val_accuracy: 0.0077\nEpoch 6/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7361 - accuracy: 0.0095 - val_loss: 4.7366 - val_accuracy: 0.0077\nEpoch 7/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7361 - accuracy: 0.0095 - val_loss: 4.7366 - val_accuracy: 0.0077\nEpoch 8/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7361 - accuracy: 0.0095 - val_loss: 4.7367 - val_accuracy: 0.0077\nEpoch 9/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7361 - accuracy: 0.0095 - val_loss: 4.7367 - val_accuracy: 0.0077\nEpoch 10/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7361 - accuracy: 0.0095 - val_loss: 4.7368 - val_accuracy: 0.0077\nEpoch 11/100\n1140/1140 [==============================] - 3s 2ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7368 - val_accuracy: 0.0077\nEpoch 12/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7368 - val_accuracy: 0.0077\nEpoch 13/100\n1140/1140 [==============================] - 1s 953us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7369 - val_accuracy: 0.0077\nEpoch 14/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7369 - val_accuracy: 0.0077\nEpoch 15/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7369 - val_accuracy: 0.0077\nEpoch 16/100\n1140/1140 [==============================] - 1s 846us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7369 - val_accuracy: 0.0077\nEpoch 17/100\n1140/1140 [==============================] - 1s 965us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7370 - val_accuracy: 0.0077\nEpoch 18/100\n1140/1140 [==============================] - 1s 848us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7370 - val_accuracy: 0.0077\nEpoch 19/100\n1140/1140 [==============================] - 1s 827us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7370 - val_accuracy: 0.0077\nEpoch 20/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7370 - val_accuracy: 0.0077\nEpoch 21/100\n1140/1140 [==============================] - 1s 907us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7370 - val_accuracy: 0.0077\nEpoch 22/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 23/100\n1140/1140 [==============================] - 1s 941us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 24/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 25/100\n1140/1140 [==============================] - 1s 907us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 26/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 27/100\n1140/1140 [==============================] - 1s 845us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 28/100\n1140/1140 [==============================] - 1s 984us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 29/100\n1140/1140 [==============================] - 1s 829us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 30/100\n1140/1140 [==============================] - 1s 957us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 31/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 32/100\n1140/1140 [==============================] - 1s 941us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 33/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 34/100\n1140/1140 [==============================] - 1s 907us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 35/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 36/100\n1140/1140 [==============================] - 1s 916us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 37/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 38/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 39/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 40/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 41/100\n1140/1140 [==============================] - 1s 985us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 42/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 43/100\n1140/1140 [==============================] - 1s 997us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 44/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 45/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 46/100\n1140/1140 [==============================] - 1s 984us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 47/100\n1140/1140 [==============================] - 1s 843us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 48/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 49/100\n1140/1140 [==============================] - 1s 935us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 50/100\n1140/1140 [==============================] - 1s 976us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 51/100\n1140/1140 [==============================] - 1s 824us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 52/100\n1140/1140 [==============================] - 1s 835us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 53/100\n1140/1140 [==============================] - 1s 968us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 54/100\n1140/1140 [==============================] - 1s 838us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 55/100\n1140/1140 [==============================] - 1s 972us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 56/100\n1140/1140 [==============================] - 1s 882us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 57/100\n1140/1140 [==============================] - 1s 842us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 58/100\n1140/1140 [==============================] - 1s 983us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 59/100\n1140/1140 [==============================] - 1s 892us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 60/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 61/100\n1140/1140 [==============================] - 1s 954us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 62/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 63/100\n1140/1140 [==============================] - 3s 2ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 64/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 65/100\n1140/1140 [==============================] - 1s 993us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 66/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 67/100\n1140/1140 [==============================] - 1s 990us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 68/100\n1140/1140 [==============================] - 1s 967us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 69/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 70/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 71/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 72/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 73/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 74/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 75/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 76/100\n1140/1140 [==============================] - 4s 4ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 77/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 78/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 79/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 80/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 81/100\n1140/1140 [==============================] - 1s 976us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 82/100\n1140/1140 [==============================] - 1s 957us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 83/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 84/100\n1140/1140 [==============================] - 1s 907us/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 85/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 86/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 87/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 88/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 89/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 90/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 91/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 92/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 93/100\n1140/1140 [==============================] - 3s 2ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 94/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 95/100\n1140/1140 [==============================] - 4s 3ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 96/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 97/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 98/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 99/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 100/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7360 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 7,
      "block_group": "b67f5aa77c0b43289c0c9a11d587bad8",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model2bs.evaluate(X_train2_1b, y_train2_1b))\nprint(\"Test Accuracy:\", model2bs.evaluate(X_test2_1b, y_test2_1b))",
      "metadata": {
        "cell_id": "f5677ac05cde42d69d81d905aa9a797d",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2850/2850 [==============================] - 2s 553us/step - loss: 4.7362 - accuracy: 0.0091\nTraining Accuracy: [4.736191272735596, 0.009144837036728859]\n713/713 [==============================] - 0s 532us/step - loss: 4.7375 - accuracy: 0.0073\nTest Accuracy: [4.73750114440918, 0.007280701771378517]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 8,
      "block_group": "dd2858be2d924ab8bd2934dc28d4a4be",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<h3>Baseline Model with Adam</h3>",
      "metadata": {
        "cell_id": "c807cf809e5c4aaca2cc84796c14f046",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "51072799aba44338af0d9b7b5b92f090"
    },
    {
      "cell_type": "code",
      "source": "model2ba = Sequential()\nmodel2ba.add(Dense(256, input_shape=(14,), activation=\"relu\"))\nmodel2ba.add(Dense(114, activation=\"softmax\"))\n\nmodel2ba.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",\n\tmetrics=[\"accuracy\"]) #use sparse for loss because label is integer and not vector\nH2ba = model2ba.fit(X_train2_1b, y_train2_1b, validation_split=0.2,\n\tepochs=100, batch_size=64)",
      "metadata": {
        "cell_id": "6f79081fc2d44038a49fa2500ff1e75e",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 6284.1514 - accuracy: 0.0091 - val_loss: 1783.6125 - val_accuracy: 0.0099\nEpoch 2/100\n1140/1140 [==============================] - 3s 2ms/step - loss: 1025.3003 - accuracy: 0.0089 - val_loss: 423.8664 - val_accuracy: 0.0094\nEpoch 3/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 185.9874 - accuracy: 0.0095 - val_loss: 4.7380 - val_accuracy: 0.0082\nEpoch 4/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7377 - accuracy: 0.0087 - val_loss: 4.7376 - val_accuracy: 0.0081\nEpoch 5/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7372 - accuracy: 0.0084 - val_loss: 4.7372 - val_accuracy: 0.0081\nEpoch 6/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7367 - accuracy: 0.0083 - val_loss: 4.7370 - val_accuracy: 0.0081\nEpoch 7/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7365 - accuracy: 0.0087 - val_loss: 4.7370 - val_accuracy: 0.0077\nEpoch 8/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7370 - val_accuracy: 0.0077\nEpoch 9/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 10/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 11/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 12/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 13/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 14/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 15/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7365 - accuracy: 0.0092 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 16/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 17/100\n1140/1140 [==============================] - 4s 3ms/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 18/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 19/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 20/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 21/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 22/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 23/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 24/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 25/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7365 - accuracy: 0.0091 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 26/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7365 - accuracy: 0.0090 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 27/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7373 - val_accuracy: 0.0077\nEpoch 28/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 29/100\n1140/1140 [==============================] - 1s 886us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 30/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 31/100\n1140/1140 [==============================] - 1s 761us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 32/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 33/100\n1140/1140 [==============================] - 1s 769us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 34/100\n1140/1140 [==============================] - 1s 837us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7373 - val_accuracy: 0.0077\nEpoch 35/100\n1140/1140 [==============================] - 1s 823us/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 36/100\n1140/1140 [==============================] - 1s 835us/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 37/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0086 - val_loss: 4.7373 - val_accuracy: 0.0077\nEpoch 38/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 39/100\n1140/1140 [==============================] - 1s 977us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 40/100\n1140/1140 [==============================] - 1s 948us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 41/100\n1140/1140 [==============================] - 1s 797us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 42/100\n1140/1140 [==============================] - 1s 800us/step - loss: 4.7365 - accuracy: 0.0091 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 43/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 44/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 45/100\n1140/1140 [==============================] - 1s 875us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 46/100\n1140/1140 [==============================] - 1s 822us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 47/100\n1140/1140 [==============================] - 1s 817us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 48/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 49/100\n1140/1140 [==============================] - 1s 851us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 50/100\n1140/1140 [==============================] - 1s 850us/step - loss: 4.7365 - accuracy: 0.0090 - val_loss: 4.7373 - val_accuracy: 0.0077\nEpoch 51/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7373 - val_accuracy: 0.0077\nEpoch 52/100\n1140/1140 [==============================] - 1s 863us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 53/100\n1140/1140 [==============================] - 1s 912us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 54/100\n1140/1140 [==============================] - 1s 981us/step - loss: 4.7365 - accuracy: 0.0092 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 55/100\n1140/1140 [==============================] - 1s 976us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 56/100\n1140/1140 [==============================] - 1s 806us/step - loss: 4.7365 - accuracy: 0.0091 - val_loss: 4.7373 - val_accuracy: 0.0077\nEpoch 57/100\n1140/1140 [==============================] - 1s 867us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 58/100\n1140/1140 [==============================] - 1s 848us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 59/100\n1140/1140 [==============================] - 1s 999us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 60/100\n1140/1140 [==============================] - 1s 857us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 61/100\n1140/1140 [==============================] - 1s 822us/step - loss: 4.7365 - accuracy: 0.0092 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 62/100\n1140/1140 [==============================] - 1s 827us/step - loss: 4.7365 - accuracy: 0.0092 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 63/100\n1140/1140 [==============================] - 1s 976us/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 64/100\n1140/1140 [==============================] - 1s 875us/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 65/100\n1140/1140 [==============================] - 1s 840us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 66/100\n1140/1140 [==============================] - 1s 905us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 67/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 68/100\n1140/1140 [==============================] - 1s 895us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7373 - val_accuracy: 0.0077\nEpoch 69/100\n1140/1140 [==============================] - 1s 879us/step - loss: 4.7365 - accuracy: 0.0092 - val_loss: 4.7373 - val_accuracy: 0.0077\nEpoch 70/100\n1140/1140 [==============================] - 1s 847us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 71/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7373 - val_accuracy: 0.0077\nEpoch 72/100\n1140/1140 [==============================] - 1s 946us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7373 - val_accuracy: 0.0077\nEpoch 73/100\n1140/1140 [==============================] - 1s 953us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 74/100\n1140/1140 [==============================] - 1s 987us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 75/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 76/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 77/100\n1140/1140 [==============================] - 1s 895us/step - loss: 4.7365 - accuracy: 0.0091 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 78/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 79/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 80/100\n1140/1140 [==============================] - 1s 883us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 81/100\n1140/1140 [==============================] - 1s 830us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7373 - val_accuracy: 0.0077\nEpoch 82/100\n1140/1140 [==============================] - 1s 916us/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7373 - val_accuracy: 0.0077\nEpoch 83/100\n1140/1140 [==============================] - 1s 972us/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 84/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 85/100\n1140/1140 [==============================] - 1s 914us/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 86/100\n1140/1140 [==============================] - 1s 987us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 87/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 88/100\n1140/1140 [==============================] - 1s 877us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 89/100\n1140/1140 [==============================] - 1s 789us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 90/100\n1140/1140 [==============================] - 1s 830us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7371 - val_accuracy: 0.0077\nEpoch 91/100\n1140/1140 [==============================] - 1s 833us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 92/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 93/100\n1140/1140 [==============================] - 1s 925us/step - loss: 4.7365 - accuracy: 0.0094 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 94/100\n1140/1140 [==============================] - 1s 845us/step - loss: 4.7365 - accuracy: 0.0093 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 95/100\n1140/1140 [==============================] - 1s 886us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 96/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 97/100\n1140/1140 [==============================] - 1s 828us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 98/100\n1140/1140 [==============================] - 1s 907us/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 99/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 4.7365 - accuracy: 0.0091 - val_loss: 4.7372 - val_accuracy: 0.0077\nEpoch 100/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 4.7365 - accuracy: 0.0095 - val_loss: 4.7371 - val_accuracy: 0.0077\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 9,
      "block_group": "6c8dabf4b02f4201a06d12d3cfe7d3f7",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model2ba.evaluate(X_train2_1b, y_train2_1b))\nprint(\"Test Accuracy:\", model2ba.evaluate(X_test2_1b, y_test2_1b))",
      "metadata": {
        "cell_id": "bd79743e32be45669072dd71251cad96",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2850/2850 [==============================] - 1s 374us/step - loss: 4.7362 - accuracy: 0.0091\nTraining Accuracy: [4.7361931800842285, 0.009144837036728859]\n713/713 [==============================] - 0s 363us/step - loss: 4.7375 - accuracy: 0.0073\nTest Accuracy: [4.7375054359436035, 0.007280701771378517]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 25,
      "block_group": "597a6b1934504b23a9855532c13e27a1",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(classification_report(y_test2_1b, np.argmax(model2ba.predict(X_test2_1b, 1), axis =-1)))",
      "metadata": {
        "cell_id": "9d82a8ac249347a992df345086c9d6c9",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "22800/22800 [==============================] - 45s 2ms/step\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       214\n           1       0.00      0.00      0.00       202\n           2       0.00      0.00      0.00       206\n           3       0.00      0.00      0.00       201\n           4       0.00      0.00      0.00       192\n           5       0.00      0.00      0.00       199\n           6       0.00      0.00      0.00       195\n           7       0.00      0.00      0.00       178\n           8       0.00      0.00      0.00       229\n           9       0.00      0.00      0.00       206\n          10       0.00      0.00      0.00       200\n          11       0.00      0.00      0.00       187\n          12       0.00      0.00      0.00       222\n          13       0.00      0.00      0.00       203\n          14       0.00      0.00      0.00       201\n          15       0.00      0.00      0.00       208\n          16       0.00      0.00      0.00       201\n          17       0.00      0.00      0.00       209\n          18       0.00      0.00      0.00       216\n          19       0.00      0.00      0.00       209\n          20       0.00      0.00      0.00       214\n          21       0.00      0.00      0.00       192\n          22       0.00      0.00      0.00       185\n          23       0.00      0.00      0.00       208\n          24       0.00      0.00      0.00       201\n          25       0.00      0.00      0.00       224\n          26       0.00      0.00      0.00       203\n          27       0.00      0.00      0.00       199\n          28       0.00      0.00      0.00       205\n          29       0.00      0.00      0.00       207\n          30       0.00      0.00      0.00       216\n          31       0.00      0.00      0.00       210\n          32       0.00      0.00      0.00       202\n          33       0.00      0.00      0.00       184\n          34       0.00      0.00      0.00       208\n          35       0.00      0.00      0.00       199\n          36       0.00      0.00      0.00       200\n          37       0.00      0.00      0.00       205\n          38       0.00      0.00      0.00       211\n          39       0.00      0.00      0.00       183\n          40       0.00      0.00      0.00       182\n          41       0.00      0.00      0.00       195\n          42       0.00      0.00      0.00       208\n          43       0.00      0.00      0.00       212\n          44       0.00      0.00      0.00       198\n          45       0.00      0.00      0.00       181\n          46       0.00      0.00      0.00       193\n          47       0.00      0.00      0.00       191\n          48       0.00      0.00      0.00       200\n          49       0.00      0.00      0.00       199\n          50       0.00      0.00      0.00       201\n          51       0.00      0.00      0.00       206\n          52       0.00      0.00      0.00       186\n          53       0.00      0.00      0.00       218\n          54       0.00      0.00      0.00       224\n          55       0.00      0.00      0.00       205\n          56       0.00      0.00      0.00       200\n          57       0.00      0.00      0.00       214\n          58       0.00      0.00      0.00       194\n          59       0.00      0.00      0.00       202\n          60       0.00      0.00      0.00       195\n          61       0.00      0.00      0.00       226\n          62       0.00      0.00      0.00       209\n          63       0.00      0.00      0.00       172\n          64       0.00      0.00      0.00       195\n          65       0.00      0.00      0.00       204\n          66       0.00      0.00      0.00       207\n          67       0.00      0.00      0.00       179\n          68       0.00      0.00      0.00       204\n          69       0.00      0.00      0.00       213\n          70       0.00      0.00      0.00       236\n          71       0.00      0.00      0.00       226\n          72       0.00      0.00      0.00       182\n          73       0.00      0.00      0.00       185\n          74       0.00      0.00      0.00       209\n          75       0.00      0.00      0.00       184\n          76       0.00      0.00      0.00       204\n          77       0.00      0.00      0.00       202\n          78       0.00      0.00      0.00       198\n          79       0.00      0.00      0.00       178\n          80       0.00      0.00      0.00       201\n          81       0.00      0.00      0.00       199\n          82       0.00      0.00      0.00       207\n          83       0.00      0.00      0.00       176\n          84       0.00      0.00      0.00       205\n          85       0.00      0.00      0.00       195\n          86       0.00      0.00      0.00       186\n          87       0.00      0.00      0.00       182\n          88       0.00      0.00      0.00       180\n          89       0.00      0.00      0.00       213\n          90       0.00      0.00      0.00       192\n          91       0.00      0.00      0.00       183\n          92       0.00      0.00      0.00       188\n          93       0.00      0.00      0.00       191\n          94       0.00      0.00      0.00       187\n          95       0.00      0.00      0.00       198\n          96       0.00      0.00      0.00       195\n          97       0.00      0.00      0.00       227\n          98       0.00      0.00      0.00       216\n          99       0.00      0.00      0.00       187\n         100       0.00      0.00      0.00       194\n         101       0.00      0.00      0.00       215\n         102       0.00      0.00      0.00       197\n         103       0.00      0.00      0.00       172\n         104       0.00      0.00      0.00       230\n         105       0.00      0.00      0.00       202\n         106       0.00      0.00      0.00       188\n         107       0.00      0.00      0.00       179\n         108       0.00      0.00      0.00       210\n         109       0.00      0.00      0.00       197\n         110       0.01      1.00      0.01       166\n         111       0.00      0.00      0.00       191\n         112       0.00      0.00      0.00       193\n         113       0.00      0.00      0.00       207\n\n    accuracy                           0.01     22800\n   macro avg       0.00      0.01      0.00     22800\nweighted avg       0.00      0.01      0.00     22800\n\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/Users/mkobayashi/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/mkobayashi/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/Users/mkobayashi/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 31,
      "block_group": "e7d5a2539c474acdb2c8d5a06050dfe9",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<h3>Rescale data</h3>",
      "metadata": {
        "cell_id": "8955602c0c7e470b95778fa782daf376",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "9353b1d8ed6f4a8ca88e479d8c6ab2d7"
    },
    {
      "cell_type": "code",
      "source": "scaler2 = preprocessing.MinMaxScaler()\nscaled_feat2 = scaler2.fit_transform(data2[features])\n\nX_train2_2, X_test2_2, y_train2_2, y_test2_2 = train_test_split(scaled_feat2, data2[\"track_genre_num\"], test_size=0.2, random_state=1)",
      "metadata": {
        "cell_id": "0e3b45ea07c2438ba44e334256386520",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": 11,
      "block_group": "265cbdcd53804ee2943b02926e42c28b",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "model2_2 = Sequential()\nmodel2_2.add(Dense(256, input_shape=(14,), activation=\"relu\"))\nmodel2_2.add(Dense(114, activation=\"softmax\"))\n\nmodel2_2.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",\n\tmetrics=[\"accuracy\"]) #use sparse for loss because label is integer and not vector\nH2_2 = model2_2.fit(X_train2_2, y_train2_2, validation_split=0.2,\n\tepochs=100, batch_size=64)",
      "metadata": {
        "cell_id": "591f800e94634b458ad31a10a9c9a1a7",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "1140/1140 [==============================] - 2s 1ms/step - loss: 3.9562 - accuracy: 0.1081 - val_loss: 3.6844 - val_accuracy: 0.1387\nEpoch 2/100\n1140/1140 [==============================] - 1s 800us/step - loss: 3.5908 - accuracy: 0.1576 - val_loss: 3.5416 - val_accuracy: 0.1639\nEpoch 3/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.4567 - accuracy: 0.1782 - val_loss: 3.4139 - val_accuracy: 0.1805\nEpoch 4/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.3440 - accuracy: 0.1931 - val_loss: 3.3264 - val_accuracy: 0.1949\nEpoch 5/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 3.2631 - accuracy: 0.2070 - val_loss: 3.2575 - val_accuracy: 0.2004\nEpoch 6/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.1996 - accuracy: 0.2153 - val_loss: 3.2065 - val_accuracy: 0.2117\nEpoch 7/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.1507 - accuracy: 0.2224 - val_loss: 3.1621 - val_accuracy: 0.2188\nEpoch 8/100\n1140/1140 [==============================] - 1s 952us/step - loss: 3.1102 - accuracy: 0.2282 - val_loss: 3.1243 - val_accuracy: 0.2245\nEpoch 9/100\n1140/1140 [==============================] - 1s 982us/step - loss: 3.0760 - accuracy: 0.2336 - val_loss: 3.1059 - val_accuracy: 0.2255\nEpoch 10/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 3.0481 - accuracy: 0.2377 - val_loss: 3.0796 - val_accuracy: 0.2313\nEpoch 11/100\n1140/1140 [==============================] - 1s 826us/step - loss: 3.0223 - accuracy: 0.2435 - val_loss: 3.0565 - val_accuracy: 0.2336\nEpoch 12/100\n1140/1140 [==============================] - 1s 981us/step - loss: 3.0002 - accuracy: 0.2463 - val_loss: 3.0352 - val_accuracy: 0.2373\nEpoch 13/100\n1140/1140 [==============================] - 1s 881us/step - loss: 2.9796 - accuracy: 0.2490 - val_loss: 3.0264 - val_accuracy: 0.2391\nEpoch 14/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.9615 - accuracy: 0.2527 - val_loss: 3.0088 - val_accuracy: 0.2438\nEpoch 15/100\n1140/1140 [==============================] - 1s 844us/step - loss: 2.9458 - accuracy: 0.2550 - val_loss: 2.9983 - val_accuracy: 0.2458\nEpoch 16/100\n1140/1140 [==============================] - 1s 808us/step - loss: 2.9302 - accuracy: 0.2576 - val_loss: 2.9852 - val_accuracy: 0.2484\nEpoch 17/100\n1140/1140 [==============================] - 1s 783us/step - loss: 2.9173 - accuracy: 0.2602 - val_loss: 2.9705 - val_accuracy: 0.2519\nEpoch 18/100\n1140/1140 [==============================] - 1s 939us/step - loss: 2.9024 - accuracy: 0.2626 - val_loss: 2.9599 - val_accuracy: 0.2507\nEpoch 19/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.8910 - accuracy: 0.2647 - val_loss: 2.9544 - val_accuracy: 0.2518\nEpoch 20/100\n1140/1140 [==============================] - 1s 913us/step - loss: 2.8781 - accuracy: 0.2678 - val_loss: 2.9425 - val_accuracy: 0.2576\nEpoch 21/100\n1140/1140 [==============================] - 1s 928us/step - loss: 2.8670 - accuracy: 0.2706 - val_loss: 2.9338 - val_accuracy: 0.2546\nEpoch 22/100\n1140/1140 [==============================] - 1s 776us/step - loss: 2.8558 - accuracy: 0.2723 - val_loss: 2.9197 - val_accuracy: 0.2604\nEpoch 23/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.8452 - accuracy: 0.2727 - val_loss: 2.9109 - val_accuracy: 0.2618\nEpoch 24/100\n1140/1140 [==============================] - 1s 785us/step - loss: 2.8357 - accuracy: 0.2757 - val_loss: 2.9062 - val_accuracy: 0.2630\nEpoch 25/100\n1140/1140 [==============================] - 1s 952us/step - loss: 2.8249 - accuracy: 0.2773 - val_loss: 2.8928 - val_accuracy: 0.2656\nEpoch 26/100\n1140/1140 [==============================] - 1s 872us/step - loss: 2.8142 - accuracy: 0.2797 - val_loss: 2.8884 - val_accuracy: 0.2615\nEpoch 27/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.8053 - accuracy: 0.2825 - val_loss: 2.8851 - val_accuracy: 0.2663\nEpoch 28/100\n1140/1140 [==============================] - 1s 788us/step - loss: 2.7953 - accuracy: 0.2837 - val_loss: 2.8747 - val_accuracy: 0.2673\nEpoch 29/100\n1140/1140 [==============================] - 1s 774us/step - loss: 2.7860 - accuracy: 0.2853 - val_loss: 2.8657 - val_accuracy: 0.2674\nEpoch 30/100\n1140/1140 [==============================] - 1s 890us/step - loss: 2.7771 - accuracy: 0.2872 - val_loss: 2.8572 - val_accuracy: 0.2723\nEpoch 31/100\n1140/1140 [==============================] - 1s 887us/step - loss: 2.7690 - accuracy: 0.2884 - val_loss: 2.8547 - val_accuracy: 0.2725\nEpoch 32/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.7604 - accuracy: 0.2896 - val_loss: 2.8492 - val_accuracy: 0.2770\nEpoch 33/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.7528 - accuracy: 0.2911 - val_loss: 2.8409 - val_accuracy: 0.2784\nEpoch 34/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.7448 - accuracy: 0.2918 - val_loss: 2.8370 - val_accuracy: 0.2741\nEpoch 35/100\n1140/1140 [==============================] - 1s 791us/step - loss: 2.7371 - accuracy: 0.2946 - val_loss: 2.8263 - val_accuracy: 0.2791\nEpoch 36/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.7291 - accuracy: 0.2954 - val_loss: 2.8271 - val_accuracy: 0.2773\nEpoch 37/100\n1140/1140 [==============================] - 1s 881us/step - loss: 2.7226 - accuracy: 0.2969 - val_loss: 2.8274 - val_accuracy: 0.2753\nEpoch 38/100\n1140/1140 [==============================] - 1s 935us/step - loss: 2.7165 - accuracy: 0.2975 - val_loss: 2.8135 - val_accuracy: 0.2808\nEpoch 39/100\n1140/1140 [==============================] - 1s 891us/step - loss: 2.7105 - accuracy: 0.2987 - val_loss: 2.8134 - val_accuracy: 0.2802\nEpoch 40/100\n1140/1140 [==============================] - 1s 998us/step - loss: 2.7035 - accuracy: 0.3008 - val_loss: 2.8034 - val_accuracy: 0.2820\nEpoch 41/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6981 - accuracy: 0.3009 - val_loss: 2.7982 - val_accuracy: 0.2817\nEpoch 42/100\n1140/1140 [==============================] - 1s 877us/step - loss: 2.6916 - accuracy: 0.3034 - val_loss: 2.8025 - val_accuracy: 0.2853\nEpoch 43/100\n1140/1140 [==============================] - 1s 882us/step - loss: 2.6863 - accuracy: 0.3043 - val_loss: 2.7939 - val_accuracy: 0.2836\nEpoch 44/100\n1140/1140 [==============================] - 1s 803us/step - loss: 2.6810 - accuracy: 0.3047 - val_loss: 2.7947 - val_accuracy: 0.2829\nEpoch 45/100\n1140/1140 [==============================] - 1s 817us/step - loss: 2.6761 - accuracy: 0.3050 - val_loss: 2.7875 - val_accuracy: 0.2884\nEpoch 46/100\n1140/1140 [==============================] - 1s 981us/step - loss: 2.6710 - accuracy: 0.3067 - val_loss: 2.7822 - val_accuracy: 0.2889\nEpoch 47/100\n1140/1140 [==============================] - 1s 765us/step - loss: 2.6655 - accuracy: 0.3077 - val_loss: 2.7784 - val_accuracy: 0.2890\nEpoch 48/100\n1140/1140 [==============================] - 1s 762us/step - loss: 2.6599 - accuracy: 0.3075 - val_loss: 2.7815 - val_accuracy: 0.2855\nEpoch 49/100\n1140/1140 [==============================] - 1s 914us/step - loss: 2.6567 - accuracy: 0.3091 - val_loss: 2.7758 - val_accuracy: 0.2877\nEpoch 50/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.6506 - accuracy: 0.3094 - val_loss: 2.7686 - val_accuracy: 0.2918\nEpoch 51/100\n1140/1140 [==============================] - 1s 859us/step - loss: 2.6462 - accuracy: 0.3111 - val_loss: 2.7665 - val_accuracy: 0.2902\nEpoch 52/100\n1140/1140 [==============================] - 1s 861us/step - loss: 2.6431 - accuracy: 0.3121 - val_loss: 2.7668 - val_accuracy: 0.2921\nEpoch 53/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6376 - accuracy: 0.3117 - val_loss: 2.7688 - val_accuracy: 0.2883\nEpoch 54/100\n1140/1140 [==============================] - 1s 933us/step - loss: 2.6338 - accuracy: 0.3125 - val_loss: 2.7598 - val_accuracy: 0.2909\nEpoch 55/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.6300 - accuracy: 0.3143 - val_loss: 2.7579 - val_accuracy: 0.2899\nEpoch 56/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6257 - accuracy: 0.3137 - val_loss: 2.7545 - val_accuracy: 0.2929\nEpoch 57/100\n1140/1140 [==============================] - 3s 3ms/step - loss: 2.6228 - accuracy: 0.3150 - val_loss: 2.7537 - val_accuracy: 0.2917\nEpoch 58/100\n1140/1140 [==============================] - 1s 930us/step - loss: 2.6180 - accuracy: 0.3173 - val_loss: 2.7517 - val_accuracy: 0.2893\nEpoch 59/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6145 - accuracy: 0.3154 - val_loss: 2.7592 - val_accuracy: 0.2922\nEpoch 60/100\n1140/1140 [==============================] - 1s 777us/step - loss: 2.6106 - accuracy: 0.3178 - val_loss: 2.7530 - val_accuracy: 0.2904\nEpoch 61/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.6073 - accuracy: 0.3176 - val_loss: 2.7502 - val_accuracy: 0.2913\nEpoch 62/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6034 - accuracy: 0.3184 - val_loss: 2.7412 - val_accuracy: 0.2950\nEpoch 63/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5993 - accuracy: 0.3202 - val_loss: 2.7383 - val_accuracy: 0.2944\nEpoch 64/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5964 - accuracy: 0.3205 - val_loss: 2.7389 - val_accuracy: 0.2925\nEpoch 65/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5921 - accuracy: 0.3211 - val_loss: 2.7391 - val_accuracy: 0.2951\nEpoch 66/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5900 - accuracy: 0.3199 - val_loss: 2.7364 - val_accuracy: 0.2939\nEpoch 67/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5869 - accuracy: 0.3206 - val_loss: 2.7388 - val_accuracy: 0.2939\nEpoch 68/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5833 - accuracy: 0.3225 - val_loss: 2.7304 - val_accuracy: 0.2976\nEpoch 69/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.5805 - accuracy: 0.3234 - val_loss: 2.7297 - val_accuracy: 0.2954\nEpoch 70/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.5774 - accuracy: 0.3225 - val_loss: 2.7278 - val_accuracy: 0.2976\nEpoch 71/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5736 - accuracy: 0.3244 - val_loss: 2.7215 - val_accuracy: 0.2979\nEpoch 72/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5711 - accuracy: 0.3240 - val_loss: 2.7260 - val_accuracy: 0.2962\nEpoch 73/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5691 - accuracy: 0.3246 - val_loss: 2.7235 - val_accuracy: 0.3001\nEpoch 74/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5662 - accuracy: 0.3263 - val_loss: 2.7266 - val_accuracy: 0.2959\nEpoch 75/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.5622 - accuracy: 0.3259 - val_loss: 2.7251 - val_accuracy: 0.2958\nEpoch 76/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.5606 - accuracy: 0.3264 - val_loss: 2.7206 - val_accuracy: 0.2990\nEpoch 77/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.5581 - accuracy: 0.3262 - val_loss: 2.7161 - val_accuracy: 0.2998\nEpoch 78/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5549 - accuracy: 0.3282 - val_loss: 2.7244 - val_accuracy: 0.2951\nEpoch 79/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5528 - accuracy: 0.3282 - val_loss: 2.7201 - val_accuracy: 0.2980\nEpoch 80/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5499 - accuracy: 0.3274 - val_loss: 2.7109 - val_accuracy: 0.3003\nEpoch 81/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5469 - accuracy: 0.3284 - val_loss: 2.7149 - val_accuracy: 0.2988\nEpoch 82/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5450 - accuracy: 0.3275 - val_loss: 2.7121 - val_accuracy: 0.2982\nEpoch 83/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5413 - accuracy: 0.3307 - val_loss: 2.7181 - val_accuracy: 0.3008\nEpoch 84/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5403 - accuracy: 0.3305 - val_loss: 2.7077 - val_accuracy: 0.3009\nEpoch 85/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5374 - accuracy: 0.3310 - val_loss: 2.7080 - val_accuracy: 0.2989\nEpoch 86/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5358 - accuracy: 0.3309 - val_loss: 2.7008 - val_accuracy: 0.2992\nEpoch 87/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5326 - accuracy: 0.3300 - val_loss: 2.7081 - val_accuracy: 0.2967\nEpoch 88/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5308 - accuracy: 0.3307 - val_loss: 2.7094 - val_accuracy: 0.2987\nEpoch 89/100\n1140/1140 [==============================] - 3s 3ms/step - loss: 2.5284 - accuracy: 0.3333 - val_loss: 2.7039 - val_accuracy: 0.3024\nEpoch 90/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5261 - accuracy: 0.3322 - val_loss: 2.7021 - val_accuracy: 0.3013\nEpoch 91/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5232 - accuracy: 0.3324 - val_loss: 2.7016 - val_accuracy: 0.2995\nEpoch 92/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5214 - accuracy: 0.3343 - val_loss: 2.6985 - val_accuracy: 0.3019\nEpoch 93/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5190 - accuracy: 0.3331 - val_loss: 2.7020 - val_accuracy: 0.3039\nEpoch 94/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5172 - accuracy: 0.3341 - val_loss: 2.6988 - val_accuracy: 0.3025\nEpoch 95/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.5159 - accuracy: 0.3355 - val_loss: 2.6948 - val_accuracy: 0.3039\nEpoch 96/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5136 - accuracy: 0.3358 - val_loss: 2.6997 - val_accuracy: 0.3044\nEpoch 97/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5115 - accuracy: 0.3357 - val_loss: 2.6977 - val_accuracy: 0.3017\nEpoch 98/100\n1140/1140 [==============================] - 1s 880us/step - loss: 2.5104 - accuracy: 0.3352 - val_loss: 2.6880 - val_accuracy: 0.3043\nEpoch 99/100\n1140/1140 [==============================] - 1s 886us/step - loss: 2.5075 - accuracy: 0.3357 - val_loss: 2.6887 - val_accuracy: 0.3043\nEpoch 100/100\n1140/1140 [==============================] - 1s 914us/step - loss: 2.5051 - accuracy: 0.3372 - val_loss: 2.6889 - val_accuracy: 0.3046\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 12,
      "block_group": "8c9d1e9a1aba4992bea202b785fe0a15",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model2_2.evaluate(X_train2_2, y_train2_2))\nprint(\"Test Accuracy:\", model2_2.evaluate(X_test2_2, y_test2_2))",
      "metadata": {
        "cell_id": "1bea104e93e1489da32663812f98dedc",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2850/2850 [==============================] - 1s 404us/step - loss: 2.5267 - accuracy: 0.3333\nTraining Accuracy: [2.5267410278320312, 0.3333260118961334]\n713/713 [==============================] - 0s 397us/step - loss: 2.6443 - accuracy: 0.3075\nTest Accuracy: [2.6442580223083496, 0.3074561357498169]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 13,
      "block_group": "d62c45fe7c9c40cabd628cf64562e55c",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<h3>Increasing Hidden Layers</h3>",
      "metadata": {
        "cell_id": "9149a2d4316e477fb3cb1d5b258fb5ad",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "08297957c1d343a1b4963961b73739a4"
    },
    {
      "cell_type": "code",
      "source": "model2_3 = Sequential()\nmodel2_3.add(Dense(256, input_shape=(14,), activation=\"relu\"))\nmodel2_3.add(Dense(128, activation=\"relu\"))\n#LayerNormalization()\nmodel2_3.add(Dense(114, activation=\"softmax\"))\n\nmodel2_3.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",\n\tmetrics=[\"accuracy\"]) #use sparse for loss because label is integer and not vector\nH2_3 = model2_3.fit(X_train2_2, y_train2_2, validation_split=0.2,\n\tepochs=100, batch_size=64, shuffle=True)",
      "metadata": {
        "cell_id": "a45f5fd8318748c4b4f08a4e5f62d1e1",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 3.7763 - accuracy: 0.1259 - val_loss: 3.4675 - val_accuracy: 0.1584\nEpoch 2/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.3300 - accuracy: 0.1868 - val_loss: 3.2820 - val_accuracy: 0.1867\nEpoch 3/100\n1140/1140 [==============================] - 1s 923us/step - loss: 3.1996 - accuracy: 0.2053 - val_loss: 3.1719 - val_accuracy: 0.2065\nEpoch 4/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.1095 - accuracy: 0.2201 - val_loss: 3.1060 - val_accuracy: 0.2203\nEpoch 5/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.0386 - accuracy: 0.2339 - val_loss: 3.0550 - val_accuracy: 0.2279\nEpoch 6/100\n1140/1140 [==============================] - 1s 982us/step - loss: 2.9806 - accuracy: 0.2433 - val_loss: 2.9962 - val_accuracy: 0.2383\nEpoch 7/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.9315 - accuracy: 0.2529 - val_loss: 2.9518 - val_accuracy: 0.2482\nEpoch 8/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.8867 - accuracy: 0.2601 - val_loss: 2.9163 - val_accuracy: 0.2547\nEpoch 9/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.8497 - accuracy: 0.2671 - val_loss: 2.8850 - val_accuracy: 0.2605\nEpoch 10/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.8181 - accuracy: 0.2728 - val_loss: 2.8582 - val_accuracy: 0.2650\nEpoch 11/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.7874 - accuracy: 0.2779 - val_loss: 2.8448 - val_accuracy: 0.2674\nEpoch 12/100\n1140/1140 [==============================] - 1s 930us/step - loss: 2.7640 - accuracy: 0.2816 - val_loss: 2.8296 - val_accuracy: 0.2627\nEpoch 13/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.7431 - accuracy: 0.2850 - val_loss: 2.8018 - val_accuracy: 0.2734\nEpoch 14/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.7212 - accuracy: 0.2869 - val_loss: 2.8023 - val_accuracy: 0.2770\nEpoch 15/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.7049 - accuracy: 0.2908 - val_loss: 2.7913 - val_accuracy: 0.2798\nEpoch 16/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.6879 - accuracy: 0.2945 - val_loss: 2.7857 - val_accuracy: 0.2844\nEpoch 17/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.6721 - accuracy: 0.2977 - val_loss: 2.7508 - val_accuracy: 0.2861\nEpoch 18/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.6562 - accuracy: 0.3003 - val_loss: 2.7712 - val_accuracy: 0.2831\nEpoch 19/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.6434 - accuracy: 0.3023 - val_loss: 2.7459 - val_accuracy: 0.2897\nEpoch 20/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.6296 - accuracy: 0.3043 - val_loss: 2.7342 - val_accuracy: 0.2885\nEpoch 21/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6158 - accuracy: 0.3067 - val_loss: 2.7134 - val_accuracy: 0.2935\nEpoch 22/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6060 - accuracy: 0.3101 - val_loss: 2.7040 - val_accuracy: 0.2934\nEpoch 23/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5923 - accuracy: 0.3122 - val_loss: 2.7150 - val_accuracy: 0.2953\nEpoch 24/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5813 - accuracy: 0.3159 - val_loss: 2.6963 - val_accuracy: 0.2971\nEpoch 25/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5720 - accuracy: 0.3152 - val_loss: 2.6951 - val_accuracy: 0.3004\nEpoch 26/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5631 - accuracy: 0.3183 - val_loss: 2.6837 - val_accuracy: 0.3003\nEpoch 27/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.5555 - accuracy: 0.3196 - val_loss: 2.6822 - val_accuracy: 0.2994\nEpoch 28/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.5466 - accuracy: 0.3205 - val_loss: 2.6849 - val_accuracy: 0.2984\nEpoch 29/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.5391 - accuracy: 0.3221 - val_loss: 2.6931 - val_accuracy: 0.2952\nEpoch 30/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.5320 - accuracy: 0.3231 - val_loss: 2.6746 - val_accuracy: 0.2970\nEpoch 31/100\n1140/1140 [==============================] - 3s 2ms/step - loss: 2.5265 - accuracy: 0.3243 - val_loss: 2.6694 - val_accuracy: 0.3036\nEpoch 32/100\n1140/1140 [==============================] - 4s 3ms/step - loss: 2.5173 - accuracy: 0.3280 - val_loss: 2.6919 - val_accuracy: 0.2973\nEpoch 33/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.5118 - accuracy: 0.3271 - val_loss: 2.6591 - val_accuracy: 0.3053\nEpoch 34/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.5056 - accuracy: 0.3289 - val_loss: 2.6510 - val_accuracy: 0.3052\nEpoch 35/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.4973 - accuracy: 0.3305 - val_loss: 2.6605 - val_accuracy: 0.3054\nEpoch 36/100\n1140/1140 [==============================] - 3s 2ms/step - loss: 2.4919 - accuracy: 0.3331 - val_loss: 2.6666 - val_accuracy: 0.3046\nEpoch 37/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.4854 - accuracy: 0.3324 - val_loss: 2.6751 - val_accuracy: 0.3003\nEpoch 38/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.4823 - accuracy: 0.3319 - val_loss: 2.6630 - val_accuracy: 0.3035\nEpoch 39/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.4743 - accuracy: 0.3347 - val_loss: 2.6521 - val_accuracy: 0.3060\nEpoch 40/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.4699 - accuracy: 0.3351 - val_loss: 2.6532 - val_accuracy: 0.3023\nEpoch 41/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.4658 - accuracy: 0.3374 - val_loss: 2.6456 - val_accuracy: 0.3052\nEpoch 42/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.4591 - accuracy: 0.3371 - val_loss: 2.6374 - val_accuracy: 0.3070\nEpoch 43/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.4527 - accuracy: 0.3386 - val_loss: 2.6392 - val_accuracy: 0.3066\nEpoch 44/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.4513 - accuracy: 0.3385 - val_loss: 2.6454 - val_accuracy: 0.3068\nEpoch 45/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.4440 - accuracy: 0.3396 - val_loss: 2.6586 - val_accuracy: 0.3039\nEpoch 46/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.4388 - accuracy: 0.3406 - val_loss: 2.6667 - val_accuracy: 0.3002\nEpoch 47/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.4338 - accuracy: 0.3419 - val_loss: 2.6477 - val_accuracy: 0.3070\nEpoch 48/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.4297 - accuracy: 0.3418 - val_loss: 2.6305 - val_accuracy: 0.3061\nEpoch 49/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.4234 - accuracy: 0.3451 - val_loss: 2.6617 - val_accuracy: 0.3032\nEpoch 50/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.4221 - accuracy: 0.3474 - val_loss: 2.6428 - val_accuracy: 0.3087\nEpoch 51/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.4183 - accuracy: 0.3466 - val_loss: 2.6411 - val_accuracy: 0.3067\nEpoch 52/100\n1140/1140 [==============================] - 3s 2ms/step - loss: 2.4117 - accuracy: 0.3463 - val_loss: 2.6362 - val_accuracy: 0.3100\nEpoch 53/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.4084 - accuracy: 0.3477 - val_loss: 2.6324 - val_accuracy: 0.3089\nEpoch 54/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.4047 - accuracy: 0.3483 - val_loss: 2.6430 - val_accuracy: 0.3050\nEpoch 55/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.4016 - accuracy: 0.3480 - val_loss: 2.6453 - val_accuracy: 0.3058\nEpoch 56/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.3956 - accuracy: 0.3503 - val_loss: 2.6298 - val_accuracy: 0.3079\nEpoch 57/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.3917 - accuracy: 0.3513 - val_loss: 2.6374 - val_accuracy: 0.3092\nEpoch 58/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.3891 - accuracy: 0.3511 - val_loss: 2.6394 - val_accuracy: 0.3085\nEpoch 59/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3856 - accuracy: 0.3507 - val_loss: 2.6308 - val_accuracy: 0.3097\nEpoch 60/100\n1140/1140 [==============================] - 1s 916us/step - loss: 2.3812 - accuracy: 0.3525 - val_loss: 2.6588 - val_accuracy: 0.3041\nEpoch 61/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3783 - accuracy: 0.3542 - val_loss: 2.6342 - val_accuracy: 0.3099\nEpoch 62/100\n1140/1140 [==============================] - 1s 935us/step - loss: 2.3729 - accuracy: 0.3546 - val_loss: 2.6372 - val_accuracy: 0.3065\nEpoch 63/100\n1140/1140 [==============================] - 1s 929us/step - loss: 2.3705 - accuracy: 0.3545 - val_loss: 2.6374 - val_accuracy: 0.3104\nEpoch 64/100\n1140/1140 [==============================] - 1s 938us/step - loss: 2.3682 - accuracy: 0.3563 - val_loss: 2.6264 - val_accuracy: 0.3086\nEpoch 65/100\n1140/1140 [==============================] - 1s 940us/step - loss: 2.3632 - accuracy: 0.3568 - val_loss: 2.6498 - val_accuracy: 0.3042\nEpoch 66/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3619 - accuracy: 0.3571 - val_loss: 2.6264 - val_accuracy: 0.3108\nEpoch 67/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3565 - accuracy: 0.3581 - val_loss: 2.6410 - val_accuracy: 0.3061\nEpoch 68/100\n1140/1140 [==============================] - 1s 950us/step - loss: 2.3541 - accuracy: 0.3568 - val_loss: 2.6455 - val_accuracy: 0.3060\nEpoch 69/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3508 - accuracy: 0.3594 - val_loss: 2.6422 - val_accuracy: 0.3078\nEpoch 70/100\n1140/1140 [==============================] - 1s 948us/step - loss: 2.3475 - accuracy: 0.3603 - val_loss: 2.6409 - val_accuracy: 0.3099\nEpoch 71/100\n1140/1140 [==============================] - 1s 943us/step - loss: 2.3443 - accuracy: 0.3607 - val_loss: 2.6416 - val_accuracy: 0.3104\nEpoch 72/100\n1140/1140 [==============================] - 1s 933us/step - loss: 2.3417 - accuracy: 0.3599 - val_loss: 2.6392 - val_accuracy: 0.3082\nEpoch 73/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3360 - accuracy: 0.3626 - val_loss: 2.6203 - val_accuracy: 0.3131\nEpoch 74/100\n1140/1140 [==============================] - 1s 988us/step - loss: 2.3343 - accuracy: 0.3615 - val_loss: 2.6389 - val_accuracy: 0.3112\nEpoch 75/100\n1140/1140 [==============================] - 1s 939us/step - loss: 2.3326 - accuracy: 0.3612 - val_loss: 2.6318 - val_accuracy: 0.3097\nEpoch 76/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3288 - accuracy: 0.3619 - val_loss: 2.6557 - val_accuracy: 0.3052\nEpoch 77/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3249 - accuracy: 0.3642 - val_loss: 2.6354 - val_accuracy: 0.3097\nEpoch 78/100\n1140/1140 [==============================] - 1s 941us/step - loss: 2.3232 - accuracy: 0.3650 - val_loss: 2.6468 - val_accuracy: 0.3073\nEpoch 79/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3212 - accuracy: 0.3639 - val_loss: 2.6313 - val_accuracy: 0.3093\nEpoch 80/100\n1140/1140 [==============================] - 1s 945us/step - loss: 2.3155 - accuracy: 0.3644 - val_loss: 2.6567 - val_accuracy: 0.3048\nEpoch 81/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3141 - accuracy: 0.3669 - val_loss: 2.6425 - val_accuracy: 0.3079\nEpoch 82/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3113 - accuracy: 0.3665 - val_loss: 2.6488 - val_accuracy: 0.3096\nEpoch 83/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3082 - accuracy: 0.3669 - val_loss: 2.6447 - val_accuracy: 0.3107\nEpoch 84/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.3061 - accuracy: 0.3668 - val_loss: 2.6376 - val_accuracy: 0.3123\nEpoch 85/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3032 - accuracy: 0.3681 - val_loss: 2.6457 - val_accuracy: 0.3056\nEpoch 86/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.3014 - accuracy: 0.3675 - val_loss: 2.6439 - val_accuracy: 0.3085\nEpoch 87/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.2960 - accuracy: 0.3689 - val_loss: 2.6446 - val_accuracy: 0.3075\nEpoch 88/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.2964 - accuracy: 0.3679 - val_loss: 2.6425 - val_accuracy: 0.3091\nEpoch 89/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.2924 - accuracy: 0.3695 - val_loss: 2.6413 - val_accuracy: 0.3103\nEpoch 90/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.2904 - accuracy: 0.3704 - val_loss: 2.6450 - val_accuracy: 0.3069\nEpoch 91/100\n1140/1140 [==============================] - 1s 982us/step - loss: 2.2884 - accuracy: 0.3713 - val_loss: 2.6379 - val_accuracy: 0.3087\nEpoch 92/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.2825 - accuracy: 0.3723 - val_loss: 2.6512 - val_accuracy: 0.3083\nEpoch 93/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.2829 - accuracy: 0.3714 - val_loss: 2.6446 - val_accuracy: 0.3095\nEpoch 94/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.2794 - accuracy: 0.3704 - val_loss: 2.6434 - val_accuracy: 0.3043\nEpoch 95/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.2768 - accuracy: 0.3730 - val_loss: 2.6470 - val_accuracy: 0.3064\nEpoch 96/100\n1140/1140 [==============================] - 1s 969us/step - loss: 2.2759 - accuracy: 0.3726 - val_loss: 2.6422 - val_accuracy: 0.3095\nEpoch 97/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.2712 - accuracy: 0.3737 - val_loss: 2.6576 - val_accuracy: 0.3063\nEpoch 98/100\n1140/1140 [==============================] - 1s 995us/step - loss: 2.2725 - accuracy: 0.3720 - val_loss: 2.6391 - val_accuracy: 0.3123\nEpoch 99/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.2667 - accuracy: 0.3737 - val_loss: 2.6621 - val_accuracy: 0.3058\nEpoch 100/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.2663 - accuracy: 0.3753 - val_loss: 2.6542 - val_accuracy: 0.3060\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 14,
      "block_group": "337e1e5e7c804d0da83df06c526c09a7",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model2_3.evaluate(X_train2_2, y_train2_2))\nprint(\"Test Accuracy:\", model2_3.evaluate(X_test2_2, y_test2_2))",
      "metadata": {
        "cell_id": "5751d766d4c84fddb1eae2bd14357764",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2850/2850 [==============================] - 1s 399us/step - loss: 2.3182 - accuracy: 0.3671\nTraining Accuracy: [2.318171739578247, 0.3670654296875]\n713/713 [==============================] - 0s 532us/step - loss: 2.5974 - accuracy: 0.3152\nTest Accuracy: [2.597400426864624, 0.31517544388771057]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 15,
      "block_group": "0c11195b5bdc49b1adfd5b68691f3515",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<h3>Halving Hidden Layer Dimension</h3>",
      "metadata": {
        "cell_id": "ea2e67905da94428956ed58cb3a7e57d",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "5350b57023d547a29d7e71f57dfeb372"
    },
    {
      "cell_type": "code",
      "source": "model2_4 = Sequential()\nmodel2_4.add(Dense(128, input_shape=(14,), activation=\"relu\"))\nmodel2_4.add(Dense(64, activation=\"relu\"))\n#LayerNormalization()\nmodel2_4.add(Dense(114, activation=\"softmax\"))\n\nmodel2_4.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",\n\tmetrics=[\"accuracy\"]) #use sparse for loss because label is integer and not vector\nH2_4 = model2_4.fit(X_train2_2, y_train2_2, validation_split=0.2,\n\tepochs=100, batch_size=64, shuffle=True)",
      "metadata": {
        "cell_id": "a4e945713f5a41f0a5924ba1db31f7c7",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n1140/1140 [==============================] - 1s 894us/step - loss: 3.9172 - accuracy: 0.1067 - val_loss: 3.6370 - val_accuracy: 0.1419\nEpoch 2/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.4919 - accuracy: 0.1671 - val_loss: 3.4008 - val_accuracy: 0.1717\nEpoch 3/100\n1140/1140 [==============================] - 1s 727us/step - loss: 3.3148 - accuracy: 0.1896 - val_loss: 3.3038 - val_accuracy: 0.1855\nEpoch 4/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.2310 - accuracy: 0.2018 - val_loss: 3.2310 - val_accuracy: 0.1973\nEpoch 5/100\n1140/1140 [==============================] - 1s 957us/step - loss: 3.1737 - accuracy: 0.2113 - val_loss: 3.1764 - val_accuracy: 0.2072\nEpoch 6/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.1271 - accuracy: 0.2186 - val_loss: 3.1379 - val_accuracy: 0.2113\nEpoch 7/100\n1140/1140 [==============================] - 1s 832us/step - loss: 3.0900 - accuracy: 0.2246 - val_loss: 3.0950 - val_accuracy: 0.2230\nEpoch 8/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.0534 - accuracy: 0.2305 - val_loss: 3.0708 - val_accuracy: 0.2294\nEpoch 9/100\n1140/1140 [==============================] - 1s 938us/step - loss: 3.0231 - accuracy: 0.2382 - val_loss: 3.0472 - val_accuracy: 0.2324\nEpoch 10/100\n1140/1140 [==============================] - 1s 805us/step - loss: 2.9937 - accuracy: 0.2431 - val_loss: 3.0238 - val_accuracy: 0.2369\nEpoch 11/100\n1140/1140 [==============================] - 1s 814us/step - loss: 2.9708 - accuracy: 0.2480 - val_loss: 2.9954 - val_accuracy: 0.2427\nEpoch 12/100\n1140/1140 [==============================] - 1s 843us/step - loss: 2.9491 - accuracy: 0.2512 - val_loss: 2.9935 - val_accuracy: 0.2418\nEpoch 13/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.9269 - accuracy: 0.2568 - val_loss: 2.9793 - val_accuracy: 0.2452\nEpoch 14/100\n1140/1140 [==============================] - 1s 778us/step - loss: 2.9061 - accuracy: 0.2598 - val_loss: 2.9460 - val_accuracy: 0.2550\nEpoch 15/100\n1140/1140 [==============================] - 1s 732us/step - loss: 2.8866 - accuracy: 0.2625 - val_loss: 2.9263 - val_accuracy: 0.2544\nEpoch 16/100\n1140/1140 [==============================] - 1s 752us/step - loss: 2.8694 - accuracy: 0.2652 - val_loss: 2.9116 - val_accuracy: 0.2578\nEpoch 17/100\n1140/1140 [==============================] - 1s 776us/step - loss: 2.8536 - accuracy: 0.2671 - val_loss: 2.8919 - val_accuracy: 0.2651\nEpoch 18/100\n1140/1140 [==============================] - 1s 916us/step - loss: 2.8364 - accuracy: 0.2703 - val_loss: 2.8868 - val_accuracy: 0.2635\nEpoch 19/100\n1140/1140 [==============================] - 1s 747us/step - loss: 2.8232 - accuracy: 0.2753 - val_loss: 2.8686 - val_accuracy: 0.2689\nEpoch 20/100\n1140/1140 [==============================] - 1s 750us/step - loss: 2.8114 - accuracy: 0.2754 - val_loss: 2.8712 - val_accuracy: 0.2649\nEpoch 21/100\n1140/1140 [==============================] - 1s 729us/step - loss: 2.8007 - accuracy: 0.2767 - val_loss: 2.8698 - val_accuracy: 0.2641\nEpoch 22/100\n1140/1140 [==============================] - 1s 972us/step - loss: 2.7887 - accuracy: 0.2786 - val_loss: 2.8621 - val_accuracy: 0.2622\nEpoch 23/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.7809 - accuracy: 0.2807 - val_loss: 2.8459 - val_accuracy: 0.2697\nEpoch 24/100\n1140/1140 [==============================] - 1s 814us/step - loss: 2.7710 - accuracy: 0.2813 - val_loss: 2.8373 - val_accuracy: 0.2702\nEpoch 25/100\n1140/1140 [==============================] - 1s 779us/step - loss: 2.7619 - accuracy: 0.2831 - val_loss: 2.8251 - val_accuracy: 0.2752\nEpoch 26/100\n1140/1140 [==============================] - 1s 815us/step - loss: 2.7541 - accuracy: 0.2860 - val_loss: 2.8222 - val_accuracy: 0.2740\nEpoch 27/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.7452 - accuracy: 0.2854 - val_loss: 2.8137 - val_accuracy: 0.2723\nEpoch 28/100\n1140/1140 [==============================] - 1s 787us/step - loss: 2.7392 - accuracy: 0.2871 - val_loss: 2.8163 - val_accuracy: 0.2743\nEpoch 29/100\n1140/1140 [==============================] - 1s 804us/step - loss: 2.7308 - accuracy: 0.2908 - val_loss: 2.7996 - val_accuracy: 0.2806\nEpoch 30/100\n1140/1140 [==============================] - 1s 758us/step - loss: 2.7262 - accuracy: 0.2906 - val_loss: 2.7983 - val_accuracy: 0.2807\nEpoch 31/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.7185 - accuracy: 0.2926 - val_loss: 2.7968 - val_accuracy: 0.2785\nEpoch 32/100\n1140/1140 [==============================] - 1s 779us/step - loss: 2.7133 - accuracy: 0.2909 - val_loss: 2.7961 - val_accuracy: 0.2760\nEpoch 33/100\n1140/1140 [==============================] - 1s 757us/step - loss: 2.7070 - accuracy: 0.2929 - val_loss: 2.7856 - val_accuracy: 0.2821\nEpoch 34/100\n1140/1140 [==============================] - 1s 736us/step - loss: 2.7023 - accuracy: 0.2944 - val_loss: 2.7793 - val_accuracy: 0.2816\nEpoch 35/100\n1140/1140 [==============================] - 1s 989us/step - loss: 2.6989 - accuracy: 0.2935 - val_loss: 2.7692 - val_accuracy: 0.2827\nEpoch 36/100\n1140/1140 [==============================] - 1s 740us/step - loss: 2.6916 - accuracy: 0.2961 - val_loss: 2.7804 - val_accuracy: 0.2820\nEpoch 37/100\n1140/1140 [==============================] - 1s 834us/step - loss: 2.6868 - accuracy: 0.2970 - val_loss: 2.7836 - val_accuracy: 0.2767\nEpoch 38/100\n1140/1140 [==============================] - 1s 804us/step - loss: 2.6833 - accuracy: 0.2984 - val_loss: 2.7741 - val_accuracy: 0.2787\nEpoch 39/100\n1140/1140 [==============================] - 1s 964us/step - loss: 2.6801 - accuracy: 0.2982 - val_loss: 2.7636 - val_accuracy: 0.2804\nEpoch 40/100\n1140/1140 [==============================] - 1s 730us/step - loss: 2.6726 - accuracy: 0.3000 - val_loss: 2.7682 - val_accuracy: 0.2805\nEpoch 41/100\n1140/1140 [==============================] - 1s 770us/step - loss: 2.6706 - accuracy: 0.3006 - val_loss: 2.7734 - val_accuracy: 0.2815\nEpoch 42/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6668 - accuracy: 0.3001 - val_loss: 2.7484 - val_accuracy: 0.2887\nEpoch 43/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.6651 - accuracy: 0.3010 - val_loss: 2.7649 - val_accuracy: 0.2817\nEpoch 44/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6598 - accuracy: 0.3014 - val_loss: 2.7570 - val_accuracy: 0.2815\nEpoch 45/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.6570 - accuracy: 0.3029 - val_loss: 2.7570 - val_accuracy: 0.2848\nEpoch 46/100\n1140/1140 [==============================] - 1s 971us/step - loss: 2.6540 - accuracy: 0.3032 - val_loss: 2.7384 - val_accuracy: 0.2876\nEpoch 47/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6517 - accuracy: 0.3020 - val_loss: 2.7571 - val_accuracy: 0.2849\nEpoch 48/100\n1140/1140 [==============================] - 1s 885us/step - loss: 2.6465 - accuracy: 0.3048 - val_loss: 2.7476 - val_accuracy: 0.2846\nEpoch 49/100\n1140/1140 [==============================] - 1s 822us/step - loss: 2.6446 - accuracy: 0.3043 - val_loss: 2.7378 - val_accuracy: 0.2859\nEpoch 50/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6409 - accuracy: 0.3049 - val_loss: 2.7324 - val_accuracy: 0.2887\nEpoch 51/100\n1140/1140 [==============================] - 1s 830us/step - loss: 2.6379 - accuracy: 0.3057 - val_loss: 2.7370 - val_accuracy: 0.2918\nEpoch 52/100\n1140/1140 [==============================] - 1s 810us/step - loss: 2.6350 - accuracy: 0.3054 - val_loss: 2.7402 - val_accuracy: 0.2878\nEpoch 53/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6310 - accuracy: 0.3076 - val_loss: 2.7224 - val_accuracy: 0.2916\nEpoch 54/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6285 - accuracy: 0.3071 - val_loss: 2.7335 - val_accuracy: 0.2882\nEpoch 55/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6258 - accuracy: 0.3072 - val_loss: 2.7260 - val_accuracy: 0.2895\nEpoch 56/100\n1140/1140 [==============================] - 1s 986us/step - loss: 2.6214 - accuracy: 0.3081 - val_loss: 2.7218 - val_accuracy: 0.2906\nEpoch 57/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6193 - accuracy: 0.3104 - val_loss: 2.7213 - val_accuracy: 0.2901\nEpoch 58/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6144 - accuracy: 0.3090 - val_loss: 2.7175 - val_accuracy: 0.2894\nEpoch 59/100\n1140/1140 [==============================] - 1s 981us/step - loss: 2.6108 - accuracy: 0.3106 - val_loss: 2.7077 - val_accuracy: 0.2956\nEpoch 60/100\n1140/1140 [==============================] - 1s 741us/step - loss: 2.6090 - accuracy: 0.3112 - val_loss: 2.7171 - val_accuracy: 0.2921\nEpoch 61/100\n1140/1140 [==============================] - 1s 806us/step - loss: 2.6066 - accuracy: 0.3114 - val_loss: 2.7060 - val_accuracy: 0.2953\nEpoch 62/100\n1140/1140 [==============================] - 1s 820us/step - loss: 2.6036 - accuracy: 0.3131 - val_loss: 2.7006 - val_accuracy: 0.2954\nEpoch 63/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5990 - accuracy: 0.3113 - val_loss: 2.7081 - val_accuracy: 0.2934\nEpoch 64/100\n1140/1140 [==============================] - 1s 836us/step - loss: 2.5982 - accuracy: 0.3118 - val_loss: 2.7091 - val_accuracy: 0.2891\nEpoch 65/100\n1140/1140 [==============================] - 1s 859us/step - loss: 2.5936 - accuracy: 0.3154 - val_loss: 2.7065 - val_accuracy: 0.2906\nEpoch 66/100\n1140/1140 [==============================] - 1s 733us/step - loss: 2.5909 - accuracy: 0.3150 - val_loss: 2.7029 - val_accuracy: 0.2927\nEpoch 67/100\n1140/1140 [==============================] - 1s 852us/step - loss: 2.5894 - accuracy: 0.3145 - val_loss: 2.6925 - val_accuracy: 0.2972\nEpoch 68/100\n1140/1140 [==============================] - 1s 726us/step - loss: 2.5867 - accuracy: 0.3152 - val_loss: 2.7082 - val_accuracy: 0.2900\nEpoch 69/100\n1140/1140 [==============================] - 1s 778us/step - loss: 2.5826 - accuracy: 0.3158 - val_loss: 2.7011 - val_accuracy: 0.2941\nEpoch 70/100\n1140/1140 [==============================] - 5s 4ms/step - loss: 2.5805 - accuracy: 0.3167 - val_loss: 2.7215 - val_accuracy: 0.2899\nEpoch 71/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5789 - accuracy: 0.3166 - val_loss: 2.6870 - val_accuracy: 0.2956\nEpoch 72/100\n1140/1140 [==============================] - 1s 911us/step - loss: 2.5761 - accuracy: 0.3166 - val_loss: 2.6875 - val_accuracy: 0.2970\nEpoch 73/100\n1140/1140 [==============================] - 1s 913us/step - loss: 2.5732 - accuracy: 0.3176 - val_loss: 2.6869 - val_accuracy: 0.2948\nEpoch 74/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5729 - accuracy: 0.3177 - val_loss: 2.6903 - val_accuracy: 0.2951\nEpoch 75/100\n1140/1140 [==============================] - 1s 954us/step - loss: 2.5703 - accuracy: 0.3175 - val_loss: 2.6937 - val_accuracy: 0.2956\nEpoch 76/100\n1140/1140 [==============================] - 1s 937us/step - loss: 2.5684 - accuracy: 0.3183 - val_loss: 2.6889 - val_accuracy: 0.2972\nEpoch 77/100\n1140/1140 [==============================] - 1s 911us/step - loss: 2.5642 - accuracy: 0.3189 - val_loss: 2.6871 - val_accuracy: 0.2957\nEpoch 78/100\n1140/1140 [==============================] - 1s 859us/step - loss: 2.5646 - accuracy: 0.3192 - val_loss: 2.6814 - val_accuracy: 0.2985\nEpoch 79/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5613 - accuracy: 0.3193 - val_loss: 2.6812 - val_accuracy: 0.2968\nEpoch 80/100\n1140/1140 [==============================] - 1s 862us/step - loss: 2.5614 - accuracy: 0.3187 - val_loss: 2.6897 - val_accuracy: 0.2966\nEpoch 81/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5574 - accuracy: 0.3188 - val_loss: 2.6785 - val_accuracy: 0.3005\nEpoch 82/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5564 - accuracy: 0.3208 - val_loss: 2.6763 - val_accuracy: 0.2998\nEpoch 83/100\n1140/1140 [==============================] - 1s 937us/step - loss: 2.5550 - accuracy: 0.3213 - val_loss: 2.6818 - val_accuracy: 0.2981\nEpoch 84/100\n1140/1140 [==============================] - 1s 820us/step - loss: 2.5526 - accuracy: 0.3212 - val_loss: 2.6795 - val_accuracy: 0.2941\nEpoch 85/100\n1140/1140 [==============================] - 1s 816us/step - loss: 2.5521 - accuracy: 0.3205 - val_loss: 2.6728 - val_accuracy: 0.3004\nEpoch 86/100\n1140/1140 [==============================] - 1s 995us/step - loss: 2.5510 - accuracy: 0.3230 - val_loss: 2.6699 - val_accuracy: 0.3001\nEpoch 87/100\n1140/1140 [==============================] - 1s 823us/step - loss: 2.5492 - accuracy: 0.3215 - val_loss: 2.6714 - val_accuracy: 0.2976\nEpoch 88/100\n1140/1140 [==============================] - 1s 793us/step - loss: 2.5484 - accuracy: 0.3236 - val_loss: 2.6747 - val_accuracy: 0.2968\nEpoch 89/100\n1140/1140 [==============================] - 1s 776us/step - loss: 2.5453 - accuracy: 0.3216 - val_loss: 2.6753 - val_accuracy: 0.2988\nEpoch 90/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5450 - accuracy: 0.3219 - val_loss: 2.6656 - val_accuracy: 0.3012\nEpoch 91/100\n1140/1140 [==============================] - 1s 818us/step - loss: 2.5425 - accuracy: 0.3224 - val_loss: 2.6947 - val_accuracy: 0.2964\nEpoch 92/100\n1140/1140 [==============================] - 1s 841us/step - loss: 2.5414 - accuracy: 0.3238 - val_loss: 2.6773 - val_accuracy: 0.2965\nEpoch 93/100\n1140/1140 [==============================] - 1s 935us/step - loss: 2.5394 - accuracy: 0.3233 - val_loss: 2.6694 - val_accuracy: 0.2997\nEpoch 94/100\n1140/1140 [==============================] - 1s 747us/step - loss: 2.5395 - accuracy: 0.3231 - val_loss: 2.6730 - val_accuracy: 0.2987\nEpoch 95/100\n1140/1140 [==============================] - 1s 863us/step - loss: 2.5378 - accuracy: 0.3248 - val_loss: 2.6661 - val_accuracy: 0.3020\nEpoch 96/100\n1140/1140 [==============================] - 1s 926us/step - loss: 2.5358 - accuracy: 0.3241 - val_loss: 2.6640 - val_accuracy: 0.2997\nEpoch 97/100\n1140/1140 [==============================] - 1s 954us/step - loss: 2.5347 - accuracy: 0.3246 - val_loss: 2.6763 - val_accuracy: 0.2976\nEpoch 98/100\n1140/1140 [==============================] - 1s 748us/step - loss: 2.5338 - accuracy: 0.3246 - val_loss: 2.6809 - val_accuracy: 0.2967\nEpoch 99/100\n1140/1140 [==============================] - 1s 795us/step - loss: 2.5336 - accuracy: 0.3245 - val_loss: 2.6731 - val_accuracy: 0.2982\nEpoch 100/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5309 - accuracy: 0.3252 - val_loss: 2.6722 - val_accuracy: 0.2991\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 16,
      "block_group": "5614a63be1734d6dbfc254fed6177512",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model2_4.evaluate(X_train2_2, y_train2_2))\nprint(\"Test Accuracy:\", model2_4.evaluate(X_test2_2, y_test2_2))",
      "metadata": {
        "cell_id": "52bc7925e2954b8f8a392a386a9b421c",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2850/2850 [==============================] - 1s 351us/step - loss: 2.5468 - accuracy: 0.3224\nTraining Accuracy: [2.546779155731201, 0.3223719596862793]\n713/713 [==============================] - 0s 364us/step - loss: 2.6311 - accuracy: 0.3046\nTest Accuracy: [2.631105422973633, 0.30460527539253235]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 17,
      "block_group": "1fa3e6bcb56a466590a178bb51d88e3e",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<h3>Add Hidden Layer</h3>",
      "metadata": {
        "cell_id": "d7636cb945504650b9824935d69d675c",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "2f82e4f46ce94bea96bd64db8a05d80e"
    },
    {
      "cell_type": "code",
      "source": "model2_5 = Sequential()\nmodel2_5.add(Dense(128, input_shape=(14,), activation=\"relu\"))\nmodel2_5.add(Dense(64, activation=\"relu\"))\nmodel2_5.add(Dense(32, activation=\"relu\"))\nmodel2_5.add(Dense(114, activation=\"softmax\"))\n\nmodel2_5.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",\n\tmetrics=[\"accuracy\"]) #use sparse for loss because label is integer and not vector\nH2_5 = model2_5.fit(X_train2_2, y_train2_2, validation_split=0.2,\n\tepochs=100, batch_size=64, shuffle=True)",
      "metadata": {
        "cell_id": "cf1edf1f7bc942188d0b0ee01aac575f",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n1140/1140 [==============================] - 1s 875us/step - loss: 3.9685 - accuracy: 0.0963 - val_loss: 3.6872 - val_accuracy: 0.1374\nEpoch 2/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.5202 - accuracy: 0.1609 - val_loss: 3.4204 - val_accuracy: 0.1715\nEpoch 3/100\n1140/1140 [==============================] - 1s 934us/step - loss: 3.3185 - accuracy: 0.1893 - val_loss: 3.2866 - val_accuracy: 0.1887\nEpoch 4/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.2185 - accuracy: 0.2040 - val_loss: 3.2235 - val_accuracy: 0.2018\nEpoch 5/100\n1140/1140 [==============================] - 1s 903us/step - loss: 3.1545 - accuracy: 0.2137 - val_loss: 3.1579 - val_accuracy: 0.2086\nEpoch 6/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 3.1071 - accuracy: 0.2207 - val_loss: 3.1037 - val_accuracy: 0.2178\nEpoch 7/100\n1140/1140 [==============================] - 1s 878us/step - loss: 3.0676 - accuracy: 0.2271 - val_loss: 3.0876 - val_accuracy: 0.2186\nEpoch 8/100\n1140/1140 [==============================] - 1s 899us/step - loss: 3.0334 - accuracy: 0.2335 - val_loss: 3.0668 - val_accuracy: 0.2221\nEpoch 9/100\n1140/1140 [==============================] - 1s 782us/step - loss: 3.0002 - accuracy: 0.2383 - val_loss: 3.0098 - val_accuracy: 0.2383\nEpoch 10/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.9692 - accuracy: 0.2442 - val_loss: 2.9995 - val_accuracy: 0.2376\nEpoch 11/100\n1140/1140 [==============================] - 1s 925us/step - loss: 2.9427 - accuracy: 0.2494 - val_loss: 2.9547 - val_accuracy: 0.2447\nEpoch 12/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.9196 - accuracy: 0.2525 - val_loss: 2.9564 - val_accuracy: 0.2448\nEpoch 13/100\n1140/1140 [==============================] - 1s 944us/step - loss: 2.8974 - accuracy: 0.2569 - val_loss: 2.9181 - val_accuracy: 0.2531\nEpoch 14/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.8798 - accuracy: 0.2605 - val_loss: 2.9057 - val_accuracy: 0.2556\nEpoch 15/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.8620 - accuracy: 0.2634 - val_loss: 2.9025 - val_accuracy: 0.2591\nEpoch 16/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.8481 - accuracy: 0.2673 - val_loss: 2.8860 - val_accuracy: 0.2631\nEpoch 17/100\n1140/1140 [==============================] - 1s 873us/step - loss: 2.8330 - accuracy: 0.2680 - val_loss: 2.8759 - val_accuracy: 0.2647\nEpoch 18/100\n1140/1140 [==============================] - 1s 1000us/step - loss: 2.8203 - accuracy: 0.2715 - val_loss: 2.8659 - val_accuracy: 0.2648\nEpoch 19/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.8110 - accuracy: 0.2732 - val_loss: 2.8531 - val_accuracy: 0.2674\nEpoch 20/100\n1140/1140 [==============================] - 1s 775us/step - loss: 2.7998 - accuracy: 0.2761 - val_loss: 2.8539 - val_accuracy: 0.2684\nEpoch 21/100\n1140/1140 [==============================] - 1s 907us/step - loss: 2.7908 - accuracy: 0.2764 - val_loss: 2.8626 - val_accuracy: 0.2627\nEpoch 22/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.7829 - accuracy: 0.2782 - val_loss: 2.8163 - val_accuracy: 0.2752\nEpoch 23/100\n1140/1140 [==============================] - 1s 795us/step - loss: 2.7723 - accuracy: 0.2817 - val_loss: 2.8194 - val_accuracy: 0.2736\nEpoch 24/100\n1140/1140 [==============================] - 1s 816us/step - loss: 2.7670 - accuracy: 0.2810 - val_loss: 2.8289 - val_accuracy: 0.2686\nEpoch 25/100\n1140/1140 [==============================] - 1s 776us/step - loss: 2.7570 - accuracy: 0.2830 - val_loss: 2.8202 - val_accuracy: 0.2728\nEpoch 26/100\n1140/1140 [==============================] - 1s 899us/step - loss: 2.7516 - accuracy: 0.2828 - val_loss: 2.8186 - val_accuracy: 0.2700\nEpoch 27/100\n1140/1140 [==============================] - 2s 2ms/step - loss: 2.7461 - accuracy: 0.2857 - val_loss: 2.8093 - val_accuracy: 0.2762\nEpoch 28/100\n1140/1140 [==============================] - 1s 774us/step - loss: 2.7394 - accuracy: 0.2860 - val_loss: 2.8055 - val_accuracy: 0.2758\nEpoch 29/100\n1140/1140 [==============================] - 1s 780us/step - loss: 2.7337 - accuracy: 0.2867 - val_loss: 2.8099 - val_accuracy: 0.2711\nEpoch 30/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.7300 - accuracy: 0.2871 - val_loss: 2.7909 - val_accuracy: 0.2783\nEpoch 31/100\n1140/1140 [==============================] - 1s 776us/step - loss: 2.7233 - accuracy: 0.2897 - val_loss: 2.7871 - val_accuracy: 0.2775\nEpoch 32/100\n1140/1140 [==============================] - 1s 860us/step - loss: 2.7166 - accuracy: 0.2905 - val_loss: 2.7801 - val_accuracy: 0.2772\nEpoch 33/100\n1140/1140 [==============================] - 1s 865us/step - loss: 2.7127 - accuracy: 0.2913 - val_loss: 2.7837 - val_accuracy: 0.2778\nEpoch 34/100\n1140/1140 [==============================] - 1s 973us/step - loss: 2.7105 - accuracy: 0.2916 - val_loss: 2.7792 - val_accuracy: 0.2780\nEpoch 35/100\n1140/1140 [==============================] - 1s 818us/step - loss: 2.7044 - accuracy: 0.2917 - val_loss: 2.7653 - val_accuracy: 0.2840\nEpoch 36/100\n1140/1140 [==============================] - 1s 777us/step - loss: 2.7003 - accuracy: 0.2934 - val_loss: 2.7755 - val_accuracy: 0.2793\nEpoch 37/100\n1140/1140 [==============================] - 1s 776us/step - loss: 2.6968 - accuracy: 0.2938 - val_loss: 2.7653 - val_accuracy: 0.2829\nEpoch 38/100\n1140/1140 [==============================] - 1s 924us/step - loss: 2.6936 - accuracy: 0.2948 - val_loss: 2.7831 - val_accuracy: 0.2779\nEpoch 39/100\n1140/1140 [==============================] - 1s 789us/step - loss: 2.6868 - accuracy: 0.2944 - val_loss: 2.7763 - val_accuracy: 0.2770\nEpoch 40/100\n1140/1140 [==============================] - 1s 775us/step - loss: 2.6860 - accuracy: 0.2964 - val_loss: 2.7523 - val_accuracy: 0.2882\nEpoch 41/100\n1140/1140 [==============================] - 1s 776us/step - loss: 2.6793 - accuracy: 0.2974 - val_loss: 2.7777 - val_accuracy: 0.2834\nEpoch 42/100\n1140/1140 [==============================] - 1s 772us/step - loss: 2.6768 - accuracy: 0.2961 - val_loss: 2.7589 - val_accuracy: 0.2889\nEpoch 43/100\n1140/1140 [==============================] - 1s 961us/step - loss: 2.6727 - accuracy: 0.2984 - val_loss: 2.7496 - val_accuracy: 0.2834\nEpoch 44/100\n1140/1140 [==============================] - 1s 777us/step - loss: 2.6701 - accuracy: 0.2995 - val_loss: 2.7662 - val_accuracy: 0.2819\nEpoch 45/100\n1140/1140 [==============================] - 1s 780us/step - loss: 2.6668 - accuracy: 0.3003 - val_loss: 2.7616 - val_accuracy: 0.2860\nEpoch 46/100\n1140/1140 [==============================] - 1s 911us/step - loss: 2.6639 - accuracy: 0.3002 - val_loss: 2.7557 - val_accuracy: 0.2851\nEpoch 47/100\n1140/1140 [==============================] - 1s 897us/step - loss: 2.6593 - accuracy: 0.3000 - val_loss: 2.7629 - val_accuracy: 0.2816\nEpoch 48/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6570 - accuracy: 0.3016 - val_loss: 2.7383 - val_accuracy: 0.2931\nEpoch 49/100\n1140/1140 [==============================] - 1s 993us/step - loss: 2.6531 - accuracy: 0.3025 - val_loss: 2.7353 - val_accuracy: 0.2874\nEpoch 50/100\n1140/1140 [==============================] - 1s 815us/step - loss: 2.6525 - accuracy: 0.3024 - val_loss: 2.7381 - val_accuracy: 0.2891\nEpoch 51/100\n1140/1140 [==============================] - 1s 830us/step - loss: 2.6505 - accuracy: 0.3022 - val_loss: 2.7375 - val_accuracy: 0.2854\nEpoch 52/100\n1140/1140 [==============================] - 1s 878us/step - loss: 2.6474 - accuracy: 0.3029 - val_loss: 2.7342 - val_accuracy: 0.2837\nEpoch 53/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.6434 - accuracy: 0.3047 - val_loss: 2.7362 - val_accuracy: 0.2892\nEpoch 54/100\n1140/1140 [==============================] - 1s 771us/step - loss: 2.6401 - accuracy: 0.3039 - val_loss: 2.7413 - val_accuracy: 0.2838\nEpoch 55/100\n1140/1140 [==============================] - 1s 773us/step - loss: 2.6390 - accuracy: 0.3051 - val_loss: 2.7205 - val_accuracy: 0.2948\nEpoch 56/100\n1140/1140 [==============================] - 1s 773us/step - loss: 2.6351 - accuracy: 0.3041 - val_loss: 2.7271 - val_accuracy: 0.2904\nEpoch 57/100\n1140/1140 [==============================] - 1s 771us/step - loss: 2.6330 - accuracy: 0.3062 - val_loss: 2.7354 - val_accuracy: 0.2899\nEpoch 58/100\n1140/1140 [==============================] - 1s 894us/step - loss: 2.6308 - accuracy: 0.3080 - val_loss: 2.7329 - val_accuracy: 0.2884\nEpoch 59/100\n1140/1140 [==============================] - 1s 848us/step - loss: 2.6257 - accuracy: 0.3071 - val_loss: 2.7283 - val_accuracy: 0.2865\nEpoch 60/100\n1140/1140 [==============================] - 1s 772us/step - loss: 2.6251 - accuracy: 0.3071 - val_loss: 2.7362 - val_accuracy: 0.2900\nEpoch 61/100\n1140/1140 [==============================] - 1s 777us/step - loss: 2.6227 - accuracy: 0.3096 - val_loss: 2.7131 - val_accuracy: 0.2904\nEpoch 62/100\n1140/1140 [==============================] - 1s 772us/step - loss: 2.6191 - accuracy: 0.3108 - val_loss: 2.7261 - val_accuracy: 0.2937\nEpoch 63/100\n1140/1140 [==============================] - 1s 912us/step - loss: 2.6171 - accuracy: 0.3089 - val_loss: 2.7225 - val_accuracy: 0.2912\nEpoch 64/100\n1140/1140 [==============================] - 1s 773us/step - loss: 2.6166 - accuracy: 0.3099 - val_loss: 2.7017 - val_accuracy: 0.2914\nEpoch 65/100\n1140/1140 [==============================] - 1s 775us/step - loss: 2.6102 - accuracy: 0.3109 - val_loss: 2.7045 - val_accuracy: 0.2948\nEpoch 66/100\n1140/1140 [==============================] - 1s 770us/step - loss: 2.6105 - accuracy: 0.3121 - val_loss: 2.7011 - val_accuracy: 0.2950\nEpoch 67/100\n1140/1140 [==============================] - 1s 773us/step - loss: 2.6076 - accuracy: 0.3115 - val_loss: 2.7079 - val_accuracy: 0.2924\nEpoch 68/100\n1140/1140 [==============================] - 1s 963us/step - loss: 2.6052 - accuracy: 0.3122 - val_loss: 2.7102 - val_accuracy: 0.2941\nEpoch 69/100\n1140/1140 [==============================] - 1s 777us/step - loss: 2.6030 - accuracy: 0.3125 - val_loss: 2.7280 - val_accuracy: 0.2893\nEpoch 70/100\n1140/1140 [==============================] - 1s 798us/step - loss: 2.6006 - accuracy: 0.3118 - val_loss: 2.7079 - val_accuracy: 0.2946\nEpoch 71/100\n1140/1140 [==============================] - 1s 808us/step - loss: 2.5979 - accuracy: 0.3126 - val_loss: 2.7000 - val_accuracy: 0.2984\nEpoch 72/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5966 - accuracy: 0.3138 - val_loss: 2.7032 - val_accuracy: 0.2962\nEpoch 73/100\n1140/1140 [==============================] - 1s 855us/step - loss: 2.5942 - accuracy: 0.3136 - val_loss: 2.6998 - val_accuracy: 0.2988\nEpoch 74/100\n1140/1140 [==============================] - 1s 778us/step - loss: 2.5935 - accuracy: 0.3144 - val_loss: 2.6965 - val_accuracy: 0.2981\nEpoch 75/100\n1140/1140 [==============================] - 1s 773us/step - loss: 2.5899 - accuracy: 0.3129 - val_loss: 2.7032 - val_accuracy: 0.2986\nEpoch 76/100\n1140/1140 [==============================] - 1s 936us/step - loss: 2.5900 - accuracy: 0.3159 - val_loss: 2.6982 - val_accuracy: 0.2958\nEpoch 77/100\n1140/1140 [==============================] - 1s 887us/step - loss: 2.5869 - accuracy: 0.3168 - val_loss: 2.6957 - val_accuracy: 0.2971\nEpoch 78/100\n1140/1140 [==============================] - 1s 783us/step - loss: 2.5844 - accuracy: 0.3145 - val_loss: 2.7007 - val_accuracy: 0.2944\nEpoch 79/100\n1140/1140 [==============================] - 1s 988us/step - loss: 2.5852 - accuracy: 0.3155 - val_loss: 2.7028 - val_accuracy: 0.2930\nEpoch 80/100\n1140/1140 [==============================] - 1s 925us/step - loss: 2.5832 - accuracy: 0.3152 - val_loss: 2.6940 - val_accuracy: 0.2961\nEpoch 81/100\n1140/1140 [==============================] - 1s 943us/step - loss: 2.5798 - accuracy: 0.3158 - val_loss: 2.7086 - val_accuracy: 0.2922\nEpoch 82/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5779 - accuracy: 0.3168 - val_loss: 2.6941 - val_accuracy: 0.2984\nEpoch 83/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5766 - accuracy: 0.3165 - val_loss: 2.7189 - val_accuracy: 0.2932\nEpoch 84/100\n1140/1140 [==============================] - 1s 840us/step - loss: 2.5753 - accuracy: 0.3181 - val_loss: 2.6888 - val_accuracy: 0.2956\nEpoch 85/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5738 - accuracy: 0.3173 - val_loss: 2.7064 - val_accuracy: 0.2959\nEpoch 86/100\n1140/1140 [==============================] - 3s 3ms/step - loss: 2.5729 - accuracy: 0.3181 - val_loss: 2.7032 - val_accuracy: 0.2980\nEpoch 87/100\n1140/1140 [==============================] - 1s 899us/step - loss: 2.5702 - accuracy: 0.3184 - val_loss: 2.6841 - val_accuracy: 0.2989\nEpoch 88/100\n1140/1140 [==============================] - 1s 964us/step - loss: 2.5694 - accuracy: 0.3182 - val_loss: 2.6825 - val_accuracy: 0.3004\nEpoch 89/100\n1140/1140 [==============================] - 1s 949us/step - loss: 2.5655 - accuracy: 0.3186 - val_loss: 2.6877 - val_accuracy: 0.2957\nEpoch 90/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5645 - accuracy: 0.3195 - val_loss: 2.7072 - val_accuracy: 0.2948\nEpoch 91/100\n1140/1140 [==============================] - 1s 879us/step - loss: 2.5622 - accuracy: 0.3207 - val_loss: 2.6915 - val_accuracy: 0.3007\nEpoch 92/100\n1140/1140 [==============================] - 1s 802us/step - loss: 2.5611 - accuracy: 0.3196 - val_loss: 2.6917 - val_accuracy: 0.3002\nEpoch 93/100\n1140/1140 [==============================] - 1s 920us/step - loss: 2.5587 - accuracy: 0.3185 - val_loss: 2.6898 - val_accuracy: 0.2953\nEpoch 94/100\n1140/1140 [==============================] - 2s 1ms/step - loss: 2.5565 - accuracy: 0.3204 - val_loss: 2.6795 - val_accuracy: 0.3009\nEpoch 95/100\n1140/1140 [==============================] - 1s 937us/step - loss: 2.5535 - accuracy: 0.3212 - val_loss: 2.7068 - val_accuracy: 0.2921\nEpoch 96/100\n1140/1140 [==============================] - 1s 975us/step - loss: 2.5542 - accuracy: 0.3207 - val_loss: 2.6798 - val_accuracy: 0.3013\nEpoch 97/100\n1140/1140 [==============================] - 1s 807us/step - loss: 2.5519 - accuracy: 0.3215 - val_loss: 2.6882 - val_accuracy: 0.2974\nEpoch 98/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5492 - accuracy: 0.3223 - val_loss: 2.6803 - val_accuracy: 0.2974\nEpoch 99/100\n1140/1140 [==============================] - 1s 1ms/step - loss: 2.5493 - accuracy: 0.3226 - val_loss: 2.6851 - val_accuracy: 0.2958\nEpoch 100/100\n1140/1140 [==============================] - 1s 817us/step - loss: 2.5471 - accuracy: 0.3239 - val_loss: 2.6968 - val_accuracy: 0.2960\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 28,
      "block_group": "c1ff8481130c46ce886b2d4741ba0476",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model2_5.evaluate(X_train2_2, y_train2_2))\nprint(\"Test Accuracy:\", model2_5.evaluate(X_test2_2, y_test2_2))",
      "metadata": {
        "cell_id": "2a07c7b5d98e40e1abbcada6bd213f4c",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2850/2850 [==============================] - 1s 445us/step - loss: 2.5699 - accuracy: 0.3186\nTraining Accuracy: [2.5699307918548584, 0.31864383816719055]\n713/713 [==============================] - 0s 426us/step - loss: 2.6577 - accuracy: 0.3029\nTest Accuracy: [2.657667636871338, 0.3028508722782135]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 29,
      "block_group": "ade3f65b84334fce9a023560f6aeb60e",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(classification_report(y_test2_2, np.argmax(model2_5.predict(X_test2_2, 1), axis =-1)))",
      "metadata": {
        "cell_id": "2fe734e9fa7442fd8070383ad7bf5bc3",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "22800/22800 [==============================] - 45s 2ms/step\n              precision    recall  f1-score   support\n\n           0       0.15      0.21      0.18       214\n           1       0.44      0.24      0.31       202\n           2       0.02      0.01      0.01       206\n           3       0.14      0.03      0.06       201\n           4       0.31      0.21      0.25       192\n           5       0.26      0.09      0.13       199\n           6       0.50      0.48      0.49       195\n           7       0.38      0.53      0.44       178\n           8       0.17      0.09      0.12       229\n           9       0.15      0.16      0.15       206\n          10       0.49      0.44      0.46       200\n          11       0.06      0.02      0.03       187\n          12       0.24      0.23      0.23       222\n          13       0.51      0.46      0.49       203\n          14       0.33      0.41      0.36       201\n          15       0.23      0.40      0.29       208\n          16       0.58      0.56      0.57       201\n          17       0.28      0.11      0.15       209\n          18       0.87      0.84      0.85       216\n          19       0.22      0.39      0.28       209\n          20       0.16      0.24      0.20       214\n          21       0.25      0.17      0.20       192\n          22       0.32      0.45      0.38       185\n          23       0.20      0.32      0.24       208\n          24       0.50      0.52      0.51       201\n          25       0.17      0.07      0.10       224\n          26       0.45      0.28      0.35       203\n          27       0.56      0.41      0.47       199\n          28       0.10      0.02      0.04       205\n          29       0.26      0.31      0.29       207\n          30       0.06      0.00      0.01       216\n          31       0.15      0.17      0.16       210\n          32       0.16      0.06      0.09       202\n          33       0.11      0.10      0.10       184\n          34       0.30      0.03      0.05       208\n          35       0.34      0.61      0.44       199\n          36       0.28      0.15      0.20       200\n          37       0.34      0.20      0.26       205\n          38       0.21      0.08      0.11       211\n          39       0.25      0.08      0.12       183\n          40       0.26      0.50      0.34       182\n          41       0.09      0.02      0.03       195\n          42       0.69      0.92      0.79       208\n          43       0.11      0.06      0.08       212\n          44       0.19      0.21      0.20       198\n          45       0.34      0.39      0.36       181\n          46       0.30      0.43      0.35       193\n          47       0.15      0.12      0.13       191\n          48       0.27      0.41      0.33       200\n          49       0.32      0.28      0.29       199\n          50       0.27      0.27      0.27       201\n          51       0.16      0.16      0.16       206\n          52       0.62      0.80      0.70       186\n          53       0.16      0.42      0.23       218\n          54       0.54      0.40      0.46       224\n          55       0.14      0.06      0.08       205\n          56       0.11      0.01      0.03       200\n          57       0.29      0.02      0.04       214\n          58       0.31      0.16      0.22       194\n          59       0.52      0.71      0.60       202\n          60       0.44      0.49      0.46       195\n          61       0.74      0.41      0.52       226\n          62       0.18      0.06      0.09       209\n          63       0.18      0.10      0.13       172\n          64       0.46      0.36      0.40       195\n          65       0.33      0.17      0.23       204\n          66       0.66      0.66      0.66       207\n          67       0.15      0.21      0.18       179\n          68       0.16      0.08      0.11       204\n          69       0.27      0.16      0.20       213\n          70       0.31      0.32      0.32       236\n          71       0.20      0.30      0.24       226\n          72       0.26      0.42      0.32       182\n          73       0.36      0.44      0.39       185\n          74       0.17      0.16      0.17       209\n          75       0.35      0.59      0.44       184\n          76       0.44      0.38      0.41       204\n          77       0.39      0.49      0.44       202\n          78       0.34      0.60      0.44       198\n          79       0.37      0.41      0.39       178\n          80       0.18      0.20      0.19       201\n          81       0.31      0.56      0.40       199\n          82       0.29      0.40      0.33       207\n          83       0.22      0.24      0.23       176\n          84       0.18      0.27      0.22       205\n          85       0.15      0.14      0.14       195\n          86       0.17      0.08      0.10       186\n          87       0.17      0.18      0.18       182\n          88       0.20      0.23      0.21       180\n          89       0.22      0.04      0.07       213\n          90       0.09      0.12      0.10       192\n          91       0.22      0.48      0.30       183\n          92       0.29      0.28      0.29       188\n          93       0.53      0.67      0.59       191\n          94       0.22      0.32      0.26       187\n          95       0.50      0.74      0.60       198\n          96       0.30      0.36      0.33       195\n          97       0.47      0.56      0.51       227\n          98       0.34      0.27      0.30       216\n          99       0.10      0.03      0.04       187\n         100       0.17      0.27      0.21       194\n         101       0.79      0.77      0.78       215\n         102       0.10      0.10      0.10       197\n         103       0.15      0.24      0.18       172\n         104       0.21      0.03      0.05       230\n         105       0.65      0.77      0.70       202\n         106       0.00      0.00      0.00       188\n         107       0.14      0.19      0.16       179\n         108       0.65      0.80      0.72       210\n         109       0.25      0.21      0.23       197\n         110       0.16      0.48      0.24       166\n         111       0.22      0.37      0.28       191\n         112       0.24      0.37      0.29       193\n         113       0.20      0.45      0.28       207\n\n    accuracy                           0.30     22800\n   macro avg       0.29      0.30      0.28     22800\nweighted avg       0.29      0.30      0.28     22800\n\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 32,
      "block_group": "9567f971938943ada53f777d907c634d",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=3da032f1-e5ab-4726-ac09-eb3d9c053730' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "7dc9ead19cb24496b72d1a5da0d62842",
    "deepnote_execution_queue": []
  }
}