{
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.impute import KNNImputer\nfrom sklearn.decomposition import PCA\n! pip install scikit-optimize\n# report scikit-optimize version number\nimport skopt\nprint('skopt %s' % skopt.__version__)\nfrom skopt.space import Integer\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.metrics import classification_report\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, Embedding, Attention\nfrom tensorflow.keras.optimizers.legacy import SGD\nfrom tensorflow.keras import backend as K\nimport argparse",
      "metadata": {
        "cell_id": "5dbe397d13734de58f15344575e06401",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Requirement already satisfied: scikit-optimize in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (0.10.1)\nRequirement already satisfied: joblib>=0.11 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (1.2.0)\nRequirement already satisfied: pyaml>=16.9 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (23.12.0)\nRequirement already satisfied: numpy>=1.20.3 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (1.26.4)\nRequirement already satisfied: scipy>=1.1.0 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (1.11.4)\nRequirement already satisfied: scikit-learn>=1.0.0 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (1.2.2)\nRequirement already satisfied: packaging>=21.3 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-optimize) (23.1)\nRequirement already satisfied: PyYAML in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /Users/mkobayashi/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.0->scikit-optimize) (2.2.0)\nskopt 0.10.1\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 1,
      "block_group": "3d9ae0d597a14408a6b7f7911c9db6da",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "#data from kaggle dataset: \"Prediction of music genre\"\ndata1 = pd.read_csv(\"~/Downloads/music_genre.csv\")\n#data from kaggle data set: \"Spotify Tracks Dataset\"\ndata2 = pd.read_csv(\"~/Downloads/dataset.csv\")",
      "metadata": {
        "cell_id": "d0eb88769b134d3aa2484aeae3f0da98",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": 2,
      "block_group": "b124fd1262a441cbb32bf90680a0c077",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<h2>Data Cleaning for Data 2</h2>",
      "metadata": {
        "cell_id": "325f9d30d3dc431e91e9907b9496031b",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "d6734c2d613c42e6be36c9f2a0d00c35"
    },
    {
      "cell_type": "code",
      "source": "#drop all the rows that are null\ndata2 = data2.dropna()",
      "metadata": {
        "cell_id": "8cff3f2cb6cc4c0590750e5597776557",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": 3,
      "block_group": "b7de6e499cc441128f07649f7c3d45f2",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "#make track_genre into label encoding\nlabelencoder = LabelEncoder()\ndata2['track_genre_num'] = labelencoder.fit_transform(data2['track_genre'])\ndata2",
      "metadata": {
        "cell_id": "607c6bf1ccf648d08fa5014e5302d2b3",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>track_id</th>\n      <th>artists</th>\n      <th>album_name</th>\n      <th>track_name</th>\n      <th>popularity</th>\n      <th>duration_ms</th>\n      <th>explicit</th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>...</th>\n      <th>mode</th>\n      <th>speechiness</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>tempo</th>\n      <th>time_signature</th>\n      <th>track_genre</th>\n      <th>track_genre_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>5SuOikwiRyPMVoIQDJUgSV</td>\n      <td>Gen Hoshino</td>\n      <td>Comedy</td>\n      <td>Comedy</td>\n      <td>73</td>\n      <td>230666</td>\n      <td>False</td>\n      <td>0.676</td>\n      <td>0.4610</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.1430</td>\n      <td>0.0322</td>\n      <td>0.000001</td>\n      <td>0.3580</td>\n      <td>0.7150</td>\n      <td>87.917</td>\n      <td>4</td>\n      <td>acoustic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>4qPNDBW1i3p13qLCt0Ki3A</td>\n      <td>Ben Woodward</td>\n      <td>Ghost (Acoustic)</td>\n      <td>Ghost - Acoustic</td>\n      <td>55</td>\n      <td>149610</td>\n      <td>False</td>\n      <td>0.420</td>\n      <td>0.1660</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0763</td>\n      <td>0.9240</td>\n      <td>0.000006</td>\n      <td>0.1010</td>\n      <td>0.2670</td>\n      <td>77.489</td>\n      <td>4</td>\n      <td>acoustic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1iJBSr7s7jYXzM8EGcbK5b</td>\n      <td>Ingrid Michaelson;ZAYN</td>\n      <td>To Begin Again</td>\n      <td>To Begin Again</td>\n      <td>57</td>\n      <td>210826</td>\n      <td>False</td>\n      <td>0.438</td>\n      <td>0.3590</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0557</td>\n      <td>0.2100</td>\n      <td>0.000000</td>\n      <td>0.1170</td>\n      <td>0.1200</td>\n      <td>76.332</td>\n      <td>4</td>\n      <td>acoustic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>6lfxq3CG4xtTiEg7opyCyx</td>\n      <td>Kina Grannis</td>\n      <td>Crazy Rich Asians (Original Motion Picture Sou...</td>\n      <td>Can't Help Falling In Love</td>\n      <td>71</td>\n      <td>201933</td>\n      <td>False</td>\n      <td>0.266</td>\n      <td>0.0596</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0363</td>\n      <td>0.9050</td>\n      <td>0.000071</td>\n      <td>0.1320</td>\n      <td>0.1430</td>\n      <td>181.740</td>\n      <td>3</td>\n      <td>acoustic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5vjLSffimiIP26QG5WcN2K</td>\n      <td>Chord Overstreet</td>\n      <td>Hold On</td>\n      <td>Hold On</td>\n      <td>82</td>\n      <td>198853</td>\n      <td>False</td>\n      <td>0.618</td>\n      <td>0.4430</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0526</td>\n      <td>0.4690</td>\n      <td>0.000000</td>\n      <td>0.0829</td>\n      <td>0.1670</td>\n      <td>119.949</td>\n      <td>4</td>\n      <td>acoustic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>113995</th>\n      <td>113995</td>\n      <td>2C3TZjDRiAzdyViavDJ217</td>\n      <td>Rainy Lullaby</td>\n      <td>#mindfulness - Soft Rain for Mindful Meditatio...</td>\n      <td>Sleep My Little Boy</td>\n      <td>21</td>\n      <td>384999</td>\n      <td>False</td>\n      <td>0.172</td>\n      <td>0.2350</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0422</td>\n      <td>0.6400</td>\n      <td>0.928000</td>\n      <td>0.0863</td>\n      <td>0.0339</td>\n      <td>125.995</td>\n      <td>5</td>\n      <td>world-music</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>113996</th>\n      <td>113996</td>\n      <td>1hIz5L4IB9hN3WRYPOCGPw</td>\n      <td>Rainy Lullaby</td>\n      <td>#mindfulness - Soft Rain for Mindful Meditatio...</td>\n      <td>Water Into Light</td>\n      <td>22</td>\n      <td>385000</td>\n      <td>False</td>\n      <td>0.174</td>\n      <td>0.1170</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0401</td>\n      <td>0.9940</td>\n      <td>0.976000</td>\n      <td>0.1050</td>\n      <td>0.0350</td>\n      <td>85.239</td>\n      <td>4</td>\n      <td>world-music</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>113997</th>\n      <td>113997</td>\n      <td>6x8ZfSoqDjuNa5SVP5QjvX</td>\n      <td>Ces√°ria Evora</td>\n      <td>Best Of</td>\n      <td>Miss Perfumado</td>\n      <td>22</td>\n      <td>271466</td>\n      <td>False</td>\n      <td>0.629</td>\n      <td>0.3290</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0420</td>\n      <td>0.8670</td>\n      <td>0.000000</td>\n      <td>0.0839</td>\n      <td>0.7430</td>\n      <td>132.378</td>\n      <td>4</td>\n      <td>world-music</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>113998</th>\n      <td>113998</td>\n      <td>2e6sXL2bYv4bSz6VTdnfLs</td>\n      <td>Michael W. Smith</td>\n      <td>Change Your World</td>\n      <td>Friends</td>\n      <td>41</td>\n      <td>283893</td>\n      <td>False</td>\n      <td>0.587</td>\n      <td>0.5060</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.0297</td>\n      <td>0.3810</td>\n      <td>0.000000</td>\n      <td>0.2700</td>\n      <td>0.4130</td>\n      <td>135.960</td>\n      <td>4</td>\n      <td>world-music</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>113999</th>\n      <td>113999</td>\n      <td>2hETkH7cOfqmz3LqZDHZf5</td>\n      <td>Ces√°ria Evora</td>\n      <td>Miss Perfumado</td>\n      <td>Barbincor</td>\n      <td>22</td>\n      <td>241826</td>\n      <td>False</td>\n      <td>0.526</td>\n      <td>0.4870</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0725</td>\n      <td>0.6810</td>\n      <td>0.000000</td>\n      <td>0.0893</td>\n      <td>0.7080</td>\n      <td>79.198</td>\n      <td>4</td>\n      <td>world-music</td>\n      <td>113</td>\n    </tr>\n  </tbody>\n</table>\n<p>113999 rows √ó 22 columns</p>\n</div>",
            "text/plain": "        Unnamed: 0                track_id                 artists  \\\n0                0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n1                1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n2                2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n3                3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n4                4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n...            ...                     ...                     ...   \n113995      113995  2C3TZjDRiAzdyViavDJ217           Rainy Lullaby   \n113996      113996  1hIz5L4IB9hN3WRYPOCGPw           Rainy Lullaby   \n113997      113997  6x8ZfSoqDjuNa5SVP5QjvX           Ces√°ria Evora   \n113998      113998  2e6sXL2bYv4bSz6VTdnfLs        Michael W. Smith   \n113999      113999  2hETkH7cOfqmz3LqZDHZf5           Ces√°ria Evora   \n\n                                               album_name  \\\n0                                                  Comedy   \n1                                        Ghost (Acoustic)   \n2                                          To Begin Again   \n3       Crazy Rich Asians (Original Motion Picture Sou...   \n4                                                 Hold On   \n...                                                   ...   \n113995  #mindfulness - Soft Rain for Mindful Meditatio...   \n113996  #mindfulness - Soft Rain for Mindful Meditatio...   \n113997                                            Best Of   \n113998                                  Change Your World   \n113999                                     Miss Perfumado   \n\n                        track_name  popularity  duration_ms  explicit  \\\n0                           Comedy          73       230666     False   \n1                 Ghost - Acoustic          55       149610     False   \n2                   To Begin Again          57       210826     False   \n3       Can't Help Falling In Love          71       201933     False   \n4                          Hold On          82       198853     False   \n...                            ...         ...          ...       ...   \n113995         Sleep My Little Boy          21       384999     False   \n113996            Water Into Light          22       385000     False   \n113997              Miss Perfumado          22       271466     False   \n113998                     Friends          41       283893     False   \n113999                   Barbincor          22       241826     False   \n\n        danceability  energy  ...  mode  speechiness  acousticness  \\\n0              0.676  0.4610  ...     0       0.1430        0.0322   \n1              0.420  0.1660  ...     1       0.0763        0.9240   \n2              0.438  0.3590  ...     1       0.0557        0.2100   \n3              0.266  0.0596  ...     1       0.0363        0.9050   \n4              0.618  0.4430  ...     1       0.0526        0.4690   \n...              ...     ...  ...   ...          ...           ...   \n113995         0.172  0.2350  ...     1       0.0422        0.6400   \n113996         0.174  0.1170  ...     0       0.0401        0.9940   \n113997         0.629  0.3290  ...     0       0.0420        0.8670   \n113998         0.587  0.5060  ...     1       0.0297        0.3810   \n113999         0.526  0.4870  ...     0       0.0725        0.6810   \n\n        instrumentalness  liveness  valence    tempo  time_signature  \\\n0               0.000001    0.3580   0.7150   87.917               4   \n1               0.000006    0.1010   0.2670   77.489               4   \n2               0.000000    0.1170   0.1200   76.332               4   \n3               0.000071    0.1320   0.1430  181.740               3   \n4               0.000000    0.0829   0.1670  119.949               4   \n...                  ...       ...      ...      ...             ...   \n113995          0.928000    0.0863   0.0339  125.995               5   \n113996          0.976000    0.1050   0.0350   85.239               4   \n113997          0.000000    0.0839   0.7430  132.378               4   \n113998          0.000000    0.2700   0.4130  135.960               4   \n113999          0.000000    0.0893   0.7080   79.198               4   \n\n        track_genre  track_genre_num  \n0          acoustic                0  \n1          acoustic                0  \n2          acoustic                0  \n3          acoustic                0  \n4          acoustic                0  \n...             ...              ...  \n113995  world-music              113  \n113996  world-music              113  \n113997  world-music              113  \n113998  world-music              113  \n113999  world-music              113  \n\n[113999 rows x 22 columns]"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "outputs_reference": null,
      "execution_count": 4,
      "block_group": "d2aee4efff7e44648ac23226cc0b87b8",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "features2 = [\"popularity\", \"duration_ms\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\nlen(features2)",
      "metadata": {
        "cell_id": "403d53604c6b458996391875ea92521b",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "14"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "outputs_reference": null,
      "execution_count": 5,
      "block_group": "c1d68c96f15741fdbb4e7e38517b370a",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "# train 80%, val 20%, test 20%\nX_train2b, X_test2b, y_train2b, y_test2b = train_test_split(data2[features2], data2[\"track_genre_num\"], test_size=0.2, random_state=1)\nX_train2b, X_val2b, y_train2b, y_val2b = train_test_split(X_train2b, y_train2b, test_size=0.25, random_state=1)",
      "metadata": {
        "cell_id": "9e2306b2075a4e01a1c793078ec29097",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": 6,
      "block_group": "c1a9577bfad7428486ecba1f62f63366",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "model2b = Sequential()\nmodel2b.add(LSTM(units=256,input_shape=(14, 1)))\nmodel2b.add(Dense(114, activation = \"softmax\"))\n\nmodel2b.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\nhistory2b = model2b.fit(X_train2b, y_train2b, validation_data=(X_val2b, y_val2b), batch_size=32, epochs=100)",
      "metadata": {
        "cell_id": "67a6b46b5bf847c398c7577b77f2d356",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n2138/2138 [==============================] - 91s 42ms/step - loss: 3.8788 - accuracy: 0.0842 - val_loss: 3.7018 - val_accuracy: 0.1010\nEpoch 2/100\n2138/2138 [==============================] - 86s 40ms/step - loss: 3.6406 - accuracy: 0.1096 - val_loss: 3.5762 - val_accuracy: 0.1198\nEpoch 3/100\n2138/2138 [==============================] - 79s 37ms/step - loss: 3.4575 - accuracy: 0.1416 - val_loss: 3.3444 - val_accuracy: 0.1675\nEpoch 4/100\n2138/2138 [==============================] - 79s 37ms/step - loss: 3.2387 - accuracy: 0.1822 - val_loss: 3.2227 - val_accuracy: 0.1879\nEpoch 5/100\n2138/2138 [==============================] - 82s 38ms/step - loss: 3.0622 - accuracy: 0.2135 - val_loss: 2.9754 - val_accuracy: 0.2339\nEpoch 6/100\n2138/2138 [==============================] - 76s 36ms/step - loss: 2.9003 - accuracy: 0.2457 - val_loss: 2.8945 - val_accuracy: 0.2494\nEpoch 7/100\n2138/2138 [==============================] - 87s 41ms/step - loss: 2.8158 - accuracy: 0.2609 - val_loss: 2.8709 - val_accuracy: 0.2509\nEpoch 8/100\n2138/2138 [==============================] - 91s 42ms/step - loss: 2.7541 - accuracy: 0.2719 - val_loss: 2.7941 - val_accuracy: 0.2686\nEpoch 9/100\n2138/2138 [==============================] - 95s 45ms/step - loss: 2.6992 - accuracy: 0.2817 - val_loss: 2.7922 - val_accuracy: 0.2661\nEpoch 10/100\n2138/2138 [==============================] - 87s 41ms/step - loss: 2.6497 - accuracy: 0.2908 - val_loss: 2.7301 - val_accuracy: 0.2777\nEpoch 11/100\n2138/2138 [==============================] - 86s 40ms/step - loss: 2.6083 - accuracy: 0.2976 - val_loss: 2.7324 - val_accuracy: 0.2825\nEpoch 12/100\n2138/2138 [==============================] - 84s 39ms/step - loss: 2.5700 - accuracy: 0.3025 - val_loss: 2.7381 - val_accuracy: 0.2791\nEpoch 13/100\n2138/2138 [==============================] - 92s 43ms/step - loss: 2.5242 - accuracy: 0.3125 - val_loss: 2.7089 - val_accuracy: 0.2864\nEpoch 14/100\n2138/2138 [==============================] - 87s 41ms/step - loss: 2.4886 - accuracy: 0.3189 - val_loss: 2.6920 - val_accuracy: 0.2874\nEpoch 15/100\n2138/2138 [==============================] - 82s 38ms/step - loss: 2.4476 - accuracy: 0.3259 - val_loss: 2.7049 - val_accuracy: 0.2843\nEpoch 16/100\n2138/2138 [==============================] - 81s 38ms/step - loss: 2.4105 - accuracy: 0.3342 - val_loss: 2.6931 - val_accuracy: 0.2916\nEpoch 17/100\n2138/2138 [==============================] - 92s 43ms/step - loss: 2.3677 - accuracy: 0.3416 - val_loss: 2.7135 - val_accuracy: 0.2849\nEpoch 18/100\n2138/2138 [==============================] - 93s 43ms/step - loss: 2.3283 - accuracy: 0.3477 - val_loss: 2.6866 - val_accuracy: 0.2931\nEpoch 19/100\n2138/2138 [==============================] - 93s 44ms/step - loss: 2.2909 - accuracy: 0.3540 - val_loss: 2.7081 - val_accuracy: 0.2875\nEpoch 20/100\n2138/2138 [==============================] - 74s 34ms/step - loss: 2.2509 - accuracy: 0.3635 - val_loss: 2.7031 - val_accuracy: 0.2885\nEpoch 21/100\n2138/2138 [==============================] - 65s 31ms/step - loss: 2.2126 - accuracy: 0.3697 - val_loss: 2.7057 - val_accuracy: 0.2904\nEpoch 22/100\n2138/2138 [==============================] - 71s 33ms/step - loss: 2.1747 - accuracy: 0.3780 - val_loss: 2.7363 - val_accuracy: 0.2825\nEpoch 23/100\n2138/2138 [==============================] - 67s 32ms/step - loss: 2.1335 - accuracy: 0.3866 - val_loss: 2.7344 - val_accuracy: 0.2880\nEpoch 24/100\n2138/2138 [==============================] - 75s 35ms/step - loss: 2.0973 - accuracy: 0.3945 - val_loss: 2.7525 - val_accuracy: 0.2841\nEpoch 25/100\n2138/2138 [==============================] - 60s 28ms/step - loss: 2.0578 - accuracy: 0.4011 - val_loss: 2.7793 - val_accuracy: 0.2824\nEpoch 26/100\n2138/2138 [==============================] - 66s 31ms/step - loss: 2.0160 - accuracy: 0.4118 - val_loss: 2.7836 - val_accuracy: 0.2844\nEpoch 27/100\n2138/2138 [==============================] - 59s 28ms/step - loss: 1.9822 - accuracy: 0.4204 - val_loss: 2.7879 - val_accuracy: 0.2824\nEpoch 28/100\n2138/2138 [==============================] - 53s 25ms/step - loss: 1.9446 - accuracy: 0.4261 - val_loss: 2.8001 - val_accuracy: 0.2823\nEpoch 29/100\n2138/2138 [==============================] - 53s 25ms/step - loss: 1.9042 - accuracy: 0.4360 - val_loss: 2.8405 - val_accuracy: 0.2796\nEpoch 30/100\n2138/2138 [==============================] - 55s 26ms/step - loss: 1.8653 - accuracy: 0.4434 - val_loss: 2.8527 - val_accuracy: 0.2758\nEpoch 31/100\n2138/2138 [==============================] - 271s 127ms/step - loss: 1.8305 - accuracy: 0.4530 - val_loss: 2.8741 - val_accuracy: 0.2774\nEpoch 32/100\n2138/2138 [==============================] - 68s 32ms/step - loss: 1.7980 - accuracy: 0.4568 - val_loss: 2.8955 - val_accuracy: 0.2746\nEpoch 33/100\n2138/2138 [==============================] - 62s 29ms/step - loss: 1.7615 - accuracy: 0.4675 - val_loss: 2.9056 - val_accuracy: 0.2763\nEpoch 34/100\n2138/2138 [==============================] - 59s 28ms/step - loss: 1.7247 - accuracy: 0.4776 - val_loss: 2.9416 - val_accuracy: 0.2783\nEpoch 35/100\n2138/2138 [==============================] - 62s 29ms/step - loss: 1.6912 - accuracy: 0.4818 - val_loss: 2.9612 - val_accuracy: 0.2725\nEpoch 36/100\n2138/2138 [==============================] - 59s 27ms/step - loss: 1.6528 - accuracy: 0.4947 - val_loss: 2.9968 - val_accuracy: 0.2700\nEpoch 37/100\n2138/2138 [==============================] - 64s 30ms/step - loss: 1.6230 - accuracy: 0.4998 - val_loss: 3.0307 - val_accuracy: 0.2742\nEpoch 38/100\n2138/2138 [==============================] - 63s 29ms/step - loss: 1.5874 - accuracy: 0.5095 - val_loss: 3.0499 - val_accuracy: 0.2665\nEpoch 39/100\n2138/2138 [==============================] - 79s 37ms/step - loss: 1.5548 - accuracy: 0.5170 - val_loss: 3.0669 - val_accuracy: 0.2686\nEpoch 40/100\n2138/2138 [==============================] - 71s 33ms/step - loss: 1.5264 - accuracy: 0.5251 - val_loss: 3.1113 - val_accuracy: 0.2687\nEpoch 41/100\n2138/2138 [==============================] - 79s 37ms/step - loss: 1.4951 - accuracy: 0.5314 - val_loss: 3.1436 - val_accuracy: 0.2669\nEpoch 42/100\n2138/2138 [==============================] - 72s 34ms/step - loss: 1.4637 - accuracy: 0.5392 - val_loss: 3.1722 - val_accuracy: 0.2668\nEpoch 43/100\n2138/2138 [==============================] - 70s 33ms/step - loss: 1.4397 - accuracy: 0.5462 - val_loss: 3.1903 - val_accuracy: 0.2647\nEpoch 44/100\n2138/2138 [==============================] - 70s 33ms/step - loss: 1.4087 - accuracy: 0.5513 - val_loss: 3.2315 - val_accuracy: 0.2599\nEpoch 45/100\n2138/2138 [==============================] - 72s 34ms/step - loss: 1.3811 - accuracy: 0.5589 - val_loss: 3.2674 - val_accuracy: 0.2623\nEpoch 46/100\n2138/2138 [==============================] - 83s 39ms/step - loss: 1.3552 - accuracy: 0.5670 - val_loss: 3.3192 - val_accuracy: 0.2561\nEpoch 47/100\n2138/2138 [==============================] - 81s 38ms/step - loss: 1.3206 - accuracy: 0.5770 - val_loss: 3.3385 - val_accuracy: 0.2577\nEpoch 48/100\n2138/2138 [==============================] - 74s 35ms/step - loss: 1.2988 - accuracy: 0.5818 - val_loss: 3.3304 - val_accuracy: 0.2586\nEpoch 49/100\n2138/2138 [==============================] - 79s 37ms/step - loss: 1.2777 - accuracy: 0.5863 - val_loss: 3.3909 - val_accuracy: 0.2563\nEpoch 50/100\n2138/2138 [==============================] - 100s 47ms/step - loss: 1.2449 - accuracy: 0.5930 - val_loss: 3.4307 - val_accuracy: 0.2565\nEpoch 51/100\n2138/2138 [==============================] - 72s 34ms/step - loss: 1.2274 - accuracy: 0.6009 - val_loss: 3.4696 - val_accuracy: 0.2554\nEpoch 52/100\n2138/2138 [==============================] - 72s 34ms/step - loss: 1.2013 - accuracy: 0.6067 - val_loss: 3.4778 - val_accuracy: 0.2532\nEpoch 53/100\n2138/2138 [==============================] - 85s 40ms/step - loss: 1.1816 - accuracy: 0.6098 - val_loss: 3.5324 - val_accuracy: 0.2526\nEpoch 54/100\n2138/2138 [==============================] - 71s 33ms/step - loss: 1.1591 - accuracy: 0.6169 - val_loss: 3.5750 - val_accuracy: 0.2518\nEpoch 55/100\n2138/2138 [==============================] - 67s 32ms/step - loss: 1.1436 - accuracy: 0.6201 - val_loss: 3.6180 - val_accuracy: 0.2511\nEpoch 56/100\n2138/2138 [==============================] - 71s 33ms/step - loss: 1.1206 - accuracy: 0.6272 - val_loss: 3.6122 - val_accuracy: 0.2496\nEpoch 57/100\n2138/2138 [==============================] - 69s 32ms/step - loss: 1.0995 - accuracy: 0.6343 - val_loss: 3.6557 - val_accuracy: 0.2509\nEpoch 58/100\n2138/2138 [==============================] - 70s 33ms/step - loss: 1.0825 - accuracy: 0.6368 - val_loss: 3.7073 - val_accuracy: 0.2481\nEpoch 59/100\n2138/2138 [==============================] - 69s 32ms/step - loss: 1.0664 - accuracy: 0.6421 - val_loss: 3.7548 - val_accuracy: 0.2454\nEpoch 60/100\n2138/2138 [==============================] - 70s 33ms/step - loss: 1.0518 - accuracy: 0.6449 - val_loss: 3.8027 - val_accuracy: 0.2449\nEpoch 61/100\n2138/2138 [==============================] - 73s 34ms/step - loss: 1.0307 - accuracy: 0.6515 - val_loss: 3.8106 - val_accuracy: 0.2429\nEpoch 62/100\n2138/2138 [==============================] - 74s 35ms/step - loss: 1.0156 - accuracy: 0.6584 - val_loss: 3.8166 - val_accuracy: 0.2441\nEpoch 63/100\n2138/2138 [==============================] - 68s 32ms/step - loss: 0.9983 - accuracy: 0.6610 - val_loss: 3.8596 - val_accuracy: 0.2468\nEpoch 64/100\n2138/2138 [==============================] - 66s 31ms/step - loss: 0.9872 - accuracy: 0.6622 - val_loss: 3.9281 - val_accuracy: 0.2430\nEpoch 65/100\n2138/2138 [==============================] - 67s 31ms/step - loss: 0.9701 - accuracy: 0.6676 - val_loss: 3.9269 - val_accuracy: 0.2421\nEpoch 66/100\n2138/2138 [==============================] - 58s 27ms/step - loss: 0.9592 - accuracy: 0.6709 - val_loss: 3.9579 - val_accuracy: 0.2424\nEpoch 67/100\n2138/2138 [==============================] - 64s 30ms/step - loss: 0.9402 - accuracy: 0.6744 - val_loss: 3.9762 - val_accuracy: 0.2432\nEpoch 68/100\n2138/2138 [==============================] - 75s 35ms/step - loss: 0.9359 - accuracy: 0.6770 - val_loss: 3.9936 - val_accuracy: 0.2432\nEpoch 69/100\n2138/2138 [==============================] - 74s 34ms/step - loss: 0.9157 - accuracy: 0.6815 - val_loss: 4.0093 - val_accuracy: 0.2422\nEpoch 70/100\n2138/2138 [==============================] - 90s 42ms/step - loss: 0.9026 - accuracy: 0.6865 - val_loss: 4.0874 - val_accuracy: 0.2414\nEpoch 71/100\n2138/2138 [==============================] - 72s 34ms/step - loss: 0.8976 - accuracy: 0.6879 - val_loss: 4.1103 - val_accuracy: 0.2425\nEpoch 72/100\n2138/2138 [==============================] - 79s 37ms/step - loss: 0.8787 - accuracy: 0.6923 - val_loss: 4.1259 - val_accuracy: 0.2422\nEpoch 73/100\n2138/2138 [==============================] - 64s 30ms/step - loss: 0.8753 - accuracy: 0.6943 - val_loss: 4.1646 - val_accuracy: 0.2435\nEpoch 74/100\n2138/2138 [==============================] - 82s 38ms/step - loss: 0.8651 - accuracy: 0.6969 - val_loss: 4.1695 - val_accuracy: 0.2368\nEpoch 75/100\n2138/2138 [==============================] - 76s 36ms/step - loss: 0.8574 - accuracy: 0.6974 - val_loss: 4.2357 - val_accuracy: 0.2406\nEpoch 76/100\n2138/2138 [==============================] - 77s 36ms/step - loss: 0.8450 - accuracy: 0.7003 - val_loss: 4.2403 - val_accuracy: 0.2399\nEpoch 77/100\n2138/2138 [==============================] - 77s 36ms/step - loss: 0.8441 - accuracy: 0.7004 - val_loss: 4.2779 - val_accuracy: 0.2376\nEpoch 78/100\n2138/2138 [==============================] - 80s 37ms/step - loss: 0.8290 - accuracy: 0.7045 - val_loss: 4.3055 - val_accuracy: 0.2411\nEpoch 79/100\n2138/2138 [==============================] - 74s 34ms/step - loss: 0.8264 - accuracy: 0.7051 - val_loss: 4.3323 - val_accuracy: 0.2355\nEpoch 80/100\n2138/2138 [==============================] - 83s 39ms/step - loss: 0.8161 - accuracy: 0.7104 - val_loss: 4.3507 - val_accuracy: 0.2393\nEpoch 81/100\n2138/2138 [==============================] - 73s 34ms/step - loss: 0.8047 - accuracy: 0.7119 - val_loss: 4.3917 - val_accuracy: 0.2372\nEpoch 82/100\n2138/2138 [==============================] - 70s 33ms/step - loss: 0.8048 - accuracy: 0.7125 - val_loss: 4.4159 - val_accuracy: 0.2339\nEpoch 83/100\n2138/2138 [==============================] - 69s 32ms/step - loss: 0.7937 - accuracy: 0.7172 - val_loss: 4.4293 - val_accuracy: 0.2342\nEpoch 84/100\n2138/2138 [==============================] - 81s 38ms/step - loss: 0.7834 - accuracy: 0.7184 - val_loss: 4.4439 - val_accuracy: 0.2385\nEpoch 85/100\n2138/2138 [==============================] - 65s 30ms/step - loss: 0.7841 - accuracy: 0.7173 - val_loss: 4.4720 - val_accuracy: 0.2364\nEpoch 86/100\n2138/2138 [==============================] - 71s 33ms/step - loss: 0.7700 - accuracy: 0.7208 - val_loss: 4.4977 - val_accuracy: 0.2347\nEpoch 87/100\n2138/2138 [==============================] - 77s 36ms/step - loss: 0.7698 - accuracy: 0.7216 - val_loss: 4.5247 - val_accuracy: 0.2356\nEpoch 88/100\n2138/2138 [==============================] - 94s 44ms/step - loss: 0.7587 - accuracy: 0.7263 - val_loss: 4.5564 - val_accuracy: 0.2328\nEpoch 89/100\n2138/2138 [==============================] - 81s 38ms/step - loss: 0.7493 - accuracy: 0.7286 - val_loss: 4.5731 - val_accuracy: 0.2361\nEpoch 90/100\n2138/2138 [==============================] - 93s 43ms/step - loss: 0.7616 - accuracy: 0.7234 - val_loss: 4.5858 - val_accuracy: 0.2360\nEpoch 91/100\n2138/2138 [==============================] - 100s 47ms/step - loss: 0.7398 - accuracy: 0.7311 - val_loss: 4.5991 - val_accuracy: 0.2331\nEpoch 92/100\n2138/2138 [==============================] - 111s 52ms/step - loss: 0.7462 - accuracy: 0.7285 - val_loss: 4.6300 - val_accuracy: 0.2368\nEpoch 93/100\n2138/2138 [==============================] - 98s 46ms/step - loss: 0.7432 - accuracy: 0.7289 - val_loss: 4.6551 - val_accuracy: 0.2333\nEpoch 94/100\n2138/2138 [==============================] - 71s 33ms/step - loss: 0.7305 - accuracy: 0.7323 - val_loss: 4.6559 - val_accuracy: 0.2322\nEpoch 95/100\n2138/2138 [==============================] - 68s 32ms/step - loss: 0.7295 - accuracy: 0.7325 - val_loss: 4.6857 - val_accuracy: 0.2328\nEpoch 96/100\n2138/2138 [==============================] - 61s 29ms/step - loss: 0.7270 - accuracy: 0.7327 - val_loss: 4.7286 - val_accuracy: 0.2345\nEpoch 97/100\n2138/2138 [==============================] - 75s 35ms/step - loss: 0.7198 - accuracy: 0.7364 - val_loss: 4.7256 - val_accuracy: 0.2368\nEpoch 98/100\n2138/2138 [==============================] - 67s 31ms/step - loss: 0.7151 - accuracy: 0.7364 - val_loss: 4.7558 - val_accuracy: 0.2316\nEpoch 99/100\n2138/2138 [==============================] - 75s 35ms/step - loss: 0.7139 - accuracy: 0.7361 - val_loss: 4.7433 - val_accuracy: 0.2326\nEpoch 100/100\n2138/2138 [==============================] - 99s 46ms/step - loss: 0.7130 - accuracy: 0.7381 - val_loss: 4.7962 - val_accuracy: 0.2346\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 7,
      "block_group": "465bf30528c34128bef744aec9f7bcad",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model2b.evaluate(X_train2b, y_train2b))\ny_test_pred2b = model2b.evaluate(X_test2b, y_test2b)\nprint(\"Test Accuracy:\", y_test_pred2b)",
      "metadata": {
        "cell_id": "278624b4ee2c44afb38077722b808b94",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2138/2138 [==============================] - 34s 16ms/step - loss: 0.5917 - accuracy: 0.7860\nTraining Accuracy: [0.5916812419891357, 0.7859764099121094]\n713/713 [==============================] - 18s 25ms/step - loss: 4.7546 - accuracy: 0.2413\nTest Accuracy: [4.75462532043457, 0.24127192795276642]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 8,
      "block_group": "a6f1578204a44651bbc3a8db0f3ff197",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(classification_report(y_test2b, np.argmax(model2b.predict(X_test2b, 1), axis =-1)))",
      "metadata": {
        "cell_id": "1bc8f334f9674f0fbd1766670da42621",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "22800/22800 [==============================] - 110s 5ms/step\n              precision    recall  f1-score   support\n\n           0       0.11      0.10      0.10       214\n           1       0.24      0.22      0.23       202\n           2       0.07      0.08      0.07       206\n           3       0.12      0.14      0.13       201\n           4       0.23      0.26      0.24       192\n           5       0.11      0.09      0.10       199\n           6       0.45      0.48      0.47       195\n           7       0.31      0.37      0.33       178\n           8       0.15      0.14      0.14       229\n           9       0.11      0.09      0.10       206\n          10       0.32      0.40      0.36       200\n          11       0.07      0.07      0.07       187\n          12       0.20      0.15      0.17       222\n          13       0.41      0.36      0.39       203\n          14       0.29      0.29      0.29       201\n          15       0.10      0.09      0.09       208\n          16       0.44      0.55      0.49       201\n          17       0.19      0.14      0.17       209\n          18       0.86      0.82      0.84       216\n          19       0.41      0.36      0.38       209\n          20       0.33      0.29      0.31       214\n          21       0.20      0.20      0.20       192\n          22       0.21      0.28      0.24       185\n          23       0.12      0.15      0.14       208\n          24       0.47      0.44      0.45       201\n          25       0.17      0.13      0.15       224\n          26       0.24      0.22      0.23       203\n          27       0.42      0.41      0.41       199\n          28       0.07      0.07      0.07       205\n          29       0.08      0.07      0.08       207\n          30       0.06      0.06      0.06       216\n          31       0.20      0.21      0.21       210\n          32       0.06      0.06      0.06       202\n          33       0.05      0.04      0.05       184\n          34       0.09      0.06      0.07       208\n          35       0.44      0.44      0.44       199\n          36       0.24      0.20      0.22       200\n          37       0.27      0.26      0.26       205\n          38       0.14      0.14      0.14       211\n          39       0.20      0.19      0.19       183\n          40       0.19      0.26      0.22       182\n          41       0.11      0.10      0.10       195\n          42       0.77      0.74      0.75       208\n          43       0.09      0.09      0.09       212\n          44       0.12      0.13      0.13       198\n          45       0.23      0.27      0.25       181\n          46       0.29      0.32      0.30       193\n          47       0.06      0.07      0.07       191\n          48       0.21      0.23      0.22       200\n          49       0.37      0.39      0.38       199\n          50       0.23      0.21      0.22       201\n          51       0.22      0.19      0.20       206\n          52       0.62      0.60      0.61       186\n          53       0.20      0.18      0.19       218\n          54       0.50      0.41      0.45       224\n          55       0.05      0.05      0.05       205\n          56       0.08      0.07      0.08       200\n          57       0.10      0.08      0.09       214\n          58       0.14      0.15      0.15       194\n          59       0.64      0.54      0.59       202\n          60       0.39      0.38      0.39       195\n          61       0.43      0.40      0.42       226\n          62       0.06      0.05      0.06       209\n          63       0.06      0.09      0.07       172\n          64       0.41      0.44      0.42       195\n          65       0.19      0.17      0.18       204\n          66       0.49      0.57      0.53       207\n          67       0.11      0.14      0.13       179\n          68       0.13      0.16      0.14       204\n          69       0.11      0.08      0.10       213\n          70       0.19      0.19      0.19       236\n          71       0.07      0.06      0.07       226\n          72       0.18      0.24      0.21       182\n          73       0.30      0.33      0.32       185\n          74       0.06      0.07      0.06       209\n          75       0.39      0.30      0.34       184\n          76       0.38      0.37      0.37       204\n          77       0.33      0.37      0.35       202\n          78       0.46      0.42      0.44       198\n          79       0.26      0.35      0.30       178\n          80       0.22      0.18      0.20       201\n          81       0.19      0.22      0.20       199\n          82       0.28      0.26      0.27       207\n          83       0.14      0.17      0.15       176\n          84       0.17      0.20      0.18       205\n          85       0.07      0.07      0.07       195\n          86       0.05      0.05      0.05       186\n          87       0.10      0.15      0.12       182\n          88       0.09      0.13      0.11       180\n          89       0.08      0.04      0.05       213\n          90       0.17      0.22      0.19       192\n          91       0.16      0.20      0.18       183\n          92       0.18      0.17      0.17       188\n          93       0.58      0.61      0.59       191\n          94       0.15      0.16      0.16       187\n          95       0.54      0.46      0.50       198\n          96       0.25      0.24      0.24       195\n          97       0.43      0.42      0.43       227\n          98       0.23      0.21      0.22       216\n          99       0.12      0.13      0.12       187\n         100       0.15      0.15      0.15       194\n         101       0.85      0.73      0.79       215\n         102       0.08      0.08      0.08       197\n         103       0.20      0.23      0.22       172\n         104       0.16      0.13      0.14       230\n         105       0.61      0.56      0.58       202\n         106       0.17      0.19      0.18       188\n         107       0.13      0.10      0.11       179\n         108       0.56      0.65      0.60       210\n         109       0.15      0.18      0.16       197\n         110       0.26      0.24      0.25       166\n         111       0.13      0.15      0.14       191\n         112       0.23      0.20      0.21       193\n         113       0.21      0.20      0.21       207\n\n    accuracy                           0.24     22800\n   macro avg       0.24      0.24      0.24     22800\nweighted avg       0.24      0.24      0.24     22800\n\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 9,
      "block_group": "79d79263bd014fd8927686b83ed10a10",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "model2b2 = Sequential()\nmodel2b2.add(LSTM(units=128,input_shape=(14, 1)))\nmodel2b2.add(Dense(114, activation = \"softmax\"))\n\nmodel2b2.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\nhistory2b2 = model2b2.fit(X_train2b, y_train2b, validation_data=(X_val2b, y_val2b), batch_size=32, epochs=100)",
      "metadata": {
        "cell_id": "25b368fdfc8048ab94495b3c27fdf280",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n2138/2138 [==============================] - 47s 20ms/step - loss: 3.9605 - accuracy: 0.0791 - val_loss: 3.7271 - val_accuracy: 0.0964\nEpoch 2/100\n2138/2138 [==============================] - 41s 19ms/step - loss: 3.6569 - accuracy: 0.1092 - val_loss: 3.6071 - val_accuracy: 0.1159\nEpoch 3/100\n2138/2138 [==============================] - 47s 22ms/step - loss: 3.5985 - accuracy: 0.1178 - val_loss: 3.5884 - val_accuracy: 0.1135\nEpoch 4/100\n2138/2138 [==============================] - 44s 21ms/step - loss: 3.4640 - accuracy: 0.1416 - val_loss: 3.3759 - val_accuracy: 0.1544\nEpoch 5/100\n2138/2138 [==============================] - 45s 21ms/step - loss: 3.2872 - accuracy: 0.1728 - val_loss: 3.2440 - val_accuracy: 0.1849\nEpoch 6/100\n2138/2138 [==============================] - 43s 20ms/step - loss: 3.1456 - accuracy: 0.2021 - val_loss: 3.1089 - val_accuracy: 0.2056\nEpoch 7/100\n2138/2138 [==============================] - 44s 21ms/step - loss: 3.0322 - accuracy: 0.2239 - val_loss: 2.9845 - val_accuracy: 0.2292\nEpoch 8/100\n2138/2138 [==============================] - 60s 28ms/step - loss: 2.9252 - accuracy: 0.2426 - val_loss: 2.9303 - val_accuracy: 0.2414\nEpoch 9/100\n2138/2138 [==============================] - 43s 20ms/step - loss: 2.8551 - accuracy: 0.2556 - val_loss: 2.8505 - val_accuracy: 0.2556\nEpoch 10/100\n2138/2138 [==============================] - 54s 25ms/step - loss: 2.8053 - accuracy: 0.2657 - val_loss: 2.8301 - val_accuracy: 0.2622\nEpoch 11/100\n2138/2138 [==============================] - 54s 25ms/step - loss: 2.7714 - accuracy: 0.2707 - val_loss: 2.8177 - val_accuracy: 0.2618\nEpoch 12/100\n2138/2138 [==============================] - 47s 22ms/step - loss: 2.7390 - accuracy: 0.2774 - val_loss: 2.8109 - val_accuracy: 0.2631\nEpoch 13/100\n2138/2138 [==============================] - 38s 18ms/step - loss: 2.7127 - accuracy: 0.2812 - val_loss: 2.8015 - val_accuracy: 0.2653\nEpoch 14/100\n2138/2138 [==============================] - 43s 20ms/step - loss: 2.6883 - accuracy: 0.2854 - val_loss: 2.8200 - val_accuracy: 0.2641\nEpoch 15/100\n2138/2138 [==============================] - 44s 21ms/step - loss: 2.6658 - accuracy: 0.2900 - val_loss: 2.7620 - val_accuracy: 0.2764\nEpoch 16/100\n2138/2138 [==============================] - 45s 21ms/step - loss: 2.6484 - accuracy: 0.2938 - val_loss: 2.7697 - val_accuracy: 0.2749\nEpoch 17/100\n2138/2138 [==============================] - 41s 19ms/step - loss: 2.6276 - accuracy: 0.2975 - val_loss: 2.7566 - val_accuracy: 0.2725\nEpoch 18/100\n2138/2138 [==============================] - 45s 21ms/step - loss: 2.6070 - accuracy: 0.3028 - val_loss: 2.7212 - val_accuracy: 0.2822\nEpoch 19/100\n2138/2138 [==============================] - 38s 18ms/step - loss: 2.5914 - accuracy: 0.3043 - val_loss: 2.7299 - val_accuracy: 0.2801\nEpoch 20/100\n2138/2138 [==============================] - 36s 17ms/step - loss: 2.5733 - accuracy: 0.3073 - val_loss: 2.7105 - val_accuracy: 0.2842\nEpoch 21/100\n2138/2138 [==============================] - 38s 18ms/step - loss: 2.5602 - accuracy: 0.3103 - val_loss: 2.7638 - val_accuracy: 0.2733\nEpoch 22/100\n2138/2138 [==============================] - 37s 17ms/step - loss: 2.5380 - accuracy: 0.3147 - val_loss: 2.7444 - val_accuracy: 0.2767\nEpoch 23/100\n2138/2138 [==============================] - 28s 13ms/step - loss: 2.5239 - accuracy: 0.3172 - val_loss: 2.7071 - val_accuracy: 0.2866\nEpoch 24/100\n2138/2138 [==============================] - 35s 16ms/step - loss: 2.5096 - accuracy: 0.3196 - val_loss: 2.7300 - val_accuracy: 0.2786\nEpoch 25/100\n2138/2138 [==============================] - 37s 17ms/step - loss: 2.4920 - accuracy: 0.3237 - val_loss: 2.7459 - val_accuracy: 0.2778\nEpoch 26/100\n2138/2138 [==============================] - 31s 15ms/step - loss: 2.4779 - accuracy: 0.3244 - val_loss: 2.7135 - val_accuracy: 0.2877\nEpoch 27/100\n2138/2138 [==============================] - 36s 17ms/step - loss: 2.4625 - accuracy: 0.3297 - val_loss: 2.7108 - val_accuracy: 0.2861\nEpoch 28/100\n2138/2138 [==============================] - 26s 12ms/step - loss: 2.4454 - accuracy: 0.3317 - val_loss: 2.7241 - val_accuracy: 0.2827\nEpoch 29/100\n2138/2138 [==============================] - 27s 13ms/step - loss: 2.4336 - accuracy: 0.3335 - val_loss: 2.7178 - val_accuracy: 0.2842\nEpoch 30/100\n2138/2138 [==============================] - 25s 12ms/step - loss: 2.4181 - accuracy: 0.3356 - val_loss: 2.7500 - val_accuracy: 0.2775\nEpoch 31/100\n2138/2138 [==============================] - 27s 13ms/step - loss: 2.4063 - accuracy: 0.3373 - val_loss: 2.7275 - val_accuracy: 0.2829\nEpoch 32/100\n2138/2138 [==============================] - 30s 14ms/step - loss: 2.3915 - accuracy: 0.3428 - val_loss: 2.7202 - val_accuracy: 0.2873\nEpoch 33/100\n2138/2138 [==============================] - 44s 20ms/step - loss: 2.3752 - accuracy: 0.3449 - val_loss: 2.7236 - val_accuracy: 0.2837\nEpoch 34/100\n2138/2138 [==============================] - 32s 15ms/step - loss: 2.3638 - accuracy: 0.3490 - val_loss: 2.7171 - val_accuracy: 0.2850\nEpoch 35/100\n2138/2138 [==============================] - 36s 17ms/step - loss: 2.3502 - accuracy: 0.3487 - val_loss: 2.7391 - val_accuracy: 0.2827\nEpoch 36/100\n2138/2138 [==============================] - 30s 14ms/step - loss: 2.3363 - accuracy: 0.3523 - val_loss: 2.7331 - val_accuracy: 0.2836\nEpoch 37/100\n2138/2138 [==============================] - 33s 16ms/step - loss: 2.3259 - accuracy: 0.3541 - val_loss: 2.7274 - val_accuracy: 0.2864\nEpoch 38/100\n2138/2138 [==============================] - 37s 17ms/step - loss: 2.3124 - accuracy: 0.3561 - val_loss: 2.7328 - val_accuracy: 0.2854\nEpoch 39/100\n2138/2138 [==============================] - 28s 13ms/step - loss: 2.2986 - accuracy: 0.3590 - val_loss: 2.7457 - val_accuracy: 0.2824\nEpoch 40/100\n2138/2138 [==============================] - 31s 14ms/step - loss: 2.2863 - accuracy: 0.3614 - val_loss: 2.7411 - val_accuracy: 0.2813\nEpoch 41/100\n2138/2138 [==============================] - 34s 16ms/step - loss: 2.2754 - accuracy: 0.3648 - val_loss: 2.7541 - val_accuracy: 0.2810\nEpoch 42/100\n2138/2138 [==============================] - 31s 14ms/step - loss: 2.2617 - accuracy: 0.3663 - val_loss: 2.7764 - val_accuracy: 0.2784\nEpoch 43/100\n2138/2138 [==============================] - 29s 13ms/step - loss: 2.2503 - accuracy: 0.3698 - val_loss: 2.7794 - val_accuracy: 0.2775\nEpoch 44/100\n2138/2138 [==============================] - 31s 14ms/step - loss: 2.2403 - accuracy: 0.3700 - val_loss: 2.7592 - val_accuracy: 0.2789\nEpoch 45/100\n2138/2138 [==============================] - 32s 15ms/step - loss: 2.2259 - accuracy: 0.3737 - val_loss: 2.7720 - val_accuracy: 0.2814\nEpoch 46/100\n2138/2138 [==============================] - 33s 15ms/step - loss: 2.2132 - accuracy: 0.3773 - val_loss: 2.7911 - val_accuracy: 0.2790\nEpoch 47/100\n2138/2138 [==============================] - 28s 13ms/step - loss: 2.2047 - accuracy: 0.3800 - val_loss: 2.7835 - val_accuracy: 0.2807\nEpoch 48/100\n2138/2138 [==============================] - 30s 14ms/step - loss: 2.1903 - accuracy: 0.3810 - val_loss: 2.7974 - val_accuracy: 0.2790\nEpoch 49/100\n2138/2138 [==============================] - 34s 16ms/step - loss: 2.1807 - accuracy: 0.3836 - val_loss: 2.7928 - val_accuracy: 0.2778\nEpoch 50/100\n2138/2138 [==============================] - 29s 14ms/step - loss: 2.1699 - accuracy: 0.3841 - val_loss: 2.7919 - val_accuracy: 0.2774\nEpoch 51/100\n2138/2138 [==============================] - 28s 13ms/step - loss: 2.1593 - accuracy: 0.3876 - val_loss: 2.8079 - val_accuracy: 0.2762\nEpoch 52/100\n2138/2138 [==============================] - 34s 16ms/step - loss: 2.1473 - accuracy: 0.3896 - val_loss: 2.8091 - val_accuracy: 0.2794\nEpoch 53/100\n2138/2138 [==============================] - 28s 13ms/step - loss: 2.1375 - accuracy: 0.3920 - val_loss: 2.8184 - val_accuracy: 0.2798\nEpoch 54/100\n2138/2138 [==============================] - 34s 16ms/step - loss: 2.1266 - accuracy: 0.3947 - val_loss: 2.8164 - val_accuracy: 0.2782\nEpoch 55/100\n2138/2138 [==============================] - 31s 15ms/step - loss: 2.1144 - accuracy: 0.3983 - val_loss: 2.8341 - val_accuracy: 0.2791\nEpoch 56/100\n2138/2138 [==============================] - 28s 13ms/step - loss: 2.1068 - accuracy: 0.3996 - val_loss: 2.8194 - val_accuracy: 0.2769\nEpoch 57/100\n2138/2138 [==============================] - 26s 12ms/step - loss: 2.0968 - accuracy: 0.3993 - val_loss: 2.8255 - val_accuracy: 0.2764\nEpoch 58/100\n2138/2138 [==============================] - 28s 13ms/step - loss: 2.0829 - accuracy: 0.4029 - val_loss: 2.8363 - val_accuracy: 0.2766\nEpoch 59/100\n2138/2138 [==============================] - 32s 15ms/step - loss: 2.0778 - accuracy: 0.4047 - val_loss: 2.8586 - val_accuracy: 0.2758\nEpoch 60/100\n2138/2138 [==============================] - 37s 17ms/step - loss: 2.0656 - accuracy: 0.4066 - val_loss: 2.8530 - val_accuracy: 0.2767\nEpoch 61/100\n2138/2138 [==============================] - 31s 15ms/step - loss: 2.0837 - accuracy: 0.4032 - val_loss: 2.8571 - val_accuracy: 0.2781\nEpoch 62/100\n2138/2138 [==============================] - 29s 13ms/step - loss: 2.0525 - accuracy: 0.4122 - val_loss: 2.8804 - val_accuracy: 0.2743\nEpoch 63/100\n2138/2138 [==============================] - 28s 13ms/step - loss: 2.0458 - accuracy: 0.4106 - val_loss: 2.8796 - val_accuracy: 0.2736\nEpoch 64/100\n2138/2138 [==============================] - 39s 18ms/step - loss: 2.0334 - accuracy: 0.4126 - val_loss: 2.8782 - val_accuracy: 0.2772\nEpoch 65/100\n2138/2138 [==============================] - 34s 16ms/step - loss: 2.0208 - accuracy: 0.4165 - val_loss: 2.8936 - val_accuracy: 0.2699\nEpoch 66/100\n2138/2138 [==============================] - 38s 18ms/step - loss: 2.0119 - accuracy: 0.4186 - val_loss: 2.8976 - val_accuracy: 0.2733\nEpoch 67/100\n2138/2138 [==============================] - 34s 16ms/step - loss: 2.0041 - accuracy: 0.4206 - val_loss: 2.8849 - val_accuracy: 0.2722\nEpoch 68/100\n2138/2138 [==============================] - 37s 17ms/step - loss: 1.9992 - accuracy: 0.4202 - val_loss: 2.9325 - val_accuracy: 0.2681\nEpoch 69/100\n2138/2138 [==============================] - 24s 11ms/step - loss: 1.9839 - accuracy: 0.4240 - val_loss: 2.9455 - val_accuracy: 0.2686\nEpoch 70/100\n2138/2138 [==============================] - 25s 12ms/step - loss: 1.9791 - accuracy: 0.4256 - val_loss: 2.9476 - val_accuracy: 0.2677\nEpoch 71/100\n2138/2138 [==============================] - 25s 12ms/step - loss: 1.9720 - accuracy: 0.4285 - val_loss: 2.9382 - val_accuracy: 0.2682\nEpoch 72/100\n2138/2138 [==============================] - 24s 11ms/step - loss: 1.9626 - accuracy: 0.4293 - val_loss: 2.9339 - val_accuracy: 0.2695\nEpoch 73/100\n2138/2138 [==============================] - 25s 12ms/step - loss: 1.9535 - accuracy: 0.4318 - val_loss: 2.9337 - val_accuracy: 0.2717\nEpoch 74/100\n2138/2138 [==============================] - 24s 11ms/step - loss: 1.9455 - accuracy: 0.4335 - val_loss: 2.9584 - val_accuracy: 0.2686\nEpoch 75/100\n2138/2138 [==============================] - 24s 11ms/step - loss: 1.9370 - accuracy: 0.4345 - val_loss: 2.9395 - val_accuracy: 0.2728\nEpoch 76/100\n2138/2138 [==============================] - 26s 12ms/step - loss: 1.9295 - accuracy: 0.4375 - val_loss: 2.9577 - val_accuracy: 0.2684\nEpoch 77/100\n2138/2138 [==============================] - 24s 11ms/step - loss: 1.9254 - accuracy: 0.4356 - val_loss: 2.9931 - val_accuracy: 0.2652\nEpoch 78/100\n2138/2138 [==============================] - 24s 11ms/step - loss: 1.9151 - accuracy: 0.4405 - val_loss: 2.9924 - val_accuracy: 0.2634\nEpoch 79/100\n2138/2138 [==============================] - 25s 12ms/step - loss: 1.9094 - accuracy: 0.4420 - val_loss: 2.9789 - val_accuracy: 0.2693\nEpoch 80/100\n2138/2138 [==============================] - 32s 15ms/step - loss: 1.8996 - accuracy: 0.4419 - val_loss: 3.0022 - val_accuracy: 0.2668\nEpoch 81/100\n2138/2138 [==============================] - 998s 467ms/step - loss: 1.8937 - accuracy: 0.4441 - val_loss: 3.0014 - val_accuracy: 0.2690\nEpoch 82/100\n2138/2138 [==============================] - 19s 9ms/step - loss: 1.8842 - accuracy: 0.4444 - val_loss: 3.0136 - val_accuracy: 0.2638\nEpoch 83/100\n2138/2138 [==============================] - 19s 9ms/step - loss: 1.8778 - accuracy: 0.4474 - val_loss: 3.0346 - val_accuracy: 0.2657\nEpoch 84/100\n2138/2138 [==============================] - 18s 8ms/step - loss: 1.8747 - accuracy: 0.4474 - val_loss: 3.0347 - val_accuracy: 0.2625\nEpoch 85/100\n2138/2138 [==============================] - 21s 10ms/step - loss: 1.8665 - accuracy: 0.4519 - val_loss: 3.0569 - val_accuracy: 0.2648\nEpoch 86/100\n2138/2138 [==============================] - 18s 8ms/step - loss: 1.8595 - accuracy: 0.4511 - val_loss: 3.0288 - val_accuracy: 0.2650\nEpoch 87/100\n2138/2138 [==============================] - 194s 91ms/step - loss: 1.8491 - accuracy: 0.4550 - val_loss: 3.0633 - val_accuracy: 0.2611\nEpoch 88/100\n2138/2138 [==============================] - 23s 11ms/step - loss: 1.8460 - accuracy: 0.4539 - val_loss: 3.0442 - val_accuracy: 0.2624\nEpoch 89/100\n2138/2138 [==============================] - 21s 10ms/step - loss: 1.8412 - accuracy: 0.4559 - val_loss: 3.0792 - val_accuracy: 0.2606\nEpoch 90/100\n2138/2138 [==============================] - 20s 9ms/step - loss: 1.8315 - accuracy: 0.4585 - val_loss: 3.0700 - val_accuracy: 0.2622\nEpoch 91/100\n2138/2138 [==============================] - 21s 10ms/step - loss: 1.8296 - accuracy: 0.4571 - val_loss: 3.0791 - val_accuracy: 0.2620\nEpoch 92/100\n2138/2138 [==============================] - 33s 15ms/step - loss: 1.8233 - accuracy: 0.4596 - val_loss: 3.0935 - val_accuracy: 0.2607\nEpoch 93/100\n2138/2138 [==============================] - 28s 13ms/step - loss: 1.8121 - accuracy: 0.4632 - val_loss: 3.0909 - val_accuracy: 0.2584\nEpoch 94/100\n2138/2138 [==============================] - 29s 14ms/step - loss: 1.8132 - accuracy: 0.4635 - val_loss: 3.1207 - val_accuracy: 0.2582\nEpoch 95/100\n2138/2138 [==============================] - 22s 10ms/step - loss: 1.8025 - accuracy: 0.4630 - val_loss: 3.1137 - val_accuracy: 0.2595\nEpoch 96/100\n2138/2138 [==============================] - 30s 14ms/step - loss: 1.7971 - accuracy: 0.4674 - val_loss: 3.1197 - val_accuracy: 0.2578\nEpoch 97/100\n2138/2138 [==============================] - 30s 14ms/step - loss: 1.7888 - accuracy: 0.4696 - val_loss: 3.1119 - val_accuracy: 0.2622\nEpoch 98/100\n2138/2138 [==============================] - 25s 12ms/step - loss: 1.7857 - accuracy: 0.4690 - val_loss: 3.1181 - val_accuracy: 0.2598\nEpoch 99/100\n2138/2138 [==============================] - 28s 13ms/step - loss: 1.7796 - accuracy: 0.4717 - val_loss: 3.1512 - val_accuracy: 0.2619\nEpoch 100/100\n2138/2138 [==============================] - 24s 11ms/step - loss: 1.7766 - accuracy: 0.4723 - val_loss: 3.1350 - val_accuracy: 0.2592\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 10,
      "block_group": "9ee991929b654083bc18c9fabb28607b",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model2b2.evaluate(X_train2b, y_train2b))\ny_test_pred2b2 = model2b2.evaluate(X_test2b, y_test2b)\nprint(\"Test Accuracy:\", y_test_pred2b2)",
      "metadata": {
        "cell_id": "96bb067f2f4041c694c82369b0c8b093",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2138/2138 [==============================] - 5s 2ms/step - loss: 1.6976 - accuracy: 0.4958\nTraining Accuracy: [1.6976228952407837, 0.49578210711479187]\n713/713 [==============================] - 2s 3ms/step - loss: 3.1139 - accuracy: 0.2646\nTest Accuracy: [3.1139473915100098, 0.26456141471862793]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 19,
      "block_group": "435c163516c448e5b4b91295f2b21ce7",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "scaler2 = preprocessing.MinMaxScaler()\nscaled_feat2 = scaler2.fit_transform(data2[features2])\n\nX_train2_2, X_test2_2, y_train2_2, y_test2_2 = train_test_split(scaled_feat2, data2[\"track_genre_num\"], test_size=0.2, random_state=1)\nX_train2_2, X_val2_2, y_train2_2, y_val2_2 = train_test_split(X_train2_2, y_train2_2, test_size=0.25, random_state=1)",
      "metadata": {
        "cell_id": "78c9e9a158ac4fcc875e6e6629d82d7f",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "outputs_reference": null,
      "execution_count": 11,
      "block_group": "b0fc57e2d90d42b495b97b5828a10f44",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "model2_2 = Sequential()\n#forget gates = 0, paththrough gate = 1\nmodel2_2.add(LSTM(units=128,input_shape=(14, 1)))\nmodel2_2.add(Dense(114, activation = \"softmax\"))\n\n#bidirectional, attention block (inform importance of other positions)- layers.attention\n\nmodel2_2.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\nhistory2_2 = model2_2.fit(X_train2_2, y_train2_2, validation_data=(X_val2_2, y_val2_2), batch_size=32, epochs=100)",
      "metadata": {
        "cell_id": "bcedb4c3f4dc4ae996a51b45059af790",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n2138/2138 [==============================] - 31s 14ms/step - loss: 4.0507 - accuracy: 0.0662 - val_loss: 3.7472 - val_accuracy: 0.0937\nEpoch 2/100\n2138/2138 [==============================] - 25s 12ms/step - loss: 3.6707 - accuracy: 0.1076 - val_loss: 3.6163 - val_accuracy: 0.1191\nEpoch 3/100\n2138/2138 [==============================] - 31s 14ms/step - loss: 3.5477 - accuracy: 0.1294 - val_loss: 3.5683 - val_accuracy: 0.1320\nEpoch 4/100\n2138/2138 [==============================] - 27s 13ms/step - loss: 3.4463 - accuracy: 0.1464 - val_loss: 3.4154 - val_accuracy: 0.1469\nEpoch 5/100\n2138/2138 [==============================] - 26s 12ms/step - loss: 3.3660 - accuracy: 0.1581 - val_loss: 3.3160 - val_accuracy: 0.1661\nEpoch 6/100\n2138/2138 [==============================] - 28s 13ms/step - loss: 3.2929 - accuracy: 0.1717 - val_loss: 3.2595 - val_accuracy: 0.1802\nEpoch 7/100\n2138/2138 [==============================] - 37s 17ms/step - loss: 3.2129 - accuracy: 0.1857 - val_loss: 3.1583 - val_accuracy: 0.1968\nEpoch 8/100\n2138/2138 [==============================] - 36s 17ms/step - loss: 3.1333 - accuracy: 0.2010 - val_loss: 3.1087 - val_accuracy: 0.2046\nEpoch 9/100\n2138/2138 [==============================] - 40s 19ms/step - loss: 3.0734 - accuracy: 0.2097 - val_loss: 3.0610 - val_accuracy: 0.2129\nEpoch 10/100\n2138/2138 [==============================] - 18s 8ms/step - loss: 3.0276 - accuracy: 0.2200 - val_loss: 3.0546 - val_accuracy: 0.2157\nEpoch 11/100\n2138/2138 [==============================] - 21s 10ms/step - loss: 2.9818 - accuracy: 0.2284 - val_loss: 2.9727 - val_accuracy: 0.2304\nEpoch 12/100\n2138/2138 [==============================] - 19s 9ms/step - loss: 2.9501 - accuracy: 0.2341 - val_loss: 2.9753 - val_accuracy: 0.2300\nEpoch 13/100\n2138/2138 [==============================] - 19s 9ms/step - loss: 2.9174 - accuracy: 0.2393 - val_loss: 2.9558 - val_accuracy: 0.2338\nEpoch 14/100\n2138/2138 [==============================] - 18s 9ms/step - loss: 2.8865 - accuracy: 0.2457 - val_loss: 2.9155 - val_accuracy: 0.2410\nEpoch 15/100\n2138/2138 [==============================] - 23s 11ms/step - loss: 2.8592 - accuracy: 0.2480 - val_loss: 2.9032 - val_accuracy: 0.2425\nEpoch 16/100\n2138/2138 [==============================] - 18s 9ms/step - loss: 2.8367 - accuracy: 0.2526 - val_loss: 2.8710 - val_accuracy: 0.2501\nEpoch 17/100\n2138/2138 [==============================] - 24s 11ms/step - loss: 2.8154 - accuracy: 0.2579 - val_loss: 2.8620 - val_accuracy: 0.2471\nEpoch 18/100\n2138/2138 [==============================] - 23s 11ms/step - loss: 2.7938 - accuracy: 0.2610 - val_loss: 2.8812 - val_accuracy: 0.2446\nEpoch 19/100\n2138/2138 [==============================] - 23s 11ms/step - loss: 2.7745 - accuracy: 0.2647 - val_loss: 2.8372 - val_accuracy: 0.2532\nEpoch 20/100\n2138/2138 [==============================] - 19s 9ms/step - loss: 2.7583 - accuracy: 0.2664 - val_loss: 2.8356 - val_accuracy: 0.2556\nEpoch 21/100\n2138/2138 [==============================] - 22s 10ms/step - loss: 2.7410 - accuracy: 0.2707 - val_loss: 2.8264 - val_accuracy: 0.2560\nEpoch 22/100\n2138/2138 [==============================] - 24s 11ms/step - loss: 2.7230 - accuracy: 0.2731 - val_loss: 2.8249 - val_accuracy: 0.2593\nEpoch 23/100\n2138/2138 [==============================] - 19s 9ms/step - loss: 2.7098 - accuracy: 0.2751 - val_loss: 2.8093 - val_accuracy: 0.2593\nEpoch 24/100\n2138/2138 [==============================] - 20s 10ms/step - loss: 2.6929 - accuracy: 0.2773 - val_loss: 2.8219 - val_accuracy: 0.2594\nEpoch 25/100\n2138/2138 [==============================] - 21s 10ms/step - loss: 2.6771 - accuracy: 0.2815 - val_loss: 2.8113 - val_accuracy: 0.2620\nEpoch 26/100\n2138/2138 [==============================] - 21s 10ms/step - loss: 2.6601 - accuracy: 0.2853 - val_loss: 2.8097 - val_accuracy: 0.2589\nEpoch 27/100\n2138/2138 [==============================] - 20s 9ms/step - loss: 2.6420 - accuracy: 0.2872 - val_loss: 2.8095 - val_accuracy: 0.2620\nEpoch 28/100\n2138/2138 [==============================] - 17s 8ms/step - loss: 2.6277 - accuracy: 0.2905 - val_loss: 2.7962 - val_accuracy: 0.2625\nEpoch 29/100\n2138/2138 [==============================] - 19s 9ms/step - loss: 2.6074 - accuracy: 0.2948 - val_loss: 2.8014 - val_accuracy: 0.2644\nEpoch 30/100\n2138/2138 [==============================] - 17s 8ms/step - loss: 2.5944 - accuracy: 0.2975 - val_loss: 2.7780 - val_accuracy: 0.2713\nEpoch 31/100\n2138/2138 [==============================] - 16s 7ms/step - loss: 2.5762 - accuracy: 0.3007 - val_loss: 2.7920 - val_accuracy: 0.2681\nEpoch 32/100\n2138/2138 [==============================] - 17s 8ms/step - loss: 2.5556 - accuracy: 0.3048 - val_loss: 2.7696 - val_accuracy: 0.2723\nEpoch 33/100\n2138/2138 [==============================] - 17s 8ms/step - loss: 2.5359 - accuracy: 0.3101 - val_loss: 2.8049 - val_accuracy: 0.2637\nEpoch 34/100\n2138/2138 [==============================] - 21s 10ms/step - loss: 2.5209 - accuracy: 0.3141 - val_loss: 2.7731 - val_accuracy: 0.2707\nEpoch 35/100\n2138/2138 [==============================] - 20s 9ms/step - loss: 2.5029 - accuracy: 0.3160 - val_loss: 2.7453 - val_accuracy: 0.2804\nEpoch 36/100\n2138/2138 [==============================] - 20s 9ms/step - loss: 2.4883 - accuracy: 0.3203 - val_loss: 2.7740 - val_accuracy: 0.2719\nEpoch 37/100\n2138/2138 [==============================] - 16s 8ms/step - loss: 2.4727 - accuracy: 0.3220 - val_loss: 2.7596 - val_accuracy: 0.2774\nEpoch 38/100\n2138/2138 [==============================] - 17s 8ms/step - loss: 2.4571 - accuracy: 0.3254 - val_loss: 2.7704 - val_accuracy: 0.2763\nEpoch 39/100\n2138/2138 [==============================] - 25s 12ms/step - loss: 2.4443 - accuracy: 0.3278 - val_loss: 2.7486 - val_accuracy: 0.2786\nEpoch 40/100\n2138/2138 [==============================] - 18s 8ms/step - loss: 2.4299 - accuracy: 0.3299 - val_loss: 2.7764 - val_accuracy: 0.2726\nEpoch 41/100\n2138/2138 [==============================] - 21s 10ms/step - loss: 2.4153 - accuracy: 0.3345 - val_loss: 2.7506 - val_accuracy: 0.2800\nEpoch 42/100\n2138/2138 [==============================] - 17s 8ms/step - loss: 2.4009 - accuracy: 0.3350 - val_loss: 2.7557 - val_accuracy: 0.2807\nEpoch 43/100\n2138/2138 [==============================] - 17s 8ms/step - loss: 2.3866 - accuracy: 0.3406 - val_loss: 2.7572 - val_accuracy: 0.2795\nEpoch 44/100\n2138/2138 [==============================] - 17s 8ms/step - loss: 2.3759 - accuracy: 0.3422 - val_loss: 2.7506 - val_accuracy: 0.2803\nEpoch 45/100\n2138/2138 [==============================] - 16s 8ms/step - loss: 2.3587 - accuracy: 0.3457 - val_loss: 2.7512 - val_accuracy: 0.2794\nEpoch 46/100\n2138/2138 [==============================] - 16s 8ms/step - loss: 2.3483 - accuracy: 0.3469 - val_loss: 2.7565 - val_accuracy: 0.2799\nEpoch 47/100\n2138/2138 [==============================] - 16s 8ms/step - loss: 2.3357 - accuracy: 0.3509 - val_loss: 2.7630 - val_accuracy: 0.2822\nEpoch 48/100\n2138/2138 [==============================] - 17s 8ms/step - loss: 2.3234 - accuracy: 0.3522 - val_loss: 2.7768 - val_accuracy: 0.2799\nEpoch 49/100\n2138/2138 [==============================] - 14s 7ms/step - loss: 2.3118 - accuracy: 0.3544 - val_loss: 2.7588 - val_accuracy: 0.2827\nEpoch 50/100\n2138/2138 [==============================] - 15s 7ms/step - loss: 2.2948 - accuracy: 0.3570 - val_loss: 2.7740 - val_accuracy: 0.2824\nEpoch 51/100\n2138/2138 [==============================] - 13s 6ms/step - loss: 2.2854 - accuracy: 0.3603 - val_loss: 2.7693 - val_accuracy: 0.2814\nEpoch 52/100\n2138/2138 [==============================] - 15s 7ms/step - loss: 2.2735 - accuracy: 0.3623 - val_loss: 2.7817 - val_accuracy: 0.2788\nEpoch 53/100\n2138/2138 [==============================] - 14s 6ms/step - loss: 2.2591 - accuracy: 0.3660 - val_loss: 2.7838 - val_accuracy: 0.2804\nEpoch 54/100\n2138/2138 [==============================] - 15s 7ms/step - loss: 2.2469 - accuracy: 0.3691 - val_loss: 2.8012 - val_accuracy: 0.2792\nEpoch 55/100\n2138/2138 [==============================] - 15s 7ms/step - loss: 2.2388 - accuracy: 0.3687 - val_loss: 2.7928 - val_accuracy: 0.2814\nEpoch 56/100\n2138/2138 [==============================] - 14s 7ms/step - loss: 2.2248 - accuracy: 0.3715 - val_loss: 2.7829 - val_accuracy: 0.2785\nEpoch 57/100\n2138/2138 [==============================] - 14s 6ms/step - loss: 2.2120 - accuracy: 0.3744 - val_loss: 2.7742 - val_accuracy: 0.2825\nEpoch 58/100\n2138/2138 [==============================] - 14s 7ms/step - loss: 2.2024 - accuracy: 0.3780 - val_loss: 2.7929 - val_accuracy: 0.2813\nEpoch 59/100\n2138/2138 [==============================] - 15s 7ms/step - loss: 2.1904 - accuracy: 0.3795 - val_loss: 2.8028 - val_accuracy: 0.2795\nEpoch 60/100\n2138/2138 [==============================] - 14s 6ms/step - loss: 2.1779 - accuracy: 0.3829 - val_loss: 2.8089 - val_accuracy: 0.2772\nEpoch 61/100\n2138/2138 [==============================] - 15s 7ms/step - loss: 2.1662 - accuracy: 0.3857 - val_loss: 2.8161 - val_accuracy: 0.2796\nEpoch 62/100\n2138/2138 [==============================] - 14s 7ms/step - loss: 2.1536 - accuracy: 0.3874 - val_loss: 2.8278 - val_accuracy: 0.2775\nEpoch 63/100\n2138/2138 [==============================] - 14s 7ms/step - loss: 2.1454 - accuracy: 0.3907 - val_loss: 2.8383 - val_accuracy: 0.2761\nEpoch 64/100\n2138/2138 [==============================] - 16s 8ms/step - loss: 2.1355 - accuracy: 0.3923 - val_loss: 2.8134 - val_accuracy: 0.2794\nEpoch 65/100\n2138/2138 [==============================] - 14s 7ms/step - loss: 2.1235 - accuracy: 0.3930 - val_loss: 2.8389 - val_accuracy: 0.2784\nEpoch 66/100\n2138/2138 [==============================] - 22s 10ms/step - loss: 2.1133 - accuracy: 0.3974 - val_loss: 2.8362 - val_accuracy: 0.2767\nEpoch 67/100\n2138/2138 [==============================] - 979s 458ms/step - loss: 2.1024 - accuracy: 0.3991 - val_loss: 2.8402 - val_accuracy: 0.2800\nEpoch 68/100\n2138/2138 [==============================] - 16s 7ms/step - loss: 2.0930 - accuracy: 0.4008 - val_loss: 2.8544 - val_accuracy: 0.2737\nEpoch 69/100\n2138/2138 [==============================] - 13s 6ms/step - loss: 2.0818 - accuracy: 0.4029 - val_loss: 2.8562 - val_accuracy: 0.2741\nEpoch 70/100\n2138/2138 [==============================] - 1002s 469ms/step - loss: 2.0711 - accuracy: 0.4062 - val_loss: 2.8559 - val_accuracy: 0.2785\nEpoch 71/100\n2138/2138 [==============================] - 15s 7ms/step - loss: 2.0633 - accuracy: 0.4061 - val_loss: 2.8548 - val_accuracy: 0.2763\nEpoch 72/100\n2138/2138 [==============================] - 15s 7ms/step - loss: 2.0536 - accuracy: 0.4091 - val_loss: 2.8777 - val_accuracy: 0.2729\nEpoch 73/100\n2138/2138 [==============================] - 14s 6ms/step - loss: 2.0408 - accuracy: 0.4129 - val_loss: 2.8781 - val_accuracy: 0.2771\nEpoch 74/100\n2138/2138 [==============================] - 15s 7ms/step - loss: 2.0345 - accuracy: 0.4120 - val_loss: 2.8855 - val_accuracy: 0.2763\nEpoch 75/100\n2138/2138 [==============================] - 13s 6ms/step - loss: 2.0232 - accuracy: 0.4167 - val_loss: 2.9050 - val_accuracy: 0.2739\nEpoch 76/100\n2138/2138 [==============================] - 16s 7ms/step - loss: 2.0140 - accuracy: 0.4191 - val_loss: 2.8845 - val_accuracy: 0.2764\nEpoch 77/100\n2138/2138 [==============================] - 14s 6ms/step - loss: 2.0029 - accuracy: 0.4221 - val_loss: 2.8972 - val_accuracy: 0.2744\nEpoch 78/100\n2138/2138 [==============================] - 14s 6ms/step - loss: 1.9950 - accuracy: 0.4229 - val_loss: 2.9160 - val_accuracy: 0.2719\nEpoch 79/100\n2138/2138 [==============================] - 14s 7ms/step - loss: 1.9889 - accuracy: 0.4250 - val_loss: 2.9066 - val_accuracy: 0.2718\nEpoch 80/100\n2138/2138 [==============================] - 13s 6ms/step - loss: 1.9807 - accuracy: 0.4274 - val_loss: 2.9168 - val_accuracy: 0.2731\nEpoch 81/100\n2138/2138 [==============================] - 14s 7ms/step - loss: 1.9709 - accuracy: 0.4290 - val_loss: 2.9295 - val_accuracy: 0.2731\nEpoch 82/100\n2138/2138 [==============================] - 13s 6ms/step - loss: 1.9624 - accuracy: 0.4313 - val_loss: 2.9410 - val_accuracy: 0.2710\nEpoch 83/100\n2138/2138 [==============================] - 13s 6ms/step - loss: 1.9549 - accuracy: 0.4343 - val_loss: 2.9363 - val_accuracy: 0.2707\nEpoch 84/100\n2138/2138 [==============================] - 13s 6ms/step - loss: 1.9448 - accuracy: 0.4324 - val_loss: 2.9562 - val_accuracy: 0.2707\nEpoch 85/100\n2138/2138 [==============================] - 17s 8ms/step - loss: 1.9366 - accuracy: 0.4353 - val_loss: 2.9686 - val_accuracy: 0.2699\nEpoch 86/100\n2138/2138 [==============================] - 14s 6ms/step - loss: 1.9303 - accuracy: 0.4373 - val_loss: 2.9529 - val_accuracy: 0.2718\nEpoch 87/100\n2138/2138 [==============================] - 13s 6ms/step - loss: 1.9213 - accuracy: 0.4391 - val_loss: 2.9640 - val_accuracy: 0.2709\nEpoch 88/100\n2138/2138 [==============================] - 13s 6ms/step - loss: 1.9152 - accuracy: 0.4391 - val_loss: 2.9815 - val_accuracy: 0.2689\nEpoch 89/100\n2138/2138 [==============================] - 13s 6ms/step - loss: 1.9026 - accuracy: 0.4443 - val_loss: 2.9977 - val_accuracy: 0.2669\nEpoch 90/100\n2138/2138 [==============================] - 13s 6ms/step - loss: 1.8960 - accuracy: 0.4434 - val_loss: 2.9952 - val_accuracy: 0.2680\nEpoch 91/100\n2138/2138 [==============================] - 12s 6ms/step - loss: 1.8886 - accuracy: 0.4463 - val_loss: 3.0140 - val_accuracy: 0.2661\nEpoch 92/100\n2138/2138 [==============================] - 12s 6ms/step - loss: 1.8832 - accuracy: 0.4471 - val_loss: 2.9961 - val_accuracy: 0.2654\nEpoch 93/100\n2138/2138 [==============================] - 12s 6ms/step - loss: 1.8755 - accuracy: 0.4497 - val_loss: 3.0186 - val_accuracy: 0.2674\nEpoch 94/100\n2138/2138 [==============================] - 13s 6ms/step - loss: 1.8690 - accuracy: 0.4508 - val_loss: 3.0264 - val_accuracy: 0.2627\nEpoch 95/100\n2138/2138 [==============================] - 14s 7ms/step - loss: 1.8621 - accuracy: 0.4517 - val_loss: 3.0206 - val_accuracy: 0.2698\nEpoch 96/100\n2138/2138 [==============================] - 14s 7ms/step - loss: 1.8503 - accuracy: 0.4552 - val_loss: 3.0410 - val_accuracy: 0.2663\nEpoch 97/100\n2138/2138 [==============================] - 14s 7ms/step - loss: 1.8484 - accuracy: 0.4559 - val_loss: 3.0454 - val_accuracy: 0.2680\nEpoch 98/100\n2138/2138 [==============================] - 19s 9ms/step - loss: 1.8431 - accuracy: 0.4584 - val_loss: 3.0525 - val_accuracy: 0.2641\nEpoch 99/100\n2138/2138 [==============================] - 16s 7ms/step - loss: 1.8337 - accuracy: 0.4589 - val_loss: 3.0657 - val_accuracy: 0.2639\nEpoch 100/100\n2138/2138 [==============================] - 16s 7ms/step - loss: 1.8261 - accuracy: 0.4606 - val_loss: 3.0712 - val_accuracy: 0.2634\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 12,
      "block_group": "8fb7cb438cc3432f9f3f174b5502e5d1",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model2_2.evaluate(X_train2_2, y_train2_2))\nprint(\"Test Accuracy:\", model2_2.evaluate(X_test2_2, y_test2_2))",
      "metadata": {
        "cell_id": "09ae1844bdc84207ac3bf2481f37fc25",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2138/2138 [==============================] - 4s 2ms/step - loss: 1.7511 - accuracy: 0.4836\nTraining Accuracy: [1.7510982751846313, 0.4836181700229645]\n713/713 [==============================] - 2s 2ms/step - loss: 3.0486 - accuracy: 0.2733\nTest Accuracy: [3.048637866973877, 0.2732894718647003]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 13,
      "block_group": "e7d543aa17c843558825e56a27a0c6e5",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "model2_3 = Sequential()\n#forget gates = 0, paththrough gate = 1\n\nmodel2_3.add((Bidirectional(LSTM(units=128, input_shape=(14, 1)))))\nmodel2_3.add(Dense(114, activation = \"softmax\"))\n\n#bidirectional, attention block (inform importance of other positions)- layers.attention\n\nmodel2_3.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\nhistory2_3 = model2_3.fit(np.expand_dims(X_train2_2, 1), y_train2_2, validation_data=(np.expand_dims(X_val2_2, 1), y_val2_2), batch_size=32, epochs=100)",
      "metadata": {
        "cell_id": "f42527df966e4fe484e2007a9fe8e0e5",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n2138/2138 [==============================] - 8s 2ms/step - loss: 3.8647 - accuracy: 0.1213 - val_loss: 3.6175 - val_accuracy: 0.1511\nEpoch 2/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.5876 - accuracy: 0.1590 - val_loss: 3.5474 - val_accuracy: 0.1663\nEpoch 3/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.5258 - accuracy: 0.1712 - val_loss: 3.4787 - val_accuracy: 0.1770\nEpoch 4/100\n2138/2138 [==============================] - 4s 2ms/step - loss: 3.4322 - accuracy: 0.1876 - val_loss: 3.3675 - val_accuracy: 0.1961\nEpoch 5/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.3121 - accuracy: 0.2028 - val_loss: 3.2511 - val_accuracy: 0.2110\nEpoch 6/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.2368 - accuracy: 0.2138 - val_loss: 3.2087 - val_accuracy: 0.2153\nEpoch 7/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.1935 - accuracy: 0.2180 - val_loss: 3.1715 - val_accuracy: 0.2188\nEpoch 8/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 3.1599 - accuracy: 0.2247 - val_loss: 3.1390 - val_accuracy: 0.2283\nEpoch 9/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 3.1318 - accuracy: 0.2284 - val_loss: 3.1194 - val_accuracy: 0.2248\nEpoch 10/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.1058 - accuracy: 0.2333 - val_loss: 3.1036 - val_accuracy: 0.2255\nEpoch 11/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.0806 - accuracy: 0.2356 - val_loss: 3.0762 - val_accuracy: 0.2326\nEpoch 12/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.0560 - accuracy: 0.2408 - val_loss: 3.0487 - val_accuracy: 0.2379\nEpoch 13/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.0330 - accuracy: 0.2443 - val_loss: 3.0284 - val_accuracy: 0.2401\nEpoch 14/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 3.0122 - accuracy: 0.2474 - val_loss: 3.0267 - val_accuracy: 0.2366\nEpoch 15/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.9912 - accuracy: 0.2511 - val_loss: 3.0038 - val_accuracy: 0.2407\nEpoch 16/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.9725 - accuracy: 0.2542 - val_loss: 2.9833 - val_accuracy: 0.2464\nEpoch 17/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.9555 - accuracy: 0.2558 - val_loss: 2.9694 - val_accuracy: 0.2511\nEpoch 18/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.9404 - accuracy: 0.2578 - val_loss: 2.9513 - val_accuracy: 0.2530\nEpoch 19/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.9248 - accuracy: 0.2608 - val_loss: 2.9512 - val_accuracy: 0.2541\nEpoch 20/100\n2138/2138 [==============================] - 4s 2ms/step - loss: 2.9101 - accuracy: 0.2629 - val_loss: 2.9265 - val_accuracy: 0.2572\nEpoch 21/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8964 - accuracy: 0.2655 - val_loss: 2.9189 - val_accuracy: 0.2582\nEpoch 22/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8830 - accuracy: 0.2672 - val_loss: 2.9132 - val_accuracy: 0.2551\nEpoch 23/100\n2138/2138 [==============================] - 4s 2ms/step - loss: 2.8698 - accuracy: 0.2698 - val_loss: 2.8966 - val_accuracy: 0.2636\nEpoch 24/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8580 - accuracy: 0.2708 - val_loss: 2.8849 - val_accuracy: 0.2616\nEpoch 25/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8451 - accuracy: 0.2734 - val_loss: 2.8774 - val_accuracy: 0.2637\nEpoch 26/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8328 - accuracy: 0.2752 - val_loss: 2.8738 - val_accuracy: 0.2653\nEpoch 27/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8212 - accuracy: 0.2774 - val_loss: 2.8632 - val_accuracy: 0.2663\nEpoch 28/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8104 - accuracy: 0.2787 - val_loss: 2.8610 - val_accuracy: 0.2636\nEpoch 29/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7995 - accuracy: 0.2809 - val_loss: 2.8626 - val_accuracy: 0.2651\nEpoch 30/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7893 - accuracy: 0.2820 - val_loss: 2.8320 - val_accuracy: 0.2718\nEpoch 31/100\n2138/2138 [==============================] - 4s 2ms/step - loss: 2.7795 - accuracy: 0.2840 - val_loss: 2.8272 - val_accuracy: 0.2723\nEpoch 32/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.7703 - accuracy: 0.2850 - val_loss: 2.8213 - val_accuracy: 0.2726\nEpoch 33/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.7605 - accuracy: 0.2851 - val_loss: 2.8228 - val_accuracy: 0.2738\nEpoch 34/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.7513 - accuracy: 0.2893 - val_loss: 2.8095 - val_accuracy: 0.2756\nEpoch 35/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7431 - accuracy: 0.2895 - val_loss: 2.8060 - val_accuracy: 0.2760\nEpoch 36/100\n2138/2138 [==============================] - 4s 2ms/step - loss: 2.7349 - accuracy: 0.2904 - val_loss: 2.8036 - val_accuracy: 0.2742\nEpoch 37/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7272 - accuracy: 0.2936 - val_loss: 2.7879 - val_accuracy: 0.2781\nEpoch 38/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7185 - accuracy: 0.2934 - val_loss: 2.7883 - val_accuracy: 0.2791\nEpoch 39/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.7110 - accuracy: 0.2958 - val_loss: 2.7800 - val_accuracy: 0.2803\nEpoch 40/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7042 - accuracy: 0.2970 - val_loss: 2.7715 - val_accuracy: 0.2818\nEpoch 41/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.6968 - accuracy: 0.2982 - val_loss: 2.7801 - val_accuracy: 0.2788\nEpoch 42/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6892 - accuracy: 0.3000 - val_loss: 2.7647 - val_accuracy: 0.2823\nEpoch 43/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6830 - accuracy: 0.3010 - val_loss: 2.7615 - val_accuracy: 0.2823\nEpoch 44/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 2.6762 - accuracy: 0.3019 - val_loss: 2.7579 - val_accuracy: 0.2849\nEpoch 45/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.6703 - accuracy: 0.3029 - val_loss: 2.7565 - val_accuracy: 0.2817\nEpoch 46/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6629 - accuracy: 0.3054 - val_loss: 2.7540 - val_accuracy: 0.2851\nEpoch 47/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6582 - accuracy: 0.3042 - val_loss: 2.7477 - val_accuracy: 0.2866\nEpoch 48/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6509 - accuracy: 0.3068 - val_loss: 2.7415 - val_accuracy: 0.2866\nEpoch 49/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6451 - accuracy: 0.3071 - val_loss: 2.7401 - val_accuracy: 0.2879\nEpoch 50/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6392 - accuracy: 0.3088 - val_loss: 2.7398 - val_accuracy: 0.2841\nEpoch 51/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6343 - accuracy: 0.3093 - val_loss: 2.7317 - val_accuracy: 0.2900\nEpoch 52/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6283 - accuracy: 0.3095 - val_loss: 2.7312 - val_accuracy: 0.2894\nEpoch 53/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6229 - accuracy: 0.3093 - val_loss: 2.7236 - val_accuracy: 0.2896\nEpoch 54/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6178 - accuracy: 0.3103 - val_loss: 2.7243 - val_accuracy: 0.2900\nEpoch 55/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6112 - accuracy: 0.3135 - val_loss: 2.7214 - val_accuracy: 0.2925\nEpoch 56/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6077 - accuracy: 0.3154 - val_loss: 2.7224 - val_accuracy: 0.2887\nEpoch 57/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6028 - accuracy: 0.3142 - val_loss: 2.7131 - val_accuracy: 0.2922\nEpoch 58/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5976 - accuracy: 0.3160 - val_loss: 2.7171 - val_accuracy: 0.2903\nEpoch 59/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5931 - accuracy: 0.3169 - val_loss: 2.7115 - val_accuracy: 0.2944\nEpoch 60/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5885 - accuracy: 0.3186 - val_loss: 2.7124 - val_accuracy: 0.2918\nEpoch 61/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5840 - accuracy: 0.3195 - val_loss: 2.7087 - val_accuracy: 0.2947\nEpoch 62/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5796 - accuracy: 0.3193 - val_loss: 2.7034 - val_accuracy: 0.2936\nEpoch 63/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5751 - accuracy: 0.3204 - val_loss: 2.7019 - val_accuracy: 0.2910\nEpoch 64/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5713 - accuracy: 0.3205 - val_loss: 2.6994 - val_accuracy: 0.2932\nEpoch 65/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5662 - accuracy: 0.3218 - val_loss: 2.6968 - val_accuracy: 0.2944\nEpoch 66/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5636 - accuracy: 0.3227 - val_loss: 2.6975 - val_accuracy: 0.2931\nEpoch 67/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5584 - accuracy: 0.3232 - val_loss: 2.6964 - val_accuracy: 0.2946\nEpoch 68/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5552 - accuracy: 0.3240 - val_loss: 2.6969 - val_accuracy: 0.2936\nEpoch 69/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5517 - accuracy: 0.3238 - val_loss: 2.6983 - val_accuracy: 0.2934\nEpoch 70/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5477 - accuracy: 0.3244 - val_loss: 2.6918 - val_accuracy: 0.2952\nEpoch 71/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5438 - accuracy: 0.3262 - val_loss: 2.6852 - val_accuracy: 0.2961\nEpoch 72/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5397 - accuracy: 0.3253 - val_loss: 2.6948 - val_accuracy: 0.2969\nEpoch 73/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5361 - accuracy: 0.3250 - val_loss: 2.6852 - val_accuracy: 0.2955\nEpoch 74/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.5324 - accuracy: 0.3281 - val_loss: 2.6861 - val_accuracy: 0.2946\nEpoch 75/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5293 - accuracy: 0.3289 - val_loss: 2.6841 - val_accuracy: 0.2946\nEpoch 76/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5265 - accuracy: 0.3293 - val_loss: 2.6815 - val_accuracy: 0.2981\nEpoch 77/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5223 - accuracy: 0.3282 - val_loss: 2.6833 - val_accuracy: 0.2961\nEpoch 78/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5191 - accuracy: 0.3299 - val_loss: 2.6836 - val_accuracy: 0.2942\nEpoch 79/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5160 - accuracy: 0.3298 - val_loss: 2.6800 - val_accuracy: 0.2969\nEpoch 80/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5138 - accuracy: 0.3306 - val_loss: 2.6735 - val_accuracy: 0.2983\nEpoch 81/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5095 - accuracy: 0.3319 - val_loss: 2.6775 - val_accuracy: 0.2978\nEpoch 82/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5063 - accuracy: 0.3323 - val_loss: 2.6797 - val_accuracy: 0.2961\nEpoch 83/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5033 - accuracy: 0.3325 - val_loss: 2.6767 - val_accuracy: 0.2971\nEpoch 84/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.5004 - accuracy: 0.3326 - val_loss: 2.6778 - val_accuracy: 0.2965\nEpoch 85/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4980 - accuracy: 0.3313 - val_loss: 2.6795 - val_accuracy: 0.2967\nEpoch 86/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4933 - accuracy: 0.3331 - val_loss: 2.6736 - val_accuracy: 0.2986\nEpoch 87/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4909 - accuracy: 0.3338 - val_loss: 2.6807 - val_accuracy: 0.2976\nEpoch 88/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4891 - accuracy: 0.3351 - val_loss: 2.6702 - val_accuracy: 0.2997\nEpoch 89/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4858 - accuracy: 0.3355 - val_loss: 2.6739 - val_accuracy: 0.2981\nEpoch 90/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4825 - accuracy: 0.3359 - val_loss: 2.6681 - val_accuracy: 0.3004\nEpoch 91/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4805 - accuracy: 0.3374 - val_loss: 2.6809 - val_accuracy: 0.2953\nEpoch 92/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4783 - accuracy: 0.3372 - val_loss: 2.6723 - val_accuracy: 0.2996\nEpoch 93/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4751 - accuracy: 0.3367 - val_loss: 2.6707 - val_accuracy: 0.2978\nEpoch 94/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4725 - accuracy: 0.3368 - val_loss: 2.6672 - val_accuracy: 0.3004\nEpoch 95/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4696 - accuracy: 0.3381 - val_loss: 2.6671 - val_accuracy: 0.3002\nEpoch 96/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4669 - accuracy: 0.3388 - val_loss: 2.6741 - val_accuracy: 0.2998\nEpoch 97/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4642 - accuracy: 0.3381 - val_loss: 2.6662 - val_accuracy: 0.2973\nEpoch 98/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 2.4626 - accuracy: 0.3388 - val_loss: 2.6621 - val_accuracy: 0.2996\nEpoch 99/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4601 - accuracy: 0.3391 - val_loss: 2.6629 - val_accuracy: 0.3011\nEpoch 100/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.4571 - accuracy: 0.3409 - val_loss: 2.6585 - val_accuracy: 0.2988\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 14,
      "block_group": "764ca710350d4e66b148e38438280ac6",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model2_3.evaluate(np.expand_dims(X_train2_2, 1), y_train2_2))\nprint(\"Test Accuracy:\", model2_3.evaluate(np.expand_dims(X_test2_2, 1), y_test2_2))",
      "metadata": {
        "cell_id": "798a2fd0cebf4c71bcd2694eb5ffa828",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2138/2138 [==============================] - 2s 743us/step - loss: 2.4271 - accuracy: 0.3472\nTraining Accuracy: [2.427096128463745, 0.3471542000770569]\n713/713 [==============================] - 1s 722us/step - loss: 2.6346 - accuracy: 0.3065\nTest Accuracy: [2.6345653533935547, 0.30653509497642517]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 15,
      "block_group": "6d3a30179682434f9ce28de263a5d527",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "model2_4 = Sequential()\n#forget gates = 0, paththrough gate = 1\n\nmodel2_4.add(Bidirectional(LSTM(units=128, input_shape=(14, 1))))\nmodel2_4.add(Dropout(0.1))\nmodel2_4.add(Dense(114, activation = \"softmax\"))\n\n#bidirectional, attention block (inform importance of other positions)- layers.attention\n\nmodel2_4.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\nhistory2_4 = model2_4.fit(np.expand_dims(X_train2_2, 1), y_train2_2, validation_data=(np.expand_dims(X_val2_2, 1), y_val2_2), batch_size=32, epochs=100)",
      "metadata": {
        "cell_id": "cba9eea6946040e48095ec0eaced04a9",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Epoch 1/100\n2138/2138 [==============================] - 8s 3ms/step - loss: 3.8814 - accuracy: 0.1191 - val_loss: 3.6228 - val_accuracy: 0.1545\nEpoch 2/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.6009 - accuracy: 0.1576 - val_loss: 3.5594 - val_accuracy: 0.1664\nEpoch 3/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 3.5458 - accuracy: 0.1692 - val_loss: 3.4942 - val_accuracy: 0.1764\nEpoch 4/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 3.4590 - accuracy: 0.1800 - val_loss: 3.3771 - val_accuracy: 0.1941\nEpoch 5/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.3412 - accuracy: 0.1963 - val_loss: 3.2665 - val_accuracy: 0.2104\nEpoch 6/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.2636 - accuracy: 0.2088 - val_loss: 3.2176 - val_accuracy: 0.2124\nEpoch 7/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.2190 - accuracy: 0.2145 - val_loss: 3.1791 - val_accuracy: 0.2176\nEpoch 8/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.1876 - accuracy: 0.2187 - val_loss: 3.1660 - val_accuracy: 0.2187\nEpoch 9/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 3.1612 - accuracy: 0.2233 - val_loss: 3.1300 - val_accuracy: 0.2265\nEpoch 10/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.1361 - accuracy: 0.2260 - val_loss: 3.1078 - val_accuracy: 0.2289\nEpoch 11/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.1157 - accuracy: 0.2292 - val_loss: 3.0849 - val_accuracy: 0.2336\nEpoch 12/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.0962 - accuracy: 0.2335 - val_loss: 3.0738 - val_accuracy: 0.2371\nEpoch 13/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.0748 - accuracy: 0.2365 - val_loss: 3.0587 - val_accuracy: 0.2341\nEpoch 14/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.0563 - accuracy: 0.2394 - val_loss: 3.0319 - val_accuracy: 0.2378\nEpoch 15/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 3.0402 - accuracy: 0.2414 - val_loss: 3.0130 - val_accuracy: 0.2423\nEpoch 16/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.0237 - accuracy: 0.2447 - val_loss: 3.0064 - val_accuracy: 0.2447\nEpoch 17/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 3.0058 - accuracy: 0.2466 - val_loss: 2.9836 - val_accuracy: 0.2461\nEpoch 18/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.9924 - accuracy: 0.2482 - val_loss: 2.9885 - val_accuracy: 0.2439\nEpoch 19/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.9800 - accuracy: 0.2499 - val_loss: 2.9621 - val_accuracy: 0.2477\nEpoch 20/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 2.9651 - accuracy: 0.2510 - val_loss: 2.9546 - val_accuracy: 0.2503\nEpoch 21/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.9538 - accuracy: 0.2540 - val_loss: 2.9395 - val_accuracy: 0.2520\nEpoch 22/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 2.9436 - accuracy: 0.2554 - val_loss: 2.9360 - val_accuracy: 0.2518\nEpoch 23/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.9319 - accuracy: 0.2584 - val_loss: 2.9344 - val_accuracy: 0.2546\nEpoch 24/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.9207 - accuracy: 0.2596 - val_loss: 2.9143 - val_accuracy: 0.2577\nEpoch 25/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.9120 - accuracy: 0.2611 - val_loss: 2.9098 - val_accuracy: 0.2591\nEpoch 26/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.9049 - accuracy: 0.2631 - val_loss: 2.8997 - val_accuracy: 0.2601\nEpoch 27/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8961 - accuracy: 0.2613 - val_loss: 2.8929 - val_accuracy: 0.2614\nEpoch 28/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8864 - accuracy: 0.2654 - val_loss: 2.8802 - val_accuracy: 0.2651\nEpoch 29/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8810 - accuracy: 0.2671 - val_loss: 2.8776 - val_accuracy: 0.2643\nEpoch 30/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 2.8708 - accuracy: 0.2694 - val_loss: 2.8776 - val_accuracy: 0.2651\nEpoch 31/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.8633 - accuracy: 0.2690 - val_loss: 2.8679 - val_accuracy: 0.2650\nEpoch 32/100\n2138/2138 [==============================] - 8s 4ms/step - loss: 2.8559 - accuracy: 0.2707 - val_loss: 2.8563 - val_accuracy: 0.2686\nEpoch 33/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.8465 - accuracy: 0.2707 - val_loss: 2.8500 - val_accuracy: 0.2686\nEpoch 34/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.8401 - accuracy: 0.2725 - val_loss: 2.8429 - val_accuracy: 0.2674\nEpoch 35/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.8326 - accuracy: 0.2737 - val_loss: 2.8336 - val_accuracy: 0.2700\nEpoch 36/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.8285 - accuracy: 0.2757 - val_loss: 2.8316 - val_accuracy: 0.2706\nEpoch 37/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8197 - accuracy: 0.2748 - val_loss: 2.8225 - val_accuracy: 0.2757\nEpoch 38/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8122 - accuracy: 0.2771 - val_loss: 2.8174 - val_accuracy: 0.2744\nEpoch 39/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8078 - accuracy: 0.2784 - val_loss: 2.8150 - val_accuracy: 0.2719\nEpoch 40/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.8034 - accuracy: 0.2792 - val_loss: 2.8119 - val_accuracy: 0.2745\nEpoch 41/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7950 - accuracy: 0.2805 - val_loss: 2.7990 - val_accuracy: 0.2791\nEpoch 42/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.7900 - accuracy: 0.2790 - val_loss: 2.7987 - val_accuracy: 0.2751\nEpoch 43/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7836 - accuracy: 0.2820 - val_loss: 2.7916 - val_accuracy: 0.2825\nEpoch 44/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7770 - accuracy: 0.2828 - val_loss: 2.7879 - val_accuracy: 0.2789\nEpoch 45/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7721 - accuracy: 0.2828 - val_loss: 2.7862 - val_accuracy: 0.2791\nEpoch 46/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7652 - accuracy: 0.2852 - val_loss: 2.7749 - val_accuracy: 0.2836\nEpoch 47/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7596 - accuracy: 0.2855 - val_loss: 2.7746 - val_accuracy: 0.2820\nEpoch 48/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 2.7565 - accuracy: 0.2863 - val_loss: 2.7700 - val_accuracy: 0.2850\nEpoch 49/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 2.7486 - accuracy: 0.2890 - val_loss: 2.7639 - val_accuracy: 0.2837\nEpoch 50/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7436 - accuracy: 0.2890 - val_loss: 2.7731 - val_accuracy: 0.2832\nEpoch 51/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.7410 - accuracy: 0.2891 - val_loss: 2.7610 - val_accuracy: 0.2842\nEpoch 52/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.7325 - accuracy: 0.2895 - val_loss: 2.7596 - val_accuracy: 0.2858\nEpoch 53/100\n2138/2138 [==============================] - 10s 5ms/step - loss: 2.7299 - accuracy: 0.2923 - val_loss: 2.7480 - val_accuracy: 0.2892\nEpoch 54/100\n2138/2138 [==============================] - 9s 4ms/step - loss: 2.7243 - accuracy: 0.2938 - val_loss: 2.7438 - val_accuracy: 0.2875\nEpoch 55/100\n2138/2138 [==============================] - 9s 4ms/step - loss: 2.7211 - accuracy: 0.2940 - val_loss: 2.7513 - val_accuracy: 0.2843\nEpoch 56/100\n2138/2138 [==============================] - 10s 5ms/step - loss: 2.7168 - accuracy: 0.2943 - val_loss: 2.7415 - val_accuracy: 0.2887\nEpoch 57/100\n2138/2138 [==============================] - 8s 4ms/step - loss: 2.7110 - accuracy: 0.2933 - val_loss: 2.7394 - val_accuracy: 0.2891\nEpoch 58/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.7076 - accuracy: 0.2944 - val_loss: 2.7387 - val_accuracy: 0.2885\nEpoch 59/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.7016 - accuracy: 0.2952 - val_loss: 2.7300 - val_accuracy: 0.2893\nEpoch 60/100\n2138/2138 [==============================] - 10s 4ms/step - loss: 2.6990 - accuracy: 0.2970 - val_loss: 2.7279 - val_accuracy: 0.2908\nEpoch 61/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.6931 - accuracy: 0.2967 - val_loss: 2.7276 - val_accuracy: 0.2883\nEpoch 62/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.6896 - accuracy: 0.2999 - val_loss: 2.7183 - val_accuracy: 0.2915\nEpoch 63/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.6878 - accuracy: 0.2992 - val_loss: 2.7178 - val_accuracy: 0.2914\nEpoch 64/100\n2138/2138 [==============================] - 8s 4ms/step - loss: 2.6829 - accuracy: 0.3005 - val_loss: 2.7177 - val_accuracy: 0.2933\nEpoch 65/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.6839 - accuracy: 0.3000 - val_loss: 2.7183 - val_accuracy: 0.2921\nEpoch 66/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.6753 - accuracy: 0.3004 - val_loss: 2.7141 - val_accuracy: 0.2942\nEpoch 67/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.6749 - accuracy: 0.3015 - val_loss: 2.7108 - val_accuracy: 0.2933\nEpoch 68/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.6689 - accuracy: 0.3030 - val_loss: 2.7108 - val_accuracy: 0.2941\nEpoch 69/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 2.6640 - accuracy: 0.3031 - val_loss: 2.7110 - val_accuracy: 0.2919\nEpoch 70/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6614 - accuracy: 0.3030 - val_loss: 2.7003 - val_accuracy: 0.2963\nEpoch 71/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6594 - accuracy: 0.3024 - val_loss: 2.7004 - val_accuracy: 0.2966\nEpoch 72/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6599 - accuracy: 0.3032 - val_loss: 2.6959 - val_accuracy: 0.2948\nEpoch 73/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6526 - accuracy: 0.3034 - val_loss: 2.7043 - val_accuracy: 0.2944\nEpoch 74/100\n2138/2138 [==============================] - 5s 3ms/step - loss: 2.6529 - accuracy: 0.3036 - val_loss: 2.7027 - val_accuracy: 0.2959\nEpoch 75/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.6494 - accuracy: 0.3052 - val_loss: 2.6973 - val_accuracy: 0.2958\nEpoch 76/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6455 - accuracy: 0.3047 - val_loss: 2.6994 - val_accuracy: 0.2955\nEpoch 77/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6406 - accuracy: 0.3078 - val_loss: 2.6924 - val_accuracy: 0.2951\nEpoch 78/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6388 - accuracy: 0.3071 - val_loss: 2.6920 - val_accuracy: 0.2980\nEpoch 79/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6379 - accuracy: 0.3066 - val_loss: 2.6872 - val_accuracy: 0.2960\nEpoch 80/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.6336 - accuracy: 0.3082 - val_loss: 2.6966 - val_accuracy: 0.2963\nEpoch 81/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.6305 - accuracy: 0.3080 - val_loss: 2.6844 - val_accuracy: 0.2988\nEpoch 82/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.6302 - accuracy: 0.3085 - val_loss: 2.6854 - val_accuracy: 0.2955\nEpoch 83/100\n2138/2138 [==============================] - 5s 2ms/step - loss: 2.6274 - accuracy: 0.3095 - val_loss: 2.6818 - val_accuracy: 0.3002\nEpoch 84/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.6237 - accuracy: 0.3096 - val_loss: 2.6807 - val_accuracy: 0.2994\nEpoch 85/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.6223 - accuracy: 0.3084 - val_loss: 2.6757 - val_accuracy: 0.2989\nEpoch 86/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.6192 - accuracy: 0.3098 - val_loss: 2.6785 - val_accuracy: 0.3014\nEpoch 87/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.6124 - accuracy: 0.3123 - val_loss: 2.6783 - val_accuracy: 0.3016\nEpoch 88/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.6126 - accuracy: 0.3126 - val_loss: 2.6733 - val_accuracy: 0.3027\nEpoch 89/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.6096 - accuracy: 0.3120 - val_loss: 2.6729 - val_accuracy: 0.3004\nEpoch 90/100\n2138/2138 [==============================] - 8s 4ms/step - loss: 2.6097 - accuracy: 0.3134 - val_loss: 2.6695 - val_accuracy: 0.3028\nEpoch 91/100\n2138/2138 [==============================] - 7s 3ms/step - loss: 2.6047 - accuracy: 0.3123 - val_loss: 2.6738 - val_accuracy: 0.2988\nEpoch 92/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.6045 - accuracy: 0.3122 - val_loss: 2.6699 - val_accuracy: 0.3000\nEpoch 93/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.5999 - accuracy: 0.3162 - val_loss: 2.6729 - val_accuracy: 0.2990\nEpoch 94/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.5989 - accuracy: 0.3141 - val_loss: 2.6758 - val_accuracy: 0.2978\nEpoch 95/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.5965 - accuracy: 0.3160 - val_loss: 2.6650 - val_accuracy: 0.3024\nEpoch 96/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.5928 - accuracy: 0.3148 - val_loss: 2.6681 - val_accuracy: 0.3011\nEpoch 97/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.5942 - accuracy: 0.3148 - val_loss: 2.6689 - val_accuracy: 0.3007\nEpoch 98/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.5928 - accuracy: 0.3166 - val_loss: 2.6657 - val_accuracy: 0.3030\nEpoch 99/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.5887 - accuracy: 0.3154 - val_loss: 2.6600 - val_accuracy: 0.3027\nEpoch 100/100\n2138/2138 [==============================] - 6s 3ms/step - loss: 2.5875 - accuracy: 0.3152 - val_loss: 2.6611 - val_accuracy: 0.3023\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 16,
      "block_group": "041780bb45d04fd289c5cc6b7c721c95",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(\"Training Accuracy:\", model2_4.evaluate(np.expand_dims(X_train2_2, 1), y_train2_2))\ny_test_pred2_4 = model2_4.evaluate(np.expand_dims(X_test2_2, 1), y_test2_2)\nprint(\"Test Accuracy:\", y_test_pred2_4)",
      "metadata": {
        "cell_id": "c46933f6b9584382810cb01a10fcebea",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2138/2138 [==============================] - 2s 860us/step - loss: 2.4882 - accuracy: 0.3381\nTraining Accuracy: [2.488222122192383, 0.3381189703941345]\n713/713 [==============================] - 1s 1ms/step - loss: 2.6406 - accuracy: 0.3028\nTest Accuracy: [2.640625238418579, 0.3027631640434265]\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 17,
      "block_group": "c0d6a03fcacd4062bf55a7db47b5ae69",
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "source": "print(classification_report(y_test2_2, np.argmax(model2_4.predict(np.expand_dims(X_test2_2, 1)), axis =-1)))",
      "metadata": {
        "cell_id": "b1c017845e2243dea7dcd4e8455eb029",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "713/713 [==============================] - 1s 711us/step\n              precision    recall  f1-score   support\n\n           0       0.19      0.15      0.17       214\n           1       0.32      0.31      0.31       202\n           2       0.12      0.04      0.06       206\n           3       0.12      0.07      0.09       201\n           4       0.31      0.40      0.35       192\n           5       0.14      0.10      0.12       199\n           6       0.59      0.51      0.55       195\n           7       0.44      0.53      0.48       178\n           8       0.11      0.02      0.04       229\n           9       0.16      0.29      0.21       206\n          10       0.37      0.46      0.41       200\n          11       0.07      0.03      0.04       187\n          12       0.25      0.20      0.22       222\n          13       0.49      0.52      0.50       203\n          14       0.34      0.38      0.36       201\n          15       0.23      0.19      0.21       208\n          16       0.51      0.59      0.55       201\n          17       0.33      0.18      0.23       209\n          18       0.95      0.84      0.89       216\n          19       0.21      0.28      0.24       209\n          20       0.21      0.21      0.21       214\n          21       0.28      0.26      0.27       192\n          22       0.31      0.42      0.36       185\n          23       0.16      0.30      0.21       208\n          24       0.60      0.45      0.52       201\n          25       0.15      0.13      0.14       224\n          26       0.41      0.31      0.35       203\n          27       0.51      0.48      0.49       199\n          28       0.11      0.02      0.03       205\n          29       0.24      0.33      0.28       207\n          30       0.17      0.10      0.13       216\n          31       0.18      0.06      0.09       210\n          32       0.14      0.05      0.07       202\n          33       0.12      0.03      0.04       184\n          34       0.13      0.07      0.09       208\n          35       0.37      0.62      0.46       199\n          36       0.26      0.20      0.22       200\n          37       0.32      0.13      0.18       205\n          38       0.23      0.11      0.15       211\n          39       0.31      0.09      0.14       183\n          40       0.27      0.46      0.34       182\n          41       0.18      0.05      0.07       195\n          42       0.83      0.89      0.86       208\n          43       0.11      0.03      0.05       212\n          44       0.18      0.17      0.17       198\n          45       0.39      0.34      0.36       181\n          46       0.39      0.41      0.40       193\n          47       0.13      0.14      0.13       191\n          48       0.34      0.24      0.28       200\n          49       0.33      0.28      0.31       199\n          50       0.24      0.38      0.29       201\n          51       0.17      0.29      0.22       206\n          52       0.72      0.80      0.76       186\n          53       0.12      0.17      0.14       218\n          54       0.50      0.36      0.42       224\n          55       0.12      0.10      0.11       205\n          56       0.08      0.01      0.02       200\n          57       0.23      0.05      0.08       214\n          58       0.24      0.13      0.17       194\n          59       0.54      0.61      0.57       202\n          60       0.54      0.41      0.47       195\n          61       0.62      0.53      0.57       226\n          62       0.12      0.02      0.04       209\n          63       0.10      0.06      0.08       172\n          64       0.29      0.48      0.36       195\n          65       0.20      0.18      0.19       204\n          66       0.61      0.70      0.65       207\n          67       0.12      0.27      0.17       179\n          68       0.18      0.16      0.17       204\n          69       0.25      0.24      0.24       213\n          70       0.28      0.34      0.31       236\n          71       0.19      0.22      0.21       226\n          72       0.18      0.48      0.26       182\n          73       0.32      0.62      0.42       185\n          74       0.18      0.21      0.19       209\n          75       0.45      0.53      0.49       184\n          76       0.50      0.36      0.42       204\n          77       0.39      0.57      0.46       202\n          78       0.32      0.56      0.40       198\n          79       0.58      0.35      0.44       178\n          80       0.15      0.21      0.17       201\n          81       0.25      0.62      0.36       199\n          82       0.31      0.39      0.34       207\n          83       0.19      0.22      0.20       176\n          84       0.20      0.19      0.19       205\n          85       0.16      0.10      0.12       195\n          86       0.16      0.13      0.15       186\n          87       0.19      0.13      0.15       182\n          88       0.12      0.10      0.11       180\n          89       0.12      0.05      0.07       213\n          90       0.10      0.17      0.12       192\n          91       0.30      0.49      0.37       183\n          92       0.30      0.27      0.28       188\n          93       0.58      0.68      0.63       191\n          94       0.24      0.45      0.31       187\n          95       0.48      0.72      0.58       198\n          96       0.34      0.34      0.34       195\n          97       0.37      0.67      0.47       227\n          98       0.28      0.23      0.25       216\n          99       0.12      0.04      0.06       187\n         100       0.18      0.41      0.25       194\n         101       0.80      0.72      0.76       215\n         102       0.18      0.05      0.08       197\n         103       0.15      0.23      0.18       172\n         104       0.24      0.05      0.08       230\n         105       0.60      0.82      0.69       202\n         106       0.15      0.03      0.05       188\n         107       0.22      0.21      0.22       179\n         108       0.69      0.78      0.73       210\n         109       0.17      0.06      0.08       197\n         110       0.23      0.37      0.29       166\n         111       0.24      0.22      0.23       191\n         112       0.20      0.50      0.29       193\n         113       0.27      0.27      0.27       207\n\n    accuracy                           0.30     22800\n   macro avg       0.29      0.30      0.28     22800\nweighted avg       0.29      0.30      0.28     22800\n\n"
        }
      ],
      "outputs_reference": null,
      "execution_count": 18,
      "block_group": "cb34e47a6c314347a5bea5a15ce92edd",
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=3da032f1-e5ab-4726-ac09-eb3d9c053730' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "a728807887654fb5a4c393553674f2a1",
    "deepnote_execution_queue": []
  }
}